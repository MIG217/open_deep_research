{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Annotated, TypedDict, operator, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "from open_deep_research.configuration import Configuration\n",
    "from open_deep_research.utils import get_config_value, select_and_execute_search\n",
    "\n",
    "## Tools factory - will be initialized based on configuration\n",
    "def get_search_tool(config: RunnableConfig):\n",
    "    \"\"\"Get the appropriate search tool based on configuration\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    search_api = get_config_value(configurable.search_api)\n",
    "    \n",
    "    # Default to Tavily if not specified\n",
    "    if search_api.lower() == \"tavily\":\n",
    "        return TavilySearch(\n",
    "            max_results=5,\n",
    "            topic=\"general\",\n",
    "            include_raw_content=True\n",
    "        )\n",
    "    else:\n",
    "        # For other search APIs, we'll use select_and_execute_search in the research agent\n",
    "        # This is a placeholder tool that will be replaced in execution\n",
    "        return TavilySearch(\n",
    "            max_results=5,\n",
    "            topic=\"general\",\n",
    "            include_raw_content=True\n",
    "        )\n",
    "\n",
    "@tool\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Research scope for this section of the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the section.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Sections(BaseModel):\n",
    "    sections: List[str] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Introduction(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the introduction, giving an overview of the report.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Conclusion(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for the conclusion of the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the conclusion, summarizing the report.\"\n",
    "    )\n",
    "\n",
    "## State\n",
    "class ReportStateOutput(TypedDict):\n",
    "    final_report: str # Final report\n",
    "\n",
    "class ReportState(MessagesState):\n",
    "    sections: list[str] # List of report sections \n",
    "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
    "    final_report: str # Final report\n",
    "\n",
    "class SectionState(MessagesState):\n",
    "    section: str # Report section  \n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SectionOutputState(TypedDict):\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "## Supervisor\n",
    "SUPERVISOR_INSTRUCTIONS = \"\"\"\n",
    "You are scoping research for a report based on a user-provided topic.\n",
    "\n",
    "### Your responsibilities:\n",
    "\n",
    "1. **Gather Background Information**  \n",
    "   Based upon the user's topic, use the `tavily_search_tool` to collect relevant information about the topic. \n",
    "   - You MUST perform at least  1 search to gather comprehensive context\n",
    "   - Take time to analyze and synthesize the search results before proceeding\n",
    "   - Do not proceed to the next step until you have an understanding of the topic\n",
    "\n",
    "2. **Clarify the Topic**  \n",
    "   After your initial research, engage with the user to clarify any questions that arose.\n",
    "   - Ask specific follow-up questions based on what you learned from your searches\n",
    "   - Do not proceed until you fully understand the topic, goals, constraints, and any preferences\n",
    "   - Synthesize what you've learned so far before asking questions\n",
    "   - You MUST engage in at least one clarification exchange with the user before proceeding\n",
    "\n",
    "3. **Define Report Structure**  \n",
    "   Only after completing both research AND clarification with the user:\n",
    "   - Use the `Sections` tool to define a list of report sections\n",
    "   - Each section should be a written description with: a section name and a section research plan\n",
    "   - Do not include sections for introductions or conclusions (We'll add these later)\n",
    "   - Ensure sections are scoped to be independently researchable\n",
    "   - Base your sections on both the search results AND user clarifications\n",
    "\n",
    "4. **Assemble the Final Report**  \n",
    "   When all sections are returned:\n",
    "   - IMPORTANT: First check your previous messages to see what you've already completed\n",
    "   - If you haven't created an introduction yet, use the `Introduction` tool to generate one\n",
    "     - Set content to include report title with a single # (H1 level) at the beginning\n",
    "     - Example: \"# [Report Title]\\n\\n[Introduction content...]\"\n",
    "   - After the introduction, use the `Conclusion` tool to summarize key insights\n",
    "     - Set content to include conclusion title with ## (H2 level) at the beginning\n",
    "     - Example: \"## Conclusion\\n\\n[Conclusion content...]\"\n",
    "   - Do not call the same tool twice - check your message history\n",
    "\n",
    "### Additional Notes:\n",
    "- You are a reasoning model. Think through problems step-by-step before acting.\n",
    "- IMPORTANT: Do not rush to create the report structure. Gather information thoroughly first.\n",
    "- Use multiple searches to build a complete picture before drawing conclusions.\n",
    "- Maintain a clear, informative, and professional tone throughout.\"\"\"\n",
    "\n",
    "RESEARCH_INSTRUCTIONS = \"\"\"\n",
    "You are a researcher responsible for completing a specific section of a report.\n",
    "\n",
    "### Your goals:\n",
    "\n",
    "1. **Understand the Section Scope**  \n",
    "   Begin by reviewing the section name and description. This defines your research focus. Use it as your objective.\n",
    "\n",
    "\n",
    "<Section Description>\n",
    "{section_description}\n",
    "</Section Description>\n",
    "\n",
    "2. **Research the Topic**  \n",
    "   Use the `tavily_search_tool` to gather relevant information and evidence. Search iteratively if needed to fully understand the section's scope.\n",
    "   - Save the URLs from your searches - you will need to cite them later\n",
    "   - Aim to gather information from at least 3 different sources\n",
    "\n",
    "3. **Use the Section Tool**  \n",
    "   Once you've gathered sufficient context, write a high-quality section using the Section tool:\n",
    "   - `name`: The title of the section\n",
    "   - `description`: The scope of research you completed (brief, 1-2 sentences)\n",
    "   - `content`: The completed body of text for the section, which MUST:\n",
    "     - Begin with the section title formatted as \"## [Section Title]\" (H2 level with ##)\n",
    "     - Be formatted in Markdown style\n",
    "     - Be MAXIMUM 200 words (strictly enforce this limit)\n",
    "     - End with a \"### Sources\" subsection (H3 level with ###) containing a numbered list of URLs used\n",
    "     - Use clear, concise language with bullet points where appropriate\n",
    "     - Include relevant facts, statistics, or expert opinions\n",
    "\n",
    "Example format for content:\n",
    "```\n",
    "## [Section Title]\n",
    "\n",
    "[Body text in markdown format, maximum 200 words...]\n",
    "\n",
    "### Sources\n",
    "1. [URL 1]\n",
    "2. [URL 2]\n",
    "3. [URL 3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Reasoning Guidance\n",
    "\n",
    "You are a reasoning model. Think through the task step-by-step before writing. Break down complex questions. If you're unsure about something, search again.\n",
    "\n",
    "- You may reason internally before producing content.\n",
    "- Your job is not to summarize randomly—it's to **research and synthesize a strong, scoped contribution** to a report.\n",
    "- Always track and cite your sources.\n",
    "- Be concise - stay within the 200 word limit for the main content.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- Do not write introductions or conclusions unless explicitly part of your section.\n",
    "- Keep a professional, factual tone.\n",
    "- If you do not have enough information to complete the section, search again or clarify your approach before continuing.\n",
    "- Always follow markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "# Tool lists will be built dynamically based on configuration\n",
    "def get_supervisor_tools(config: RunnableConfig):\n",
    "    \"\"\"Get supervisor tools based on configuration\"\"\"\n",
    "    search_tool = get_search_tool(config)\n",
    "    tool_list = [search_tool, Sections, Introduction, Conclusion]\n",
    "    return tool_list, {tool.name: tool for tool in tool_list}\n",
    "\n",
    "def get_research_tools(config: RunnableConfig):\n",
    "    \"\"\"Get research tools based on configuration\"\"\"\n",
    "    search_tool = get_search_tool(config)\n",
    "    tool_list = [search_tool, Section]\n",
    "    return tool_list, {tool.name: tool for tool in tool_list}\n",
    "\n",
    "def supervisor(state: ReportState, config: RunnableConfig):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    # Messages\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    supervisor_model = get_config_value(configurable.supervisor_model)\n",
    "    \n",
    "    # Initialize the model\n",
    "    llm = init_chat_model(model=supervisor_model)\n",
    "    \n",
    "    # If sections have been completed, but we don't yet have the final report, then we need to initiate writing the introduction and conclusion\n",
    "    if state.get(\"completed_sections\") and not state.get(\"final_report\"):\n",
    "        research_complete_message = {\"role\": \"user\", \"content\": \"Research is complete. Now write the introduction and conclusion for the report. Here are the completed main body sections: \\n\\n\" + \"\\n\\n\".join([s.content for s in state[\"completed_sections\"]])}\n",
    "        messages = messages + [research_complete_message]\n",
    "\n",
    "    # Get tools based on configuration\n",
    "    supervisor_tool_list, _ = get_supervisor_tools(config)\n",
    "    \n",
    "    # Invoke\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm.bind_tools(supervisor_tool_list).invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": SUPERVISOR_INSTRUCTIONS,\n",
    "                    }\n",
    "                ]\n",
    "                + messages\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def supervisor_tools(state: ReportState, config: RunnableConfig)  -> Command[Literal[\"supervisor\", \"research_team\", \"__end__\"]]:\n",
    "    \"\"\"Performs the tool call and sends to the research agent\"\"\"\n",
    "\n",
    "    result = []\n",
    "    sections_list = []\n",
    "    intro_content = None\n",
    "    conclusion_content = None\n",
    "\n",
    "    # Get tools based on configuration\n",
    "    _, supervisor_tools_by_name = get_supervisor_tools(config)\n",
    "    \n",
    "    # First process all tool calls to ensure we respond to each one (required for OpenAI)\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Get the tool\n",
    "        tool = supervisor_tools_by_name[tool_call[\"name\"]]\n",
    "        # Perform the tool call\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "        # Append to messages \n",
    "        result.append({\"role\": \"tool\", \n",
    "                       \"content\": observation, \n",
    "                       \"name\": tool_call[\"name\"], \n",
    "                       \"tool_call_id\": tool_call[\"id\"]})\n",
    "        \n",
    "        # Store special tool results for processing after all tools have been called\n",
    "        if tool_call[\"name\"] == \"Sections\":\n",
    "            sections_list = observation.sections\n",
    "        elif tool_call[\"name\"] == \"Introduction\":\n",
    "            # Format introduction with proper H1 heading if not already formatted\n",
    "            if not observation.content.startswith(\"# \"):\n",
    "                intro_content = f\"# {observation.name}\\n\\n{observation.content}\"\n",
    "            else:\n",
    "                intro_content = observation.content\n",
    "        elif tool_call[\"name\"] == \"Conclusion\":\n",
    "            # Format conclusion with proper H2 heading if not already formatted\n",
    "            if not observation.content.startswith(\"## \"):\n",
    "                conclusion_content = f\"## {observation.name}\\n\\n{observation.content}\"\n",
    "            else:\n",
    "                conclusion_content = observation.content\n",
    "    \n",
    "    # After processing all tool calls, decide what to do next\n",
    "    if sections_list:\n",
    "        # Send the sections to the research agents\n",
    "        return Command(goto=[Send(\"research_team\", {\"section\": s}) for s in sections_list], update={\"messages\": result})\n",
    "    elif intro_content:\n",
    "        # Store introduction while waiting for conclusion\n",
    "        # Append to messages to guide the LLM to write conclusion next\n",
    "        result.append({\"role\": \"user\", \"content\": \"Introduction written. Now write a conclusion section.\"})\n",
    "        return Command(goto=\"supervisor\", update={\"final_report\": intro_content, \"messages\": result})\n",
    "    elif conclusion_content:\n",
    "        # Get all sections and combine in proper order: Introduction, Body Sections, Conclusion\n",
    "        intro = state.get(\"final_report\", \"\")\n",
    "        body_sections = \"\\n\\n\".join([s.content for s in state[\"completed_sections\"]])\n",
    "        \n",
    "        # Assemble final report in correct order\n",
    "        complete_report = f\"{intro}\\n\\n{body_sections}\\n\\n{conclusion_content}\"\n",
    "        \n",
    "        # Append to messages to indicate completion\n",
    "        result.append({\"role\": \"user\", \"content\": \"Report is now complete with introduction, body sections, and conclusion.\"})\n",
    "        return Command(goto=\"supervisor\", update={\"final_report\": complete_report, \"messages\": result})\n",
    "    else:\n",
    "        # Default case (for search tools, etc.)\n",
    "        return Command(goto=\"supervisor\", update={\"messages\": result})\n",
    "\n",
    "def supervisor_should_continue(state: ReportState) -> Literal[\"supervisor_tools\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"supervisor_tools\"\n",
    "    \n",
    "    # Else end because the supervisor asked a question or is finished\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def research_agent(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    \n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    researcher_model = get_config_value(configurable.researcher_model)\n",
    "    \n",
    "    # Initialize the model\n",
    "    llm = init_chat_model(model=researcher_model)\n",
    "\n",
    "    # Get tools based on configuration\n",
    "    research_tool_list, _ = get_research_tools(config)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            # Enforce tool calling to either perform more search or call the Section tool to write the section\n",
    "            llm.bind_tools(research_tool_list).invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": RESEARCH_INSTRUCTIONS.format(section_description=state[\"section\"])\n",
    "                    }\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def research_agent_tools(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\"Performs the tool call and route to supervisor or continue the research loop\"\"\"\n",
    "\n",
    "    result = []\n",
    "    completed_section = None\n",
    "    \n",
    "    # Get tools based on configuration\n",
    "    _, research_tools_by_name = get_research_tools(config)\n",
    "    \n",
    "    # Process all tool calls first (required for OpenAI)\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Get the tool\n",
    "        tool = research_tools_by_name[tool_call[\"name\"]]\n",
    "        # Perform the tool call \n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        # Append to messages \n",
    "        result.append({\"role\": \"tool\", \n",
    "                       \"content\": observation, \n",
    "                       \"name\": tool_call[\"name\"], \n",
    "                       \"tool_call_id\": tool_call[\"id\"]})\n",
    "        \n",
    "        # Store the section observation if a Section tool was called\n",
    "        if tool_call[\"name\"] == \"Section\":\n",
    "            completed_section = observation\n",
    "    \n",
    "    # After processing all tools, decide what to do next\n",
    "    if completed_section:\n",
    "        # Write the completed section to state and return to the supervisor\n",
    "        return {\"messages\": result, \"completed_sections\": [completed_section]}\n",
    "    else:\n",
    "        # Continue the research loop for search tools, etc.\n",
    "        return {\"messages\": result}\n",
    "\n",
    "def research_agent_should_continue(state: SectionState) -> Literal[\"research_agent_tools\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"research_agent_tools\"\n",
    "\n",
    "    else:\n",
    "        return END\n",
    "    \n",
    "\"\"\"Build the multi-agent workflow\"\"\"\n",
    "\n",
    "# Research agent workflow\n",
    "research_builder = StateGraph(SectionState, output=SectionOutputState, config_schema=Configuration)\n",
    "research_builder.add_node(\"research_agent\", research_agent)\n",
    "research_builder.add_node(\"research_agent_tools\", research_agent_tools)\n",
    "research_builder.add_edge(START, \"research_agent\") \n",
    "research_builder.add_conditional_edges(\n",
    "    \"research_agent\",\n",
    "    research_agent_should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"research_agent_tools\": \"research_agent_tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "research_builder.add_edge(\"research_agent_tools\", \"research_agent\")\n",
    "\n",
    "# Supervisor workflow\n",
    "supervisor_builder = StateGraph(ReportState, input=MessagesState, output=ReportStateOutput)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "supervisor_builder.add_node(\"research_team\", research_builder.compile())\n",
    "\n",
    "# Flow of the supervisor agent\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    supervisor_should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"supervisor_tools\": \"supervisor_tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "supervisor_builder.add_edge(\"research_team\", \"supervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = research_builder.compile(name=\"research_agent\")\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the supervisor agent\n",
    "agent = supervisor_builder.compile(name=\"research_team\")\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is model context protocol?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_i7PMNtBPw5HMV44bry9kydrU)\n",
      " Call ID: call_i7PMNtBPw5HMV44bry9kydrU\n",
      "  Args:\n",
      "    query: Model Context Protocol architecture\n",
      "    search_depth: advanced\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{'query': 'Model Context Protocol architecture', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Specification - Model Context Protocol', 'url': 'https://spec.modelcontextprotocol.io/specification/draft/architecture/', 'content': 'Scroll to top\\nSpecification\\nSpecification (Draft)\\nArchitecture\\nArchitecture\\nThe Model Context Protocol (MCP) follows a client-host-server architecture where each host can run multiple client instances. This architecture enables users to integrate AI capabilities across applications while maintaining clear security boundaries and isolating concerns. Built on JSON-RPC, MCP provides a stateful session protocol focused on context exchange and sampling coordination between clients and servers. [...] Each message type follows the JSON-RPC 2.0 specification for structure and delivery semantics.\\nCapability Negotiation\\nThe Model Context Protocol uses a capability-based negotiation system where clients and servers explicitly declare their supported features during initialization. Capabilities determine which protocol features and primitives are available during a session.', 'score': 0.9471519, 'raw_content': 'Architecture – Model Context Protocol Specification\\n Specification (Draft) Specification (Latest) Resources\\nCTRL K\\nGitHub\\nCTRL K\\n\\n\\nSpecification\\n\\n\\nSpecification (Draft)\\n\\nArchitecture\\nCore Components\\nDesign Principles\\nMessage Types\\nCapability Negotiation\\n\\n\\n\\nBase Protocol\\n\\nTransports\\nMessages\\nLifecycle\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nSpecification (Latest)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nMessages\\nLifecycle\\nTransports\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nClaude\\n\\n\\n\\n\\nSchema ↗\\n\\nMore\\nUser Guide ↗\\nPython SDK ↗\\n\\nTypeScript SDK ↗\\n\\n\\nSpecification (Draft)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nTransports\\nMessages\\nLifecycle\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nSpecification (Latest)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nMessages\\nLifecycle\\nTransports\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nClaude\\n\\nSchema ↗\\nMore\\nUser Guide ↗\\nPython SDK ↗\\nTypeScript SDK ↗\\n\\nLightDark\\nOn this page\\n\\nCore Components\\nHost\\nClients\\nServers\\nDesign Principles\\nMessage Types\\nCapability Negotiation\\n\\nScroll to top\\nSpecification\\nSpecification (Draft)\\nArchitecture\\nArchitecture\\nThe Model Context Protocol (MCP) follows a client-host-server architecture where each host can run multiple client instances. This architecture enables users to integrate AI capabilities across applications while maintaining clear security boundaries and isolating concerns. Built on JSON-RPC, MCP provides a stateful session protocol focused on context exchange and sampling coordination between clients and servers.\\nCore Components\\ngraph LR\\n    subgraph \"Application Host Process\"\\n        H[Host]\\n        C1[Client 1]\\n        C2[Client 2]\\n        C3[Client 3]\\n        H --> C1\\n        H --> C2\\n        H --> C3\\n    end\\nsubgraph \"Local machine\"\\n    S1\\\\[Server 1\\n\\nFiles & Git]\\n        S2[Server 2\\nDatabase]\\n        R1[(\"Local\\nResource A\")]\\n        R2[(\"Local\\nResource B\")]\\n    C1 --\\\\> S1\\n    C2 --\\\\> S2\\n    S1 <\\\\--\\\\> R1\\n    S2 <\\\\--\\\\> R2\\nend\\n\\nsubgraph \"Internet\"\\n    S3\\\\[Server 3\\n\\nExternal APIs]\\n        R3[(\"Remote\\nResource C\")]\\n    C3 --\\\\> S3\\n    S3 <\\\\--\\\\> R3\\nend\\n\\nHost\\nThe host process acts as the container and coordinator:\\n\\nCreates and manages multiple client instances\\nControls client connection permissions and lifecycle\\nEnforces security policies and consent requirements\\nHandles user authorization decisions\\nCoordinates AI/LLM integration and sampling\\nManages context aggregation across clients\\n\\nClients\\nEach client is created by the host and maintains an isolated server connection:\\n\\nEstablishes one stateful session per server\\nHandles protocol negotiation and capability exchange\\nRoutes protocol messages bidirectionally\\nManages subscriptions and notifications\\nMaintains security boundaries between servers\\n\\nA host application creates and manages multiple clients, with each client having a 1:1 relationship with a particular server.\\nServers\\nServers provide specialized context and capabilities:\\n\\nExpose resources, tools and prompts via MCP primitives\\nOperate independently with focused responsibilities\\nRequest sampling through client interfaces\\nMust respect security constraints\\nCan be local processes or remote services\\n\\nDesign Principles\\nMCP is built on several key design principles that inform its architecture and implementation:\\n\\n\\nServers should be extremely easy to build\\n\\nHost applications handle complex orchestration responsibilities\\nServers focus on specific, well-defined capabilities\\nSimple interfaces minimize implementation overhead\\nClear separation enables maintainable code\\n\\nServers should be highly composable\\n\\n\\nEach server provides focused functionality in isolation\\n\\nMultiple servers can be combined seamlessly\\nShared protocol enables interoperability\\nModular design supports extensibility\\n\\nServers should not be able to read the whole conversation, nor “see into” other servers\\n\\n\\nServers receive only necessary contextual information\\n\\nFull conversation history stays with the host\\nEach server connection maintains isolation\\nCross-server interactions are controlled by the host\\nHost process enforces security boundaries\\n\\nFeatures can be added to servers and clients progressively\\n\\n\\nCore protocol provides minimal required functionality\\n\\nAdditional capabilities can be negotiated as needed\\nServers and clients evolve independently\\nProtocol designed for future extensibility\\nBackwards compatibility is maintained\\n\\n\\n\\nMessage Types\\nMCP defines three core message types based on JSON-RPC 2.0:\\n\\nRequests: Bidirectional messages with method and parameters expecting a response\\nResponses: Successful results or errors matching specific request IDs\\nNotifications: One-way messages requiring no response\\n\\nEach message type follows the JSON-RPC 2.0 specification for structure and delivery semantics.\\nCapability Negotiation\\nThe Model Context Protocol uses a capability-based negotiation system where clients and servers explicitly declare their supported features during initialization. Capabilities determine which protocol features and primitives are available during a session.\\n\\nServers declare capabilities like resource subscriptions, tool support, and prompt templates\\nClients declare capabilities like sampling support and notification handling\\nBoth parties must respect declared capabilities throughout the session\\nAdditional capabilities can be negotiated through extensions to the protocol\\n\\nsequenceDiagram\\n    participant Host\\n    participant Client\\n    participant Server\\nHost-\\\\>\\\\>+Client: Initialize client\\nClient-\\\\>\\\\>+Server: Initialize session with capabilities\\nServer--\\\\>\\\\>Client: Respond with supported capabilities\\n\\nNote over Host,Server: Active Session with Negotiated Features\\n\\nloop Client Requests\\n    Host-\\\\>\\\\>Client: User- or model-initiated action\\n    Client-\\\\>\\\\>Server: Request (tools/resources)\\n    Server--\\\\>\\\\>Client: Response\\n    Client--\\\\>\\\\>Host: Update UI or respond to model\\nend\\n\\nloop Server Requests\\n    Server-\\\\>\\\\>Client: Request (sampling)\\n    Client-\\\\>\\\\>Host: Forward to AI\\n    Host--\\\\>\\\\>Client: AI response\\n    Client--\\\\>\\\\>Server: Response\\nend\\n\\nloop Notifications\\n    Server--)Client: Resource updates\\n    Client--)Server: Status changes\\nend\\n\\nHost-\\\\>\\\\>Client: Terminate\\nClient-\\\\>\\\\>\\\\-Server: End session\\ndeactivate Server\\n\\nEach capability unlocks specific protocol features for use during the session. For example:\\n\\nImplemented server features must be advertised in the server’s capabilities\\nEmitting resource subscription notifications requires the server to declare subscription support\\nTool invocation requires the server to declare tool capabilities\\nSampling requires the client to declare support in its capabilities\\n\\nThis capability negotiation ensures clients and servers have a clear understanding of supported functionality while maintaining protocol extensibility.\\nPowered by Hextra'}, {'url': 'https://www.anthropic.com/news/model-context-protocol', 'title': 'Introducing the Model Context Protocol - Anthropic', 'content': \"The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\\nToday, we're introducing three major components of the Model Context Protocol for developers:\", 'score': 0.9247491, 'raw_content': 'Introducing the Model Context Protocol \\\\ Anthropic\\nSkip to main contentSkip to footer\\n\\nClaude\\nAPI\\nResearch\\nCommitments\\nLearn\\nNews\\nTry Claude\\nAnnouncements\\nIntroducing the Model Context Protocol\\nNov 25, 2024●3 min read\\n\\nToday, we\\'re open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.\\nAs AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.\\nMCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.\\nModel Context Protocol\\nThe Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\\nToday, we\\'re introducing three major components of the Model Context Protocol for developers:\\n\\nThe Model Context Protocol specification and SDKs\\nLocal MCP server support in the Claude Desktop apps\\nAn open-source repository of MCP servers\\n\\nClaude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, we’re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.\\nEarly adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platforms—enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts.\\n\"At Block, open source is more than a development model—it’s the foundation of our work and a commitment to creating technology that drives meaningful change and serves as a public good for all,” said Dhanji R. Prasanna, Chief Technology Officer at Block. “Open technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build agentic systems, which remove the burden of the mechanical so people can focus on the creative.”\\nInstead of maintaining separate connectors for each data source, developers can now build against a standard protocol. As the ecosystem matures, AI systems will maintain context as they move between different tools and datasets, replacing today\\'s fragmented integrations with a more sustainable architecture.\\nGetting started\\nDevelopers can start building and testing MCP connectors today. All Claude.ai plans support connecting MCP servers to the Claude Desktop app.\\nClaude for Work customers can begin testing MCP servers locally, connecting Claude to internal systems and datasets. We\\'ll soon provide developer toolkits for deploying remote production MCP servers that can serve your entire Claude for Work organization.\\nTo start building:\\n\\nInstall pre-built MCP servers through the Claude Desktop app\\nFollow our quickstart guide to build your first MCP server\\nContribute to our open-source repositories of connectors and implementations\\n\\nAn open community\\nWe’re committed to building MCP as a collaborative, open-source project and ecosystem, and we’re eager to hear your feedback. Whether you’re an AI tool developer, an enterprise looking to leverage existing data, or an early adopter exploring the frontier, we invite you to build the future of context-aware AI together.\\n\\n\\nProduct\\n\\nClaude overview\\nClaude team plan\\nClaude enterprise plan\\nDownload Claude apps\\nClaude.ai pricing plans\\nClaude.ai login\\n\\nAPI Platform\\n\\nAPI overview\\nDeveloper docs\\nPricing\\nConsole login\\n\\nResearch\\n\\nResearch overview\\nEconomic Index\\n\\nClaude models\\n\\nClaude 3.7 Sonnet\\nClaude 3.5 Haiku\\nClaude 3 Opus\\n\\nCommitments\\n\\nTransparency\\nResponsible scaling policy\\nSecurity and compliance\\n\\nSolutions\\n\\nCoding\\n\\nLearning resources\\n\\nNews\\nCustomer stories\\nEngineering at Anthropic\\n\\nCompany\\n\\nAbout us\\nCareers\\n\\nHelp and security\\n\\nStatus\\nAvailability\\nSupport center\\n\\nTerms and policies\\nPrivacy choices*   Privacy policy\\n*   Responsible disclosure policy\\n*   Terms of service - consumer\\n*   Terms of service - commercial\\n*   Usage policy\\n© 2025 Anthropic PBC\\n'}, {'title': 'Understanding Model Context Protocol: A Deep Dive into ... - Medium', 'url': 'https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd', 'content': 'Core Architecture of the Model Context Protocol (MCP) 🏗️🔗\\nUnderstanding the Connection Between Clients, Servers, and LLMs\\nThe Model Context Protocol (MCP) features a flexible and extensible architecture that facilitates seamless communication between LLM applications and integrations. This section outlines the core architectural components and concepts.\\nOverview 🔍\\nMCP operates on a client-server architecture where: [...] 📝 Conclusion\\nThe Model Context Protocol implementation demonstrated here showcases a robust, scalable, and flexible architecture for building sophisticated AI applications. Its modular design and support for multiple transport protocols make it an excellent choice for complex AI systems.\\nReferences [...] From Source\\nModel Context Protocol (MCP) is a stateful, context-preserving framework designed to power intelligent, multi-step interactions between humans and AI agents. Unlike traditional API calls that treat each request as an isolated event, MCP introduces a persistent, evolving context layer 🌐 that allows AI systems to retain memory, learn dynamically, and act autonomously over time.\\nAccording to modelcontextprotocol.io, MCP is built on three pillars:', 'score': 0.9159971, 'raw_content': 'Published Time: 2025-03-17T17:00:26.392Z\\n🚀 Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nHome\\nLibrary\\nYour listsSaved listsHighlightsReading history\\nStoriesStats\\nThe AI Forum ----------------\\nHomeAbout\\n·Follow publication\\n\\nIts AI forum where all the topics spread across Data Analytics, Data Science, Machine Learning, Deep Learning are discussed.\\nFollow publication\\n🚀 Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration\\n\\n\\nPlaban Nayak\\n·Follow\\nPublished in\\nThe AI Forum\\n·\\n14 min read\\n·\\nMar 17, 2025\\n\\n105\\n\\nListen\\nShare\\n\\n🌟 Introduction\\nIn the quest to harness the full capabilities of Large Language Models (LLMs), developers often encounter a significant challenge: accessing the myriad of unique APIs provided by popular tools like Slack, GitHub, or your own local filesystem. While LLMs can connect to these APIs using various tools, the process typically involves writing code to establish these connections.\\nFor those using desktop applications such as Cursor or Claude Desktop, the situation can be even more restrictive, as manual addition of new tools is not feasible.\\nImagine a world where you could seamlessly access a set of pre-built tools specifically designed to integrate with your existing desktop applications. Enter the Model Context Protocol (MCP), a revolutionary approach that allows you to create customizable toolsets that plug directly into your apps.\\n🌐🛠️MCP enables the specification of various commands for LLMs, such as ‘fetch repository’ for GitHub or ‘comment on PR,’ providing a frictionless experience. Whether through established desktop applications or your own software, MCP clients can easily communicate with servers to activate these tools.\\nAs marketplaces for MCP servers rapidly emerge, countless companies are eager to introduce their solutions. This soon-to-be reality will empower you to choose from a diverse array of ready-made tools, enhancing your LLM workflows and driving efficiency in your projects. 🚀✨\\n🔍 What is Model Context Protocol (MCP)?\\n\\nFrom Source\\nModel Context Protocol (MCP) is a stateful, context-preserving framework designed to power intelligent, multi-step interactions between humans and AI agents. Unlike traditional API calls that treat each request as an isolated event, MCP introduces a persistent, evolving context layer 🌐 that allows AI systems to retain memory, learn dynamically, and act autonomously over time.\\nAccording to modelcontextprotocol.io, MCP is built on three pillars:\\n\\nStatefulness 🧠: Maintains session-specific and long-term memory.\\nInteroperability 🔗: Works seamlessly across models, tools, and data sources.\\nAgent-Centric Design 🤖: Prioritizes autonomous decision-making within defined boundaries.\\n\\nExample:\\n\\nAn MCP-powered travel agent 🧳 remembers your budget, allergies, and past trip feedback across conversations to plan a personalized itinerary — no need to repeat yourself!\\n\\nCore Architecture of the Model Context Protocol (MCP) 🏗️🔗\\nUnderstanding the Connection Between Clients, Servers, and LLMs\\nThe Model Context Protocol (MCP) features a flexible and extensible architecture that facilitates seamless communication between LLM applications and integrations. This section outlines the core architectural components and concepts.\\nOverview 🔍\\nMCP operates on a client-server architecture where:\\n\\nHosts: LLM applications (such as Claude Desktop or Integrated Development Environments) that initiate connections.\\nClients: Maintain 1:1 connections with their corresponding servers, operating within the host application.\\nServers: Provide essential context, tools, and prompts to clients.\\n\\n\\nFrom Source\\nServer Process ⚙️\\nThe server process involves the following components:\\n\\nHost: An application that hosts the interaction.\\nTransport Layer: The layer responsible for facilitating communication between clients and servers.\\nMCP Client: The entity that communicates with the MCP server.\\nMCP Server: The component that manages tools, context, and handles requests from the MCP Client.\\n\\nThis architecture ensures that LLMs can effectively access and utilize tools and resources, enhancing their capabilities in various applications. 🌐✨\\nCore Components of the Model Context Protocol (MCP) 🛠️🔗\\nProtocol Layer 📡\\nThe protocol layer is responsible for message framing, linking requests with responses, and defining high-level communication patterns.\\nKey Classes Include:\\n\\nProtocol\\nClient\\nServer\\n\\nTransport Layer 🚚\\nThe transport layer manages the actual communication between clients and servers, supporting multiple transport mechanisms:\\n\\nStdio Transport: Utilizes standard input/output for communication, making it ideal for local processes.\\nHTTP with SSE Transport: Employs Server-Sent Events for server-to-client messages and HTTP POST for client-to-server communications.\\n\\nAll transport mechanisms leverage JSON-RPC 2.0 for message exchanges. Refer to the specification for detailed information about the Model Context Protocol message format.\\nMessage Types 📨\\nMCP defines several key message types:\\n\\nRequests: Expect a response from the receiving side.\\nResults: Indicate successful responses to requests.\\nErrors: Indicate that a request has failed.\\n\\n🚀How MCP Works: A Technical Deep Dive\\n1. Context Window Management\\nMCP uses a dynamic context window that grows with each interaction, storing:\\n\\nUser preferences (e.g., language, tone).\\nConversation history (prior queries/responses).\\nEnvironmental data (e.g., device type, location 🌍).\\n\\n2. Context Embedding & Compression\\nTo avoid overload, MCP compresses non-critical data into embeddings (e.g., summarizing a 10-message chat into a intent vector 🔢) while retaining key details.\\n3. Stateful Workflows\\nMCP enables multi-step workflows where agents:\\n\\nRemember past actions (e.g., “User already uploaded their ID”).\\nAdapt strategies (e.g., switching from email to SMS if the user is offline 📴).\\nSelf-correct using feedback (e.g., “User disliked Option A; prioritize Option B”).\\n\\n# Hypothetical MCP stateful workflow (from official docs)  \\nclass TravelAgent(MCPAgent):  \\n    def __init__(self, user_id):  \\n        self.context = load_context(user_id)  # Load past interactions  \\n        self.preferences = self.context.get(\"preferences\", {})    \\ndef book\\\\_flight(self, query):    \\n    if self.preferences.get(\"class\") == \"economy\":    \\n        return search\\\\_flights(query, budget=True)    \\n    else:    \\n        return search\\\\_flights(query)\\n\\nWhy Not Just Give the LLM Access to the API? 🤔\\nA common question regarding the Model Context Protocol (MCP) is: “Why do we need a custom protocol? Can’t LLMs simply learn to use APIs on their own?”\\nIn theory, the answer may be affirmative. Most public APIs come with documentation that outlines their functionalities. One could provide this documentation to an LLM, enabling it to derive the necessary steps to achieve its objectives.\\nHowever, in practice, this method is often inefficient. As developers focused on enhancing user experience, it’s essential to prioritize speed and responsiveness. By presenting tools to the LLM in an easily consumable format, we significantly reduce latency and streamline the overall process. This approach ensures a smoother interaction and quicker results for users. 🏎️💨\\n🆚 MCP vs. Traditional API Calls: A Game Changer\\n\\nWhy the shift? APIs are like snapshots 📸 — great for static tasks. MCP is a video 🎥, capturing the full narrative of user intent.\\nIsn’t This Just Tool Calling? 🤔🔧\\nA question I often encounter regarding the Model Context Protocol (MCP) is, “How does this differ from tool calling?”\\nTool calling refers to the mechanism by which LLMs invoke functions to perform tasks in the real world. In this setup, the LLM operates alongside a tool executor that calls the specified tools and returns the results. The typical process looks like this:\\n\\n📝 Describe Tool to Be Called\\n📤 Send Result\\n🤖 LLM\\n⚙️ Tool Executor\\n\\n\\nFrom Source\\nHowever, this interaction typically occurs within the same environment, whether that be a single server or a specific desktop application.\\nIn contrast, the MCP framework enables LLMs to access tools from a separate process, which can be either local or hosted on a remote server. The structure is as follows:\\n\\n🌐 MCP Server\\n🖥️ MCP Client\\n📝 Describe Tool to Be Called\\n🔧 Call Tool\\n📤 Send Result\\n🔄 Return Result\\n🤖 LLM\\n📜 MCP Protocol\\n⚙️ Tool Executor\\n\\n\\nFrom Source\\nThe key distinction lies in the complete decoupling of the MCP server from the client. This separation offers greater flexibility and scalability, enhancing how LLMs interact with external tools. 🌐✨\\n🌟 Why MCP is Revolutionary for Agentic AI\\nAgentic frameworks require AI to act autonomously, not just respond. MCP’s importance lies in:\\n1. Enabling True Autonomy\\nAgents can now:\\n\\nMake decisions based on historical data (e.g., a healthcare agent 🏥 recalling a patient’s allergy list).\\nChain tasks without human intervention (e.g., “Research → Draft → Edit → Publish” blog posts ✍️).\\n\\n2. Collaborative Intelligence\\nMCP allows agents to share context with:\\n\\nOther agents (e.g., a customer service bot 🤖 escalating to a human agent 👩💻).\\nExternal tools (e.g., pulling real-time stock data 📈 into a financial advisor’s response).\\n\\n3. Ethical Guardrails\\n\\nAuditability: Full context history helps trace biased/inaccurate outputs.\\nPrivacy: Sensitive data (e.g., medical records) is compartmentalized 🔒.\\n\\nWithout MCP, agents would lack continuity — like a chef 🧑🍳 forgetting recipe steps midway!\\n4. Enables Long-Term Autonomy\\n\\nPersistent Memory: Agents remember user preferences (e.g., “Alex hates spam emails 📧”).\\nGoal Chaining: Execute multi-step tasks (e.g., Research → Negotiate → Book a business trip ✈️)\\n\\nConnection Lifecycle 🔄\\nThe connection lifecycle within MPI is essential for managing the states and transitions of interactions between clients and servers, ensuring robust communication and functionality throughout the process.\\nThis structured approach to components within the MCP provides a clear framework for efficient communication and integration, empowering LLM applications to thrive. 🌟✨\\n\\n1. Initialization 🚀\\nDuring the initialization phase, the following steps occur between the server and client:\\n\\nClient sends an initialize request that includes the protocol version and its capabilities.\\nServer responds with its own protocol version and capabilities.\\nClient sends an initialized notification to acknowledge successful connection establishment.\\nThe connection is now ready for use, and normal message exchange begins.\\n\\n2. Message Exchange 📨\\nAfter the initialization phase, the MCP supports the following communication patterns:\\n\\nRequest-Response: Either the client or server can send requests, to which the other party will respond.\\nNotifications: Either side may send one-way messages without expecting a response.\\n\\n3. Termination 🛑\\nThe connection can be terminated by either party, which can occur in several ways:\\n\\nClean Shutdown: Achieved through the close() method.\\nTransport Disconnection: Occurs when communication channels are lost.\\nError Conditions: Termination may also result from encountering errors.\\n\\nError Handling ❗\\nMCP defines a set of standard error codes to effectively manage issues that may arise:\\nenum ErrorCode {\\n  // Standard JSON-RPC error codes\\n  ParseError = -32700,\\n  InvalidRequest = -32600,\\n  MethodNotFound = -32601,\\n  InvalidParams = -32602,\\n  InternalError = -32603\\n}\\nAdditionally, SDKs and applications can define their own custom error codes starting from -32000.\\nError Propagation:\\nErrors are communicated through:\\n\\nError Responses: Returned in response to requests with issues.\\nError Events: Triggered on transports to notify of errors.\\nProtocol-Level Error Handlers: Manage errors at the MCP level.\\n\\nThis structured lifecycle ensures robust and efficient communication between clients and servers while gracefully handling errors as they arise. 🌐💼\\n🏗️ Code Implementation\\nThe implementation showcases a sophisticated multi-server setup that handles different types of operations through various transport protocols. Here’s a visual representation of the system:\\n\\nInstall required Dependencies\\npip install mcp httpx langchain langchain-core langchai-community langchain-groq langchain-ollama langchain_mcp_adapters\\nSet up groq api key in .env file\\nimport os\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nCreate Servers\\nMath Server\\n# math_server.py\\nfrom mcp.server.fastmcp import FastMCP  \\nmcp = FastMCP(\"Math\")  \\n@mcp.tool()\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Add two numbers\"\"\"\\n    return a + b  \\n@mcp.tool()\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply two numbers\"\"\"\\n    return a * b  \\nif __name__ == \"__main__\":\\n    mcp.run(transport=\"stdio\")\\nWeather Server(weather.py)\\nfrom typing import Any\\nimport httpx\\nfrom mcp.server.fastmcp import FastMCP  \\n# Initialize FastMCP server\\nmcp = FastMCP(\"weather\")  \\n# Constants\\nNWS_API_BASE = \"https://api.weather.gov\"\\nUSER_AGENT = \"weather-app/1.0\"  \\nasync def make_nws_request(url: str) -> dict[str, Any] | None:\\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\\n    headers = {\\n        \"User-Agent\": USER_AGENT,\\n        \"Accept\": \"application/geo+json\"\\n    }\\n    async with httpx.AsyncClient() as client:\\n        try:\\n            response = await client.get(url, headers=headers, timeout=30.0)\\n            response.raise_for_status()\\n            return response.json()\\n        except Exception:\\n            return None  \\ndef format_alert(feature: dict) -> str:\\n    \"\"\"Format an alert feature into a readable string.\"\"\"\\n    props = feature[\"properties\"]\\n    return f\"\"\"\\nEvent: {props.get(\\'event\\', \\'Unknown\\')}\\nArea: {props.get(\\'areaDesc\\', \\'Unknown\\')}\\nSeverity: {props.get(\\'severity\\', \\'Unknown\\')}\\nDescription: {props.get(\\'description\\', \\'No description available\\')}\\nInstructions: {props.get(\\'instruction\\', \\'No specific instructions provided\\')}\\n\"\"\"  \\n@mcp.tool()\\nasync def get_alerts(state: str) -> str:\\n    \"\"\"Get weather alerts for a US state.  \\nArgs:  \\n    state: Two-letter US state code (e.g. CA, NY)  \\n\"\"\"  \\nurl = f\"{NWS\\\\_API\\\\_BASE}/alerts/active/area/{state}\"  \\ndata = await make\\\\_nws\\\\_request(url)\\n\\nif not data or \"features\" not in data:  \\n    return \"Unable to fetch alerts or no alerts found.\"\\n\\nif not data\\\\[\"features\"\\\\]:  \\n    return \"No active alerts for this state.\"\\n\\nalerts = \\\\[format\\\\_alert(feature) for feature in data\\\\[\"features\"\\\\]\\\\]  \\nreturn \"\\\\\\\\n---\\\\\\\\n\".join(alerts)\\n\\n@mcp.tool()\\nasync def get_forecast(latitude: float, longitude: float) -> str:\\n    \"\"\"Get weather forecast for a location.  \\nArgs:  \\n    latitude: Latitude of the location  \\n    longitude: Longitude of the location  \\n\"\"\"  \\n\\\\# First get the forecast grid endpoint  \\npoints\\\\_url = f\"{NWS\\\\_API\\\\_BASE}/points/{latitude},{longitude}\"  \\npoints\\\\_data = await make\\\\_nws\\\\_request(points\\\\_url)\\n\\nif not points\\\\_data:  \\n    return \"Unable to fetch forecast data for this location.\"\\n\\n\\\\# Get the forecast URL from the points response  \\nforecast\\\\_url = points\\\\_data\\\\[\"properties\"\\\\]\\\\[\"forecast\"\\\\]  \\nforecast\\\\_data = await make\\\\_nws\\\\_request(forecast\\\\_url)\\n\\nif not forecast\\\\_data:  \\n    return \"Unable to fetch detailed forecast.\"\\n\\n\\\\# Format the periods into a readable forecast  \\nperiods = forecast\\\\_data\\\\[\"properties\"\\\\]\\\\[\"periods\"\\\\]  \\nforecasts = \\\\[\\\\]  \\nfor period in periods\\\\[:5\\\\]:  \\\\# Only show next 5 periods  \\n    forecast = f\"\"\"\\n\\n{period[\\'name\\']}:\\nTemperature: {period[\\'temperature\\']}°{period[\\'temperatureUnit\\']}\\nWind: {period[\\'windSpeed\\']} {period[\\'windDirection\\']}\\nForecast: {period[\\'detailedForecast\\']}\\n\"\"\"\\n        forecasts.append(forecast)  \\nreturn \"\\\\\\\\n---\\\\\\\\n\".join(forecasts)\\n\\nif __name__ == \"__main__\":\\n    # Initialize and run the server\\n    mcp.run(transport=\\'sse\\')\\nCreate client\\nlangchain_mcp_multiserver.py\\nimport asyncio\\nfrom mcp import ClientSession, StdioServerParameters\\nfrom mcp.client.stdio import stdio_client\\nfrom IPython.display import display, Markdown\\nfrom langchain_core.messages import HumanMessage, ToolMessage, AIMessage\\nfrom langchain_mcp_adapters.tools import load_mcp_tools\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\\nfrom langgraph.prebuilt import create_react_agent  \\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\\nfrom langchain_groq import ChatGroq\\nfrom langchain_ollama import ChatOllama\\nfrom langchain_core.prompts import PromptTemplate\\nfrom dotenv import load_dotenv  \\nload_dotenv()  \\nmodel = ChatGroq(model=\"llama-3.3-70b-versatile\",temperature=0.5)\\nmodel = ChatOllama(model=\"llama3.2:1b\",temperature=0.0,max_new_tokens=500)\\nserver_params = StdioServerParameters(\\n    command=\"python\",\\n    # Make sure to update to the full absolute path to your math_server.py file\\n    args=[\"weather.py\"],\\n)\\nasync def run_app(user_question):  \\nasync with MultiServerMCPClient(  \\n    {  \\n        \"weather\": {  \\n            \"url\": \"http://localhost:8000/sse\",  \\n            \"transport\": \"sse\",  \\n        },  \\n        \"math\": {  \\n        \"command\": \"python\",  \\n        \\\\# Make sure to update to the full absolute path to your math\\\\_server.py file  \\n        \"args\": \\\\[\"math\\\\_server.py\"\\\\],  \\n        \"transport\": \"stdio\",  \\n        },  \\n    }  \\n) as client:  \\n    agent = create\\\\_react\\\\_agent(model, client.get\\\\_tools())  \\n    agent\\\\_response = await agent.ainvoke({\"messages\": user\\\\_question})  \\n    print(agent\\\\_response\\\\[\\'messages\\'\\\\]\\\\[-1\\\\].content)  \\n    \\\\# # Stream the response chunks  \\n    \\\\# async for chunk in agent.astream({\"messages\": user\\\\_question}):  \\n    \\\\#     # Extract the message content from the AddableUpdatesDict structure  \\n    \\\\#     if \\'agent\\' in chunk and \\'messages\\' in chunk\\\\[\\'agent\\'\\\\]:  \\n    \\\\#         for message in chunk\\\\[\\'agent\\'\\\\]\\\\[\\'messages\\'\\\\]:  \\n    \\\\#             if isinstance(message, AIMessage):  \\n    \\\\#                 # Handle different content formats  \\n    \\\\#                 if isinstance(message.content, list):  \\n    \\\\#                     # For structured content with text and tool use  \\n    \\\\#                     for item in message.content:  \\n    \\\\#                         if isinstance(item, dict) and \\'text\\' in item:  \\n    \\\\#                             print(f\"\\\\*\\\\*AI\\\\*\\\\*: {item\\\\[\\'text\\'\\\\]}\")  \\n    \\\\#                 else:  \\n    \\\\#                     # For simple text content  \\n    \\\\#                     print(f\"\\\\*\\\\*AI\\\\*\\\\*: {message.content}\")\\n\\n    \\\\#     elif \\'tools\\' in chunk and \\'messages\\' in chunk\\\\[\\'tools\\'\\\\]:  \\n    \\\\#         for message in chunk\\\\[\\'tools\\'\\\\]\\\\[\\'messages\\'\\\\]:  \\n    \\\\#             if hasattr(message, \\'name\\') and hasattr(message, \\'content\\'):  \\n    \\\\#                 # Display tool response  \\n    \\\\#                 print(f\"\\\\*\\\\*Tool ({message.name})\\\\*\\\\*: {message.content}\")  \\n    return agent\\\\_response\\\\[\\'messages\\'\\\\]\\\\[-1\\\\].content\\n\\nif __name__ == \"__main__\":\\n    #user_question = \"what is the weather in california?\"\\n    #user_question = \"what\\'s (3 + 5) x 12?\"\\n    #user_question = \"what\\'s the weather in seattle?\"\\n    user_question = \"what\\'s the weather in NYC?\"\\n    response = asyncio.run(run_app(user_question=user_question))\\n    print(response)             \\nPrior to invoking the client make sure that the weather server is up and running\\npython weather.py\\nInvoke the client\\npython langchain_mcp_multiserver.py\\nResponse: “what’s (3 + 5) x 12?”\\nThe result of (3 + 5) is 8, and 8 x 12 is 96.  \\nResposne : “what’s the weather in NYC?”\\nIt appears you\\'ve provided a list of weather alerts from the National Weather Service (NWS) for various regions in New York State, Vermont, and parts of Massachusetts.  \\nHere\\'s a breakdown of what each alert is saying:  \\n**Flooding Alerts**  \\n* The NWS has issued several flood watches across New York State, including:\\n        + Northern St. Lawrence; Northern Franklin; Eastern Clinton; Southeastern St. Lawrence; Southern Franklin; Western Clinton; Western Essex; Southwestern St. Lawrence; Grand Isle; Western Franklin; Orleans; Essex; Western Chittenden; Lamoille; Caledonia; Washington; Western Addison; Orange; Western Rutland; Eastern Franklin; Eastern Chittenden; Eastern Addison; Eastern Rutland; Western Windsor; Eastern Windsor\\n        + Northern Herkimer; Hamilton; Southern Herkimer; Southern Fulton; Montgomery; Northern Saratoga; Northern Warren; Northern Washington; Northern Fulton; Southeast Warren; Southern Washington; Bennington; Western Windham; Eastern Windham\\n* The NWS has also issued a flood watch for parts of Vermont, including:\\n        + Northern New York and northern and central Vermont  \\n**Ice Jam Alerts**  \\n* The NWS has warned about the possibility of ice jams in several areas, including:\\n        + Bennington; Western Windham; Eastern Windham\\n        + Southern Vermont, Bennington and Windham Counties\\n        + Central New York, Herkimer County\\n        + Northern New York, Hamilton, Montgomery, Fulton, Herkimer, Warren, Washington Counties  \\n**Other Alerts**  \\n* The NWS has issued several warnings about heavy rainfall and snowmelt leading to minor river flooding.\\n* There are also alerts for isolated ice jams that could further increase the flood risk.  \\nIt\\'s essential to stay informed about weather conditions in your area and follow the instructions of local authorities. If you\\'re planning outdoor activities, be prepared for changing weather conditions and take necessary precautions to stay safe.\\nIt appears you\\'ve provided a list of weather alerts from the National Weather Service (NWS) for various regions in New York State, Vermont, and parts of Massachusetts.  \\nHere\\'s a breakdown of what each alert is saying:  \\n**Flooding Alerts**  \\n* The NWS has issued several flood watches across New York State, including:\\n        + Northern St. Lawrence; Northern Franklin; Eastern Clinton; Southeastern St. Lawrence; Southern Franklin; Western Clinton; Western Essex; Southwestern St. Lawrence; Grand Isle; Western Franklin; Orleans; Essex; Western Chittenden; Lamoille; Caledonia; Washington; Western Addison; Orange; Western Rutland; Eastern Franklin; Eastern Chittenden; Eastern Addison; Eastern Rutland; Western Windsor; Eastern Windsor\\n        + Northern Herkimer; Hamilton; Southern Herkimer; Southern Fulton; Montgomery; Northern Saratoga; Northern Warren; Northern Washington; Northern Fulton; Southeast Warren; Southern Washington; Bennington; Western Windham; Eastern Windham\\n* The NWS has also issued a flood watch for parts of Vermont, including:\\n        + Northern New York and northern and central Vermont  \\n**Ice Jam Alerts**  \\n* The NWS has warned about the possibility of ice jams in several areas, including:\\n        + Bennington; Western Windham; Eastern Windham\\n        + Southern Vermont, Bennington and Windham Counties\\n        + Central New York, Herkimer County\\n        + Northern New York, Hamilton, Montgomery, Fulton, Herkimer, Warren, Washington Counties  \\n**Other Alerts**  \\n* The NWS has issued several warnings about heavy rainfall and snowmelt leading to minor river flooding.\\n* There are also alerts for isolated ice jams that could further increase the flood risk.  \\nIt\\'s essential to stay informed about weather conditions in your area and follow the instructions of local authorities. If you\\'re planning outdoor activities, be prepared for changing weather conditions and take necessary precautions to stay safe.\\nNote here the MCP Client is able to make connections to respective servers based on the question asked. We did not explicitly mention any routing logic.\\n🚀 The Future of MCP\\nPer the official docs, upcoming features include:\\n\\nCross-Platform Context Sync: Unify context across apps (e.g., your email ✉️, Slack 💬, and CRM 📊).\\nContext-Aware Security: Dynamically adjust permissions based on user behavior.\\nSelf-Optimization: Agents refine their own context management rules 🔄.\\n\\n💡 Key Takeaways\\n\\nMCP > APIs for complex, evolving tasks.\\nCritical for Agents 🤖 to act proactively, not just reactively.\\nBalances Power & Safety via auditable, compartmentalized context.\\n\\n🎯 Use Cases\\nThis architecture is particularly useful for:\\n\\nMulti-modal AI applications\\nComplex workflow orchestration\\nDistributed AI systems\\nReal-time data processing applications\\n\\n📝 Conclusion\\nThe Model Context Protocol implementation demonstrated here showcases a robust, scalable, and flexible architecture for building sophisticated AI applications. Its modular design and support for multiple transport protocols make it an excellent choice for complex AI systems.\\nReferences\\nGitHub - modelcontextprotocol/python-sdk: The official Python SDK for Model Context Protocol… --------------------------------------------------------------------------------------------- ### The official Python SDK for Model Context Protocol servers and clients - modelcontextprotocol/python-sdk github.com\\nIntroduction - Model Context Protocol ------------------------------------- ### Get started with the Model Context Protocol (MCP) modelcontextprotocol.io\\nGitHub - langchain-ai/langchain-mcp-adapters -------------------------------------------- ### Contribute to langchain-ai/langchain-mcp-adapters development by creating an account on GitHub. github.com\\nconnect with me\\nNote: This experimentation as conducted by following available online resources.\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to audio narrations\\nRead offline with the Medium app\\nTry for $5/month\\nModel Context Protocol\\n\\n105\\n\\n105\\n\\n\\n\\nFollow\\nPublished in The AI Forum -------------------------\\n773 Followers\\n·Last published 2 days ago\\nIts AI forum where all the topics spread across Data Analytics, Data Science, Machine Learning, Deep Learning are discussed.\\nFollow\\n\\n\\nFollow\\nWritten by Plaban Nayak -----------------------\\n4.7K Followers\\n·391 Following\\nMachine Learning and Deep Learning enthusiast\\nFollow\\nNo responses yet\\n\\n\\nWrite a response\\nWhat are your thoughts?\\nCancel\\nRespond\\nAlso publish to my profile\\nMore from Plaban Nayak and The AI Forum\\n\\n\\nIn\\nNerd For Tech\\nby\\nPlaban Nayak\\nFixing RAG with Reasoning Augmented Generation ---------------------------------------------- ### 🛠️ Why RAG is Broken — And How ReAG Fixes It\\nFeb 16\\n297 5\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nSemantic Chunking for RAG ------------------------- ### What is Chunking ?\\nApr 21, 2024\\n650 8\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nWhich Vector Database Should You Use? Choosing the Best One for Your Needs -------------------------------------------------------------------------- ### Introduction\\nApr 19, 2024\\n654 10\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nBuild a Reliable RAG Agent using LangGraph ------------------------------------------ ### Introduction\\nMay 4, 2024\\n776 10\\n\\nSee all from Plaban Nayak\\nSee all from The AI Forum\\nRecommended from Medium\\n\\n\\nIn\\nData Science Collective\\nby\\nBuse Şenol\\nModel Context Protocol (MCP): An End-To-End Tutorial With Hands-On Project with Python -------------------------------------------------------------------------------------- ### What is MCP? How to create an MPC Server that brings news from a web site with Claude Desktop?\\nMar 18\\n288 6\\n\\n\\n\\nIn\\nGenerative AI\\nby\\nSimranjeet Singh\\nMCP Servers [Explained] Python and Agentic AI Tool Integration ---------------------------------------------------------------- ### Explore MCP servers by Anthropic for AI integration, enabling seamless data connectivity with GitHub, Google Drive, and enterprise tools.\\nMar 18\\n227 2\\n\\n\\n\\nIn\\nAI Advances\\nby\\nTechscribe Central\\nModel Context Protocol (MCP): The AI Integration Breakthrough You Need to Know ------------------------------------------------------------------------------ ### See how MCP is reshaping AI’s ability to interact with external data sources.\\nMar 18\\n209 2\\n\\n\\n\\nIn\\nEveryday AI\\nby\\nManpreet Singh\\nSimplified Guide for Model Context Protocol (MCP) ------------------------------------------------- ### The simplest guide related to MCP\\n5d ago\\n208 6\\n\\n\\n\\nIn\\nLevel Up Coding\\nby\\nUjjwal Jindal\\nAPI-First AI Agentic Patterns: Building Smarter Systems Without the Framework Overhead -------------------------------------------------------------------------------------- ### We’ve worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple…\\nMar 19\\n451 3\\n\\n\\n\\nIn\\nData And Beyond\\nby\\nTONI RAMCHANDANI\\nThe Model Context Protocol (MCP): The Ultimate Guide ---------------------------------------------------- ### Introduction to the Model Context Protocol (MCP)\\nMar 10\\n246 6\\n\\nSee more recommendations\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nRules\\nTerms\\nText to speech'}, {'url': 'https://opencv.org/blog/model-context-protocol/', 'title': 'A beginners Guide on Model Context Protocol (MCP) - OpenCV', 'content': 'The Model Context Protocol (MCP) is a universal standard that enables seamless interaction between AI-powered applications and external data sources. It works by establishing a secure and efficient client-server architecture, where AI systems (clients) request relevant context from data repositories or tools (servers). MCP eliminates the need for fragmented integrations by providing a standardized framework for accessing real-time context, such as files, databases, or APIs. Through this [...] Source\\nMCP: Architecture & Key Roles\\nThe Model Context Protocol (MCP) is designed to bridge AI applications (or “agents”) with external systems, data sources, and tools, all while maintaining secure boundaries. At its core, MCP adopts a client–host–server pattern, aiming to standardize how different components communicate and share “context.” This approach is built atop JSON-RPC and emphasizes stateful sessions that coordinate context exchange and sampling.\\n\\nSource\\n\\n\\nHost Process', 'score': 0.9158166, 'raw_content': 'Published Time: 2025-04-03T17:27:55+00:00\\nA beginners Guide on Model Context Protocol (MCP)\\n\\nSkip to primary navigation\\nSkip to main content\\n\\nOpenCV\\nOpen Computer Vision Library\\n\\nLibrary\\nReleases\\nPlatforms\\nLicense\\n\\n\\nForum\\nOpenCV Forum (New!)\\nAskbot (Old Forum)\\n\\n\\nOpenCV University\\nAll Courses\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\n\\nCVDL Master Program\\nCourses [Enrolled Users]\\nFor Organizations\\nStudent Discount\\nCareerX\\n\\n\\nFree Courses\\nPyTorch Bootcamp NEW\\nTensorFlow Keras Course FREE\\nOpenCV Course FREE\\nPython for Beginners FREE\\n\\n\\nServices\\nFace Recognition\\nContribute\\nArea Chairs\\nMembership\\nPlatinum\\nGold\\n\\n\\nDevelopment Partnership\\nCourse Partnership\\n\\n\\nResources\\nNews\\nResearch Papers\\nGet Started\\nPodcast\\nTools\\nRoboflow\\n\\n\\nLinks\\nMedia Kit\\nLeadership\\nAbout\\n\\n\\n\\n\\nSearch for:Search Button\\n\\n\\nMCP – A Beginner’s Guide\\nHome / AI Careers / MCP – A Beginner’s Guide\\nMCP – A Beginner’s Guide\\nsandeep April 3, 2025 AI Careers Tags: artificial intelligence MCP\\nImagine you’re using an AI writing assistant to draft an email. It’s excellent at creating clear and concise text, but you need the email to reference specific updates from a recent team meeting and attach a relevant report. While the assistant is impressive in crafting polished content, it falters when accessing real-time context, like your meeting notes or files. Without direct access to this information, you’re left to manually provide the details—defeating the purpose of seamless assistance.\\nThis limitation underscores a broader challenge with large language models (LLMs). While they are remarkable at processing vast amounts of data, they struggle with real-time context outside their frozen training set. For AI agents to truly thrive, they need more than raw intelligence; they require dynamic access to files, tools, knowledge bases, and the ability to act on this information—like automating tasks or updating documents. Historically, integrating models with external sources was a messy process, relying on fragile custom implementations that were hard to scale.\\nEnter the Model Context Protocol (MCP). Introduced by Anthropic in late 2024, MCP is a groundbreaking open standard designed to bridge the gap between AI assistants and the data-rich ecosystems they need to navigate. MCP eliminates patchy integrations by providing a universal framework for connecting tools and data sources, empowering AI systems with seamless access to diverse contexts. With MCP gaining traction among major AI players and open-source communities. It’s clear—MCP is paving the way for the future of truly connected and agentic AI.\\nThe Rise of the Model Context Protocol (MCP)\\nIn November 2024, Anthropic introduced the Model Context Protocol (MCP). At first, it was overshadowed by discussions on advanced language models, but by early 2025, MCP took center stage. Teams realized building “intelligent agents” was simpler than connecting them to real-world data, making MCP’s approach a long-overdue solution.\\nToday, Block (Square), Apollo, Zed, Replit, Codeium, Sourcegraph, and others have implemented MCP. Over 1,000 open-source connectors emerged by February 2025, expanding its ecosystem with each addition. Anthropic has continued refining MCP’s specification, documentation, and workshops, speeding up adoption. By remaining open, model-agnostic, and reminiscent of open standards (like USB or HTTP), MCP eliminates barriers to broad collaboration.\\nWhy MCP Matters\\nEnthusiasm for MCP isn’t just about novelty; it’s about solving real pain points in machine learning and AI deployment. Here’s a closer look at the core benefits.\\n1. Reproducibility\\n\\nComplete Model Context: All the necessary details—datasets, environment specifications, and hyperparameters- live in one place. All the relevant data is fetched\\xa0from these sources and fed as context to the LLM for a better grounded answer\\n\\n2. Standardization & Collaboration\\n\\n\\nInter-organizational sharing: Many companies build specialized AI tools or maintain custom data sources. If a model is packaged with an MCP file, another organization can plug it into their own system with fewer roadblocks.  \\n\\n\\nOpen-Source Integration: Communities like Hugging Face or GitHub rely on consistent metadata standards to streamline how models are shared and discovered. MCP’s open nature and model-agnostic design make it an ideal candidate to unify these ecosystems.\\n\\n\\nHow MCP Works\\nThe Model Context Protocol (MCP) is a universal standard that enables seamless interaction between AI-powered applications and external data sources. It works by establishing a secure and efficient client-server architecture, where AI systems (clients) request relevant context from data repositories or tools (servers). MCP eliminates the need for fragmented integrations by providing a standardized framework for accessing real-time context, such as files, databases, or APIs. Through this protocol, AI assistants can not only retrieve information but also take meaningful actions, such as updating documents or automating workflows – bridging the gap between isolated intelligence and dynamic, context-aware functionality.\\n\\nSource\\nMCP: Architecture & Key Roles\\nThe Model Context Protocol (MCP) is designed to bridge AI applications (or “agents”) with external systems, data sources, and tools, all while maintaining secure boundaries. At its core, MCP adopts a client–host–server pattern, aiming to standardize how different components communicate and share “context.” This approach is built atop JSON-RPC and emphasizes stateful sessions that coordinate context exchange and sampling.\\n\\nSource\\n\\n\\nHost Process\\n\\n\\nActs as a “container” or coordinator for multiple client instances.  \\n\\n\\nManages lifecycle and security policies, e.g., permissions, user authorization, and enforcement of consent requirements.  \\n\\n\\nOversees how AI or language model integration happens within each client, gathering and merging context as needed.\\n\\n\\n\\n\\nIn plain terms, the host is like the control tower for incoming AI “flights.” It decides which AI client gets to fly, who checks them in, and which runways (servers) they connect to.\\n\\n\\nClient Instances\\n\\n\\nEach client runs inside the host. It establishes a one-to-one session with a specific MCP server using a stateful connection.  \\n\\n\\nHandles capability negotiation and orchestrates messages between itself and the server.  \\n\\n\\nMaintains security boundaries—one client shouldn’t snoop on resources belonging to another client.  \\n\\n\\nTypically, these clients represent AI applications or agents (like Anthropic’s Curser or Windsurf) that can “talk” to servers using the MCP’s standardized interfaces.\\n\\n\\n\\n\\nThink of the client as an AI agent wanting to access external tools or data. Once it’s “MCP-compatible,” it can easily connect to any MCP server—no extra glue code is needed.\\n\\n\\nServers\\n\\n\\nProvide specialized capabilities or resources in a standardized way so that any MCP client can use them.  \\n\\n\\nIt could be a local process or a remote service, each wrapping data sources, APIs, or other utilities (like CRMs, Git repos, or file systems).  \\n\\n\\nDefine “tools,” “prompts,” and “resources” that the client can invoke or retrieve.  \\n\\n\\nMust adhere to security constraints and user permissions enforced by the host.\\n\\n\\n\\n\\nEssentially, servers are the AI ecosystem’s “toolboxes.” By exposing consistent interfaces, they solve the classic “N times M problem,” where you’d otherwise need separate integrations for every AI app and every data source. Build a server once, and any MCP-compatible client can tap into it.\\n\\nFor official specification details, see the MCP Architecture Reference (2024-11-05)\\nFeatures of MCP Server\\nWhen building MCP servers, you’re effectively creating the building blocks that add crucial context to language models. In MCP’s framework, these building blocks are referred to as “primitives” and include:\\n\\nPrompts\\nResources\\nTools\\n\\nThese primitives act as the interface between client-side AI applications (agents) and the specialized environments or data sources the server manages. By properly implementing each primitive, server developers enable rich interactions and enhanced capabilities for language models.\\nPrimitives and Their Control Hierarchy\\nPrimitiveControlDescriptionExamplePromptsUser-controlledPre-defined templates or instructions triggered by user actionsSlash commands, menu selectionsResourcesApplication-controlledContextual data attached and managed by the clientFile contents, Git history logsToolsModel-controlledExecutable functions that the LLM can decide to invokeAPI calls, file writing operations\\nUse Cases\\n1. AI Assistant Managing Calendars\\nImagine you have an AI assistant—let’s call it Claude Desktop—that needs to schedule a meeting. Under the hood, Claude Desktop serves as an MCP host, which relies on an MCP client to connect with a calendar service. The calendar service itself runs as an MCP server. Here’s the workflow:\\n\\n\\nAI Request: The assistant sends a request (e.g., to retrieve free time slots).  \\n\\n\\nServer Response: The MCP server fetches the user’s calendar data and returns it to the assistant.  \\n\\n\\nAI Output: With those available slots in hand, the AI crafts a suitable response or automatically schedules the meeting.  \\n\\n\\n2. Secure AI in Healthcare\\nA medical AI system must securely access patient records. Leveraging MCP:\\n\\n\\nConsent & Privacy: The server checks user permissions and enforces encryption and logging.  \\n\\n\\nData Sharing: The MCP client fetches patient records only after ensuring compliance with healthcare data regulations.  \\n\\n\\nAI Interaction: The AI interprets that data (e.g., to draft a consultation summary or schedule follow-ups).\\n\\n\\nBoth scenarios highlight MCP’s emphasis on user control, data privacy, tool safety, and LLM sampling controls—key pillars for trustworthy, real-world AI solutions.\\nRelated Article: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents\\nConclusion\\nThe Model Context Protocol (MCP) marks a significant leap forward in the journey toward seamless AI integration. By providing an open, standardized way to connect large language models to external tools and data sources, MCP tackles the longstanding challenges of interoperability, scaling, and security head-on.\\nAs AI systems mature, the ability to interface with dynamic, real-world data becomes ever more critical. MCP paves the way for robust, secure, and highly modular AI architectures—much like TCP/IP once did for computer networking. With its growing ecosystem of adopters, SDKs, and off-the-shelf connectors, MCP stands poised to become the go-to standard for building responsive, intelligent applications across virtually every domain.\\nRelated Posts\\n\\nAI Careers\\nAI Jobs in 2025: A Comprehensive GuideAugust 16, 2023\\n\\nAI Careers\\nIntroduction to Artificial Intelligence in 2025August 23, 2023\\n\\nAI Careers\\nHistory of AI: Unraveling the Epic Saga of Mind...August 30, 2023\\nBecome a Member\\nStay up to date on OpenCV and Computer Vision news\\nJoin our Newsletter \\xa0\\nFree Courses\\n\\nPyTorch Bootcamp\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\n\\nPython for Beginners\\n\\n\\nPyTorch Bootcamp\\n\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\nPython for Beginners\\n\\nCourses\\n\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\n\\nMastering Generative AI for Art\\n\\n\\nMastering OpenCV with Python\\n\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\nPartnership\\n\\nIntel, OpenCV’s Platinum Member\\nGold Membership\\nDevelopment Partnership\\nCUDA\\n\\nARM\\n\\n\\nIntel, OpenCV’s Platinum Member\\n\\nGold Membership\\nDevelopment Partnership\\nCUDA\\nARM\\n\\nResources\\n\\nNews\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\n\\nWeb Stories\\n\\n\\nNews\\n\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\nWeb Stories\\n\\nGeneral Link\\n\\nAbout\\nReleases\\n\\nLicense\\n\\n\\nAbout\\n\\nReleases\\nLicense\\n\\nCopyright © 2025, OpenCV team\\n\\nContact Us\\nTerms and Conditions\\n\\nPrivacy Policy\\n\\n\\nContact Us\\n\\nTerms and Conditions\\nPrivacy Policy\\n\\nFree Courses\\n\\nPyTorch Bootcamp\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\nPython for Beginners\\n\\nCourses\\n\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\nPartnership\\n\\nIntel, OpenCV’s Platinum Member\\nGold Membership\\nDevelopment Partnership\\nCUDA\\nARM\\n\\nResources\\n\\nNews\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\nWeb Stories\\n\\nGeneral Link\\n\\nAbout\\nReleases\\nLicense\\n\\nCopyright ©\\xa02025 , OpenCV team\\nContact Us | Privacy Policy | Terms & Conditions\\n\\nGet started with Computer Vision with our Free OpenCV Bootcamp\\nENROLL NOW'}, {'title': 'Model Context Protocol (MCP): The Hidden Architecture Behind ... - Medium', 'url': 'https://medium.com/@iamanraghuvanshi/model-context-protocol-mcp-the-hidden-architecture-behind-smarter-ai-systems-2f64f971ad0c', 'content': 'Model Context Protocol (MCP): The Hidden Architecture Behind Smarter AI Systems This is where the Model Context Protocol (MCP) enters the conversation. At its core, MCP (Model Context Protocol) is a design framework for delivering relevant, structured, and layered context to a machine learning model at the moment it’s generating output (inference). In more formal terms, MCP is the interface between intention and intelligence — the middle layer that takes structured system and user information, wraps it into a coherent prompt or data sequence, and feeds it to the model in a way that maximizes relevance, accuracy, and traceability. Without MCP: With MCP: Eventually, “prompt engineering” may become a subset of a larger Context Engineering discipline — centered around MCP principles. That’s where the Model Context Protocol (MCP) shines.', 'score': 0.9037116, 'raw_content': None}], 'response_time': 4.33}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I’ve gathered some initial information about the Model Context Protocol (MCP):\n",
      "\n",
      "• MCP is an open‑source standard introduced by Anthropic in late 2024 that allows AI assistants (or any “model clients”) to connect securely to external tools and data sources (“servers”) through a common, JSON‑RPC–based protocol.  \n",
      "• It follows a client–host–server architecture:  \n",
      "  – A host application (e.g., Claude Desktop, an IDE, or any AI‑enabled product) spins up one or more MCP clients.  \n",
      "  – Each client maintains a stateful session with exactly one MCP server that exposes “prompts, tools, and resources.”  \n",
      "• A capability‑negotiation step lets both sides declare what features they support (streaming, resource subscriptions, tool invocation, sampling, etc.).  \n",
      "• Transports are pluggable (stdio for local processes, HTTP + Server‑Sent Events for remote services, WebSocket, etc.), but all messages conform to JSON‑RPC 2.0 framing.  \n",
      "• The design goal is to replace a tangle of bespoke “LLM integrations” with a single, composable way to share context, enforce security boundaries, and coordinate tool calls—ultimately letting agents act on real‑world data with less glue code.  \n",
      "• Major early adopters include Block, Apollo, Zed, Replit, Codeium, and others; open‑source SDKs exist for Python and TypeScript, and LangChain released MCP adapters.\n",
      "\n",
      "Before I outline a research report for you, could you clarify a few things so the final product matches your needs?\n",
      "\n",
      "1. Depth & Audience  \n",
      "   • Do you want a developer‑focused, in‑depth technical write‑up (spec details, code snippets, transport diagrams), or a higher‑level briefing suitable for product managers / executives?  \n",
      "\n",
      "2. Primary Goals  \n",
      "   • Are you mainly interested in how MCP works, why it matters (benefits & limitations), implementation guidance, comparison to alternatives, or potential business use cases?  \n",
      "   • Any specific industry or application (e.g., healthcare compliance, enterprise search, coding agents)?\n",
      "\n",
      "3. Scope Constraints  \n",
      "   • How long or detailed should the report be?  \n",
      "   • Should we include hands‑on examples or keep it conceptual?\n",
      "\n",
      "4. Timing & Currency  \n",
      "   • Do you need the very latest roadmap items (spec revisions in 2025) or is the November 2024 draft sufficient?\n",
      "\n",
      "Let me know your preferences, and I’ll structure the report accordingly.\n"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Checkpointer\n",
    "checkpointer = MemorySaver()\n",
    "agent = supervisor_builder.compile(name=\"research_team\", checkpointer=checkpointer)\n",
    "\n",
    "# Run the agent\n",
    "thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "msg =  [{\"role\": \"user\", \"content\": \"What is model context protocol?\"}]\n",
    "# TODO: Why is response empty? Issue w/ invoke?\n",
    "response = agent.invoke({\"messages\": msg}, config=thread_config)\n",
    "for m in agent.get_state(thread_config).values['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is model context protocol?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_i7PMNtBPw5HMV44bry9kydrU)\n",
      " Call ID: call_i7PMNtBPw5HMV44bry9kydrU\n",
      "  Args:\n",
      "    query: Model Context Protocol architecture\n",
      "    search_depth: advanced\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{'query': 'Model Context Protocol architecture', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Specification - Model Context Protocol', 'url': 'https://spec.modelcontextprotocol.io/specification/draft/architecture/', 'content': 'Scroll to top\\nSpecification\\nSpecification (Draft)\\nArchitecture\\nArchitecture\\nThe Model Context Protocol (MCP) follows a client-host-server architecture where each host can run multiple client instances. This architecture enables users to integrate AI capabilities across applications while maintaining clear security boundaries and isolating concerns. Built on JSON-RPC, MCP provides a stateful session protocol focused on context exchange and sampling coordination between clients and servers. [...] Each message type follows the JSON-RPC 2.0 specification for structure and delivery semantics.\\nCapability Negotiation\\nThe Model Context Protocol uses a capability-based negotiation system where clients and servers explicitly declare their supported features during initialization. Capabilities determine which protocol features and primitives are available during a session.', 'score': 0.9471519, 'raw_content': 'Architecture – Model Context Protocol Specification\\n Specification (Draft) Specification (Latest) Resources\\nCTRL K\\nGitHub\\nCTRL K\\n\\n\\nSpecification\\n\\n\\nSpecification (Draft)\\n\\nArchitecture\\nCore Components\\nDesign Principles\\nMessage Types\\nCapability Negotiation\\n\\n\\n\\nBase Protocol\\n\\nTransports\\nMessages\\nLifecycle\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nSpecification (Latest)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nMessages\\nLifecycle\\nTransports\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nClaude\\n\\n\\n\\n\\nSchema ↗\\n\\nMore\\nUser Guide ↗\\nPython SDK ↗\\n\\nTypeScript SDK ↗\\n\\n\\nSpecification (Draft)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nTransports\\nMessages\\nLifecycle\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nSpecification (Latest)\\n\\nArchitecture\\n\\nBase Protocol\\n\\nMessages\\nLifecycle\\nTransports\\nVersioning\\n\\nUtilities\\n\\nPing\\nCancellation\\nProgress\\n\\n\\n\\n\\n\\nServer Features\\n\\nPrompts\\nResources\\nTools\\n\\nUtilities\\n\\nCompletion\\nLogging\\nPagination\\n\\n\\n\\n\\n\\nClient Features\\n\\nRoots\\nSampling\\n\\n\\n\\nContributions\\n\\n\\nRevisions\\n\\n2024-11-05 (Current)\\n\\n\\n\\n\\n\\nClaude\\n\\nSchema ↗\\nMore\\nUser Guide ↗\\nPython SDK ↗\\nTypeScript SDK ↗\\n\\nLightDark\\nOn this page\\n\\nCore Components\\nHost\\nClients\\nServers\\nDesign Principles\\nMessage Types\\nCapability Negotiation\\n\\nScroll to top\\nSpecification\\nSpecification (Draft)\\nArchitecture\\nArchitecture\\nThe Model Context Protocol (MCP) follows a client-host-server architecture where each host can run multiple client instances. This architecture enables users to integrate AI capabilities across applications while maintaining clear security boundaries and isolating concerns. Built on JSON-RPC, MCP provides a stateful session protocol focused on context exchange and sampling coordination between clients and servers.\\nCore Components\\ngraph LR\\n    subgraph \"Application Host Process\"\\n        H[Host]\\n        C1[Client 1]\\n        C2[Client 2]\\n        C3[Client 3]\\n        H --> C1\\n        H --> C2\\n        H --> C3\\n    end\\nsubgraph \"Local machine\"\\n    S1\\\\[Server 1\\n\\nFiles & Git]\\n        S2[Server 2\\nDatabase]\\n        R1[(\"Local\\nResource A\")]\\n        R2[(\"Local\\nResource B\")]\\n    C1 --\\\\> S1\\n    C2 --\\\\> S2\\n    S1 <\\\\--\\\\> R1\\n    S2 <\\\\--\\\\> R2\\nend\\n\\nsubgraph \"Internet\"\\n    S3\\\\[Server 3\\n\\nExternal APIs]\\n        R3[(\"Remote\\nResource C\")]\\n    C3 --\\\\> S3\\n    S3 <\\\\--\\\\> R3\\nend\\n\\nHost\\nThe host process acts as the container and coordinator:\\n\\nCreates and manages multiple client instances\\nControls client connection permissions and lifecycle\\nEnforces security policies and consent requirements\\nHandles user authorization decisions\\nCoordinates AI/LLM integration and sampling\\nManages context aggregation across clients\\n\\nClients\\nEach client is created by the host and maintains an isolated server connection:\\n\\nEstablishes one stateful session per server\\nHandles protocol negotiation and capability exchange\\nRoutes protocol messages bidirectionally\\nManages subscriptions and notifications\\nMaintains security boundaries between servers\\n\\nA host application creates and manages multiple clients, with each client having a 1:1 relationship with a particular server.\\nServers\\nServers provide specialized context and capabilities:\\n\\nExpose resources, tools and prompts via MCP primitives\\nOperate independently with focused responsibilities\\nRequest sampling through client interfaces\\nMust respect security constraints\\nCan be local processes or remote services\\n\\nDesign Principles\\nMCP is built on several key design principles that inform its architecture and implementation:\\n\\n\\nServers should be extremely easy to build\\n\\nHost applications handle complex orchestration responsibilities\\nServers focus on specific, well-defined capabilities\\nSimple interfaces minimize implementation overhead\\nClear separation enables maintainable code\\n\\nServers should be highly composable\\n\\n\\nEach server provides focused functionality in isolation\\n\\nMultiple servers can be combined seamlessly\\nShared protocol enables interoperability\\nModular design supports extensibility\\n\\nServers should not be able to read the whole conversation, nor “see into” other servers\\n\\n\\nServers receive only necessary contextual information\\n\\nFull conversation history stays with the host\\nEach server connection maintains isolation\\nCross-server interactions are controlled by the host\\nHost process enforces security boundaries\\n\\nFeatures can be added to servers and clients progressively\\n\\n\\nCore protocol provides minimal required functionality\\n\\nAdditional capabilities can be negotiated as needed\\nServers and clients evolve independently\\nProtocol designed for future extensibility\\nBackwards compatibility is maintained\\n\\n\\n\\nMessage Types\\nMCP defines three core message types based on JSON-RPC 2.0:\\n\\nRequests: Bidirectional messages with method and parameters expecting a response\\nResponses: Successful results or errors matching specific request IDs\\nNotifications: One-way messages requiring no response\\n\\nEach message type follows the JSON-RPC 2.0 specification for structure and delivery semantics.\\nCapability Negotiation\\nThe Model Context Protocol uses a capability-based negotiation system where clients and servers explicitly declare their supported features during initialization. Capabilities determine which protocol features and primitives are available during a session.\\n\\nServers declare capabilities like resource subscriptions, tool support, and prompt templates\\nClients declare capabilities like sampling support and notification handling\\nBoth parties must respect declared capabilities throughout the session\\nAdditional capabilities can be negotiated through extensions to the protocol\\n\\nsequenceDiagram\\n    participant Host\\n    participant Client\\n    participant Server\\nHost-\\\\>\\\\>+Client: Initialize client\\nClient-\\\\>\\\\>+Server: Initialize session with capabilities\\nServer--\\\\>\\\\>Client: Respond with supported capabilities\\n\\nNote over Host,Server: Active Session with Negotiated Features\\n\\nloop Client Requests\\n    Host-\\\\>\\\\>Client: User- or model-initiated action\\n    Client-\\\\>\\\\>Server: Request (tools/resources)\\n    Server--\\\\>\\\\>Client: Response\\n    Client--\\\\>\\\\>Host: Update UI or respond to model\\nend\\n\\nloop Server Requests\\n    Server-\\\\>\\\\>Client: Request (sampling)\\n    Client-\\\\>\\\\>Host: Forward to AI\\n    Host--\\\\>\\\\>Client: AI response\\n    Client--\\\\>\\\\>Server: Response\\nend\\n\\nloop Notifications\\n    Server--)Client: Resource updates\\n    Client--)Server: Status changes\\nend\\n\\nHost-\\\\>\\\\>Client: Terminate\\nClient-\\\\>\\\\>\\\\-Server: End session\\ndeactivate Server\\n\\nEach capability unlocks specific protocol features for use during the session. For example:\\n\\nImplemented server features must be advertised in the server’s capabilities\\nEmitting resource subscription notifications requires the server to declare subscription support\\nTool invocation requires the server to declare tool capabilities\\nSampling requires the client to declare support in its capabilities\\n\\nThis capability negotiation ensures clients and servers have a clear understanding of supported functionality while maintaining protocol extensibility.\\nPowered by Hextra'}, {'url': 'https://www.anthropic.com/news/model-context-protocol', 'title': 'Introducing the Model Context Protocol - Anthropic', 'content': \"The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\\nToday, we're introducing three major components of the Model Context Protocol for developers:\", 'score': 0.9247491, 'raw_content': 'Introducing the Model Context Protocol \\\\ Anthropic\\nSkip to main contentSkip to footer\\n\\nClaude\\nAPI\\nResearch\\nCommitments\\nLearn\\nNews\\nTry Claude\\nAnnouncements\\nIntroducing the Model Context Protocol\\nNov 25, 2024●3 min read\\n\\nToday, we\\'re open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.\\nAs AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.\\nMCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.\\nModel Context Protocol\\nThe Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\\nToday, we\\'re introducing three major components of the Model Context Protocol for developers:\\n\\nThe Model Context Protocol specification and SDKs\\nLocal MCP server support in the Claude Desktop apps\\nAn open-source repository of MCP servers\\n\\nClaude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, we’re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.\\nEarly adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platforms—enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts.\\n\"At Block, open source is more than a development model—it’s the foundation of our work and a commitment to creating technology that drives meaningful change and serves as a public good for all,” said Dhanji R. Prasanna, Chief Technology Officer at Block. “Open technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build agentic systems, which remove the burden of the mechanical so people can focus on the creative.”\\nInstead of maintaining separate connectors for each data source, developers can now build against a standard protocol. As the ecosystem matures, AI systems will maintain context as they move between different tools and datasets, replacing today\\'s fragmented integrations with a more sustainable architecture.\\nGetting started\\nDevelopers can start building and testing MCP connectors today. All Claude.ai plans support connecting MCP servers to the Claude Desktop app.\\nClaude for Work customers can begin testing MCP servers locally, connecting Claude to internal systems and datasets. We\\'ll soon provide developer toolkits for deploying remote production MCP servers that can serve your entire Claude for Work organization.\\nTo start building:\\n\\nInstall pre-built MCP servers through the Claude Desktop app\\nFollow our quickstart guide to build your first MCP server\\nContribute to our open-source repositories of connectors and implementations\\n\\nAn open community\\nWe’re committed to building MCP as a collaborative, open-source project and ecosystem, and we’re eager to hear your feedback. Whether you’re an AI tool developer, an enterprise looking to leverage existing data, or an early adopter exploring the frontier, we invite you to build the future of context-aware AI together.\\n\\n\\nProduct\\n\\nClaude overview\\nClaude team plan\\nClaude enterprise plan\\nDownload Claude apps\\nClaude.ai pricing plans\\nClaude.ai login\\n\\nAPI Platform\\n\\nAPI overview\\nDeveloper docs\\nPricing\\nConsole login\\n\\nResearch\\n\\nResearch overview\\nEconomic Index\\n\\nClaude models\\n\\nClaude 3.7 Sonnet\\nClaude 3.5 Haiku\\nClaude 3 Opus\\n\\nCommitments\\n\\nTransparency\\nResponsible scaling policy\\nSecurity and compliance\\n\\nSolutions\\n\\nCoding\\n\\nLearning resources\\n\\nNews\\nCustomer stories\\nEngineering at Anthropic\\n\\nCompany\\n\\nAbout us\\nCareers\\n\\nHelp and security\\n\\nStatus\\nAvailability\\nSupport center\\n\\nTerms and policies\\nPrivacy choices*   Privacy policy\\n*   Responsible disclosure policy\\n*   Terms of service - consumer\\n*   Terms of service - commercial\\n*   Usage policy\\n© 2025 Anthropic PBC\\n'}, {'title': 'Understanding Model Context Protocol: A Deep Dive into ... - Medium', 'url': 'https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd', 'content': 'Core Architecture of the Model Context Protocol (MCP) 🏗️🔗\\nUnderstanding the Connection Between Clients, Servers, and LLMs\\nThe Model Context Protocol (MCP) features a flexible and extensible architecture that facilitates seamless communication between LLM applications and integrations. This section outlines the core architectural components and concepts.\\nOverview 🔍\\nMCP operates on a client-server architecture where: [...] 📝 Conclusion\\nThe Model Context Protocol implementation demonstrated here showcases a robust, scalable, and flexible architecture for building sophisticated AI applications. Its modular design and support for multiple transport protocols make it an excellent choice for complex AI systems.\\nReferences [...] From Source\\nModel Context Protocol (MCP) is a stateful, context-preserving framework designed to power intelligent, multi-step interactions between humans and AI agents. Unlike traditional API calls that treat each request as an isolated event, MCP introduces a persistent, evolving context layer 🌐 that allows AI systems to retain memory, learn dynamically, and act autonomously over time.\\nAccording to modelcontextprotocol.io, MCP is built on three pillars:', 'score': 0.9159971, 'raw_content': 'Published Time: 2025-03-17T17:00:26.392Z\\n🚀 Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nHome\\nLibrary\\nYour listsSaved listsHighlightsReading history\\nStoriesStats\\nThe AI Forum ----------------\\nHomeAbout\\n·Follow publication\\n\\nIts AI forum where all the topics spread across Data Analytics, Data Science, Machine Learning, Deep Learning are discussed.\\nFollow publication\\n🚀 Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration\\n\\n\\nPlaban Nayak\\n·Follow\\nPublished in\\nThe AI Forum\\n·\\n14 min read\\n·\\nMar 17, 2025\\n\\n105\\n\\nListen\\nShare\\n\\n🌟 Introduction\\nIn the quest to harness the full capabilities of Large Language Models (LLMs), developers often encounter a significant challenge: accessing the myriad of unique APIs provided by popular tools like Slack, GitHub, or your own local filesystem. While LLMs can connect to these APIs using various tools, the process typically involves writing code to establish these connections.\\nFor those using desktop applications such as Cursor or Claude Desktop, the situation can be even more restrictive, as manual addition of new tools is not feasible.\\nImagine a world where you could seamlessly access a set of pre-built tools specifically designed to integrate with your existing desktop applications. Enter the Model Context Protocol (MCP), a revolutionary approach that allows you to create customizable toolsets that plug directly into your apps.\\n🌐🛠️MCP enables the specification of various commands for LLMs, such as ‘fetch repository’ for GitHub or ‘comment on PR,’ providing a frictionless experience. Whether through established desktop applications or your own software, MCP clients can easily communicate with servers to activate these tools.\\nAs marketplaces for MCP servers rapidly emerge, countless companies are eager to introduce their solutions. This soon-to-be reality will empower you to choose from a diverse array of ready-made tools, enhancing your LLM workflows and driving efficiency in your projects. 🚀✨\\n🔍 What is Model Context Protocol (MCP)?\\n\\nFrom Source\\nModel Context Protocol (MCP) is a stateful, context-preserving framework designed to power intelligent, multi-step interactions between humans and AI agents. Unlike traditional API calls that treat each request as an isolated event, MCP introduces a persistent, evolving context layer 🌐 that allows AI systems to retain memory, learn dynamically, and act autonomously over time.\\nAccording to modelcontextprotocol.io, MCP is built on three pillars:\\n\\nStatefulness 🧠: Maintains session-specific and long-term memory.\\nInteroperability 🔗: Works seamlessly across models, tools, and data sources.\\nAgent-Centric Design 🤖: Prioritizes autonomous decision-making within defined boundaries.\\n\\nExample:\\n\\nAn MCP-powered travel agent 🧳 remembers your budget, allergies, and past trip feedback across conversations to plan a personalized itinerary — no need to repeat yourself!\\n\\nCore Architecture of the Model Context Protocol (MCP) 🏗️🔗\\nUnderstanding the Connection Between Clients, Servers, and LLMs\\nThe Model Context Protocol (MCP) features a flexible and extensible architecture that facilitates seamless communication between LLM applications and integrations. This section outlines the core architectural components and concepts.\\nOverview 🔍\\nMCP operates on a client-server architecture where:\\n\\nHosts: LLM applications (such as Claude Desktop or Integrated Development Environments) that initiate connections.\\nClients: Maintain 1:1 connections with their corresponding servers, operating within the host application.\\nServers: Provide essential context, tools, and prompts to clients.\\n\\n\\nFrom Source\\nServer Process ⚙️\\nThe server process involves the following components:\\n\\nHost: An application that hosts the interaction.\\nTransport Layer: The layer responsible for facilitating communication between clients and servers.\\nMCP Client: The entity that communicates with the MCP server.\\nMCP Server: The component that manages tools, context, and handles requests from the MCP Client.\\n\\nThis architecture ensures that LLMs can effectively access and utilize tools and resources, enhancing their capabilities in various applications. 🌐✨\\nCore Components of the Model Context Protocol (MCP) 🛠️🔗\\nProtocol Layer 📡\\nThe protocol layer is responsible for message framing, linking requests with responses, and defining high-level communication patterns.\\nKey Classes Include:\\n\\nProtocol\\nClient\\nServer\\n\\nTransport Layer 🚚\\nThe transport layer manages the actual communication between clients and servers, supporting multiple transport mechanisms:\\n\\nStdio Transport: Utilizes standard input/output for communication, making it ideal for local processes.\\nHTTP with SSE Transport: Employs Server-Sent Events for server-to-client messages and HTTP POST for client-to-server communications.\\n\\nAll transport mechanisms leverage JSON-RPC 2.0 for message exchanges. Refer to the specification for detailed information about the Model Context Protocol message format.\\nMessage Types 📨\\nMCP defines several key message types:\\n\\nRequests: Expect a response from the receiving side.\\nResults: Indicate successful responses to requests.\\nErrors: Indicate that a request has failed.\\n\\n🚀How MCP Works: A Technical Deep Dive\\n1. Context Window Management\\nMCP uses a dynamic context window that grows with each interaction, storing:\\n\\nUser preferences (e.g., language, tone).\\nConversation history (prior queries/responses).\\nEnvironmental data (e.g., device type, location 🌍).\\n\\n2. Context Embedding & Compression\\nTo avoid overload, MCP compresses non-critical data into embeddings (e.g., summarizing a 10-message chat into a intent vector 🔢) while retaining key details.\\n3. Stateful Workflows\\nMCP enables multi-step workflows where agents:\\n\\nRemember past actions (e.g., “User already uploaded their ID”).\\nAdapt strategies (e.g., switching from email to SMS if the user is offline 📴).\\nSelf-correct using feedback (e.g., “User disliked Option A; prioritize Option B”).\\n\\n# Hypothetical MCP stateful workflow (from official docs)  \\nclass TravelAgent(MCPAgent):  \\n    def __init__(self, user_id):  \\n        self.context = load_context(user_id)  # Load past interactions  \\n        self.preferences = self.context.get(\"preferences\", {})    \\ndef book\\\\_flight(self, query):    \\n    if self.preferences.get(\"class\") == \"economy\":    \\n        return search\\\\_flights(query, budget=True)    \\n    else:    \\n        return search\\\\_flights(query)\\n\\nWhy Not Just Give the LLM Access to the API? 🤔\\nA common question regarding the Model Context Protocol (MCP) is: “Why do we need a custom protocol? Can’t LLMs simply learn to use APIs on their own?”\\nIn theory, the answer may be affirmative. Most public APIs come with documentation that outlines their functionalities. One could provide this documentation to an LLM, enabling it to derive the necessary steps to achieve its objectives.\\nHowever, in practice, this method is often inefficient. As developers focused on enhancing user experience, it’s essential to prioritize speed and responsiveness. By presenting tools to the LLM in an easily consumable format, we significantly reduce latency and streamline the overall process. This approach ensures a smoother interaction and quicker results for users. 🏎️💨\\n🆚 MCP vs. Traditional API Calls: A Game Changer\\n\\nWhy the shift? APIs are like snapshots 📸 — great for static tasks. MCP is a video 🎥, capturing the full narrative of user intent.\\nIsn’t This Just Tool Calling? 🤔🔧\\nA question I often encounter regarding the Model Context Protocol (MCP) is, “How does this differ from tool calling?”\\nTool calling refers to the mechanism by which LLMs invoke functions to perform tasks in the real world. In this setup, the LLM operates alongside a tool executor that calls the specified tools and returns the results. The typical process looks like this:\\n\\n📝 Describe Tool to Be Called\\n📤 Send Result\\n🤖 LLM\\n⚙️ Tool Executor\\n\\n\\nFrom Source\\nHowever, this interaction typically occurs within the same environment, whether that be a single server or a specific desktop application.\\nIn contrast, the MCP framework enables LLMs to access tools from a separate process, which can be either local or hosted on a remote server. The structure is as follows:\\n\\n🌐 MCP Server\\n🖥️ MCP Client\\n📝 Describe Tool to Be Called\\n🔧 Call Tool\\n📤 Send Result\\n🔄 Return Result\\n🤖 LLM\\n📜 MCP Protocol\\n⚙️ Tool Executor\\n\\n\\nFrom Source\\nThe key distinction lies in the complete decoupling of the MCP server from the client. This separation offers greater flexibility and scalability, enhancing how LLMs interact with external tools. 🌐✨\\n🌟 Why MCP is Revolutionary for Agentic AI\\nAgentic frameworks require AI to act autonomously, not just respond. MCP’s importance lies in:\\n1. Enabling True Autonomy\\nAgents can now:\\n\\nMake decisions based on historical data (e.g., a healthcare agent 🏥 recalling a patient’s allergy list).\\nChain tasks without human intervention (e.g., “Research → Draft → Edit → Publish” blog posts ✍️).\\n\\n2. Collaborative Intelligence\\nMCP allows agents to share context with:\\n\\nOther agents (e.g., a customer service bot 🤖 escalating to a human agent 👩💻).\\nExternal tools (e.g., pulling real-time stock data 📈 into a financial advisor’s response).\\n\\n3. Ethical Guardrails\\n\\nAuditability: Full context history helps trace biased/inaccurate outputs.\\nPrivacy: Sensitive data (e.g., medical records) is compartmentalized 🔒.\\n\\nWithout MCP, agents would lack continuity — like a chef 🧑🍳 forgetting recipe steps midway!\\n4. Enables Long-Term Autonomy\\n\\nPersistent Memory: Agents remember user preferences (e.g., “Alex hates spam emails 📧”).\\nGoal Chaining: Execute multi-step tasks (e.g., Research → Negotiate → Book a business trip ✈️)\\n\\nConnection Lifecycle 🔄\\nThe connection lifecycle within MPI is essential for managing the states and transitions of interactions between clients and servers, ensuring robust communication and functionality throughout the process.\\nThis structured approach to components within the MCP provides a clear framework for efficient communication and integration, empowering LLM applications to thrive. 🌟✨\\n\\n1. Initialization 🚀\\nDuring the initialization phase, the following steps occur between the server and client:\\n\\nClient sends an initialize request that includes the protocol version and its capabilities.\\nServer responds with its own protocol version and capabilities.\\nClient sends an initialized notification to acknowledge successful connection establishment.\\nThe connection is now ready for use, and normal message exchange begins.\\n\\n2. Message Exchange 📨\\nAfter the initialization phase, the MCP supports the following communication patterns:\\n\\nRequest-Response: Either the client or server can send requests, to which the other party will respond.\\nNotifications: Either side may send one-way messages without expecting a response.\\n\\n3. Termination 🛑\\nThe connection can be terminated by either party, which can occur in several ways:\\n\\nClean Shutdown: Achieved through the close() method.\\nTransport Disconnection: Occurs when communication channels are lost.\\nError Conditions: Termination may also result from encountering errors.\\n\\nError Handling ❗\\nMCP defines a set of standard error codes to effectively manage issues that may arise:\\nenum ErrorCode {\\n  // Standard JSON-RPC error codes\\n  ParseError = -32700,\\n  InvalidRequest = -32600,\\n  MethodNotFound = -32601,\\n  InvalidParams = -32602,\\n  InternalError = -32603\\n}\\nAdditionally, SDKs and applications can define their own custom error codes starting from -32000.\\nError Propagation:\\nErrors are communicated through:\\n\\nError Responses: Returned in response to requests with issues.\\nError Events: Triggered on transports to notify of errors.\\nProtocol-Level Error Handlers: Manage errors at the MCP level.\\n\\nThis structured lifecycle ensures robust and efficient communication between clients and servers while gracefully handling errors as they arise. 🌐💼\\n🏗️ Code Implementation\\nThe implementation showcases a sophisticated multi-server setup that handles different types of operations through various transport protocols. Here’s a visual representation of the system:\\n\\nInstall required Dependencies\\npip install mcp httpx langchain langchain-core langchai-community langchain-groq langchain-ollama langchain_mcp_adapters\\nSet up groq api key in .env file\\nimport os\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nCreate Servers\\nMath Server\\n# math_server.py\\nfrom mcp.server.fastmcp import FastMCP  \\nmcp = FastMCP(\"Math\")  \\n@mcp.tool()\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Add two numbers\"\"\"\\n    return a + b  \\n@mcp.tool()\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply two numbers\"\"\"\\n    return a * b  \\nif __name__ == \"__main__\":\\n    mcp.run(transport=\"stdio\")\\nWeather Server(weather.py)\\nfrom typing import Any\\nimport httpx\\nfrom mcp.server.fastmcp import FastMCP  \\n# Initialize FastMCP server\\nmcp = FastMCP(\"weather\")  \\n# Constants\\nNWS_API_BASE = \"https://api.weather.gov\"\\nUSER_AGENT = \"weather-app/1.0\"  \\nasync def make_nws_request(url: str) -> dict[str, Any] | None:\\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\\n    headers = {\\n        \"User-Agent\": USER_AGENT,\\n        \"Accept\": \"application/geo+json\"\\n    }\\n    async with httpx.AsyncClient() as client:\\n        try:\\n            response = await client.get(url, headers=headers, timeout=30.0)\\n            response.raise_for_status()\\n            return response.json()\\n        except Exception:\\n            return None  \\ndef format_alert(feature: dict) -> str:\\n    \"\"\"Format an alert feature into a readable string.\"\"\"\\n    props = feature[\"properties\"]\\n    return f\"\"\"\\nEvent: {props.get(\\'event\\', \\'Unknown\\')}\\nArea: {props.get(\\'areaDesc\\', \\'Unknown\\')}\\nSeverity: {props.get(\\'severity\\', \\'Unknown\\')}\\nDescription: {props.get(\\'description\\', \\'No description available\\')}\\nInstructions: {props.get(\\'instruction\\', \\'No specific instructions provided\\')}\\n\"\"\"  \\n@mcp.tool()\\nasync def get_alerts(state: str) -> str:\\n    \"\"\"Get weather alerts for a US state.  \\nArgs:  \\n    state: Two-letter US state code (e.g. CA, NY)  \\n\"\"\"  \\nurl = f\"{NWS\\\\_API\\\\_BASE}/alerts/active/area/{state}\"  \\ndata = await make\\\\_nws\\\\_request(url)\\n\\nif not data or \"features\" not in data:  \\n    return \"Unable to fetch alerts or no alerts found.\"\\n\\nif not data\\\\[\"features\"\\\\]:  \\n    return \"No active alerts for this state.\"\\n\\nalerts = \\\\[format\\\\_alert(feature) for feature in data\\\\[\"features\"\\\\]\\\\]  \\nreturn \"\\\\\\\\n---\\\\\\\\n\".join(alerts)\\n\\n@mcp.tool()\\nasync def get_forecast(latitude: float, longitude: float) -> str:\\n    \"\"\"Get weather forecast for a location.  \\nArgs:  \\n    latitude: Latitude of the location  \\n    longitude: Longitude of the location  \\n\"\"\"  \\n\\\\# First get the forecast grid endpoint  \\npoints\\\\_url = f\"{NWS\\\\_API\\\\_BASE}/points/{latitude},{longitude}\"  \\npoints\\\\_data = await make\\\\_nws\\\\_request(points\\\\_url)\\n\\nif not points\\\\_data:  \\n    return \"Unable to fetch forecast data for this location.\"\\n\\n\\\\# Get the forecast URL from the points response  \\nforecast\\\\_url = points\\\\_data\\\\[\"properties\"\\\\]\\\\[\"forecast\"\\\\]  \\nforecast\\\\_data = await make\\\\_nws\\\\_request(forecast\\\\_url)\\n\\nif not forecast\\\\_data:  \\n    return \"Unable to fetch detailed forecast.\"\\n\\n\\\\# Format the periods into a readable forecast  \\nperiods = forecast\\\\_data\\\\[\"properties\"\\\\]\\\\[\"periods\"\\\\]  \\nforecasts = \\\\[\\\\]  \\nfor period in periods\\\\[:5\\\\]:  \\\\# Only show next 5 periods  \\n    forecast = f\"\"\"\\n\\n{period[\\'name\\']}:\\nTemperature: {period[\\'temperature\\']}°{period[\\'temperatureUnit\\']}\\nWind: {period[\\'windSpeed\\']} {period[\\'windDirection\\']}\\nForecast: {period[\\'detailedForecast\\']}\\n\"\"\"\\n        forecasts.append(forecast)  \\nreturn \"\\\\\\\\n---\\\\\\\\n\".join(forecasts)\\n\\nif __name__ == \"__main__\":\\n    # Initialize and run the server\\n    mcp.run(transport=\\'sse\\')\\nCreate client\\nlangchain_mcp_multiserver.py\\nimport asyncio\\nfrom mcp import ClientSession, StdioServerParameters\\nfrom mcp.client.stdio import stdio_client\\nfrom IPython.display import display, Markdown\\nfrom langchain_core.messages import HumanMessage, ToolMessage, AIMessage\\nfrom langchain_mcp_adapters.tools import load_mcp_tools\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\\nfrom langgraph.prebuilt import create_react_agent  \\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\\nfrom langchain_groq import ChatGroq\\nfrom langchain_ollama import ChatOllama\\nfrom langchain_core.prompts import PromptTemplate\\nfrom dotenv import load_dotenv  \\nload_dotenv()  \\nmodel = ChatGroq(model=\"llama-3.3-70b-versatile\",temperature=0.5)\\nmodel = ChatOllama(model=\"llama3.2:1b\",temperature=0.0,max_new_tokens=500)\\nserver_params = StdioServerParameters(\\n    command=\"python\",\\n    # Make sure to update to the full absolute path to your math_server.py file\\n    args=[\"weather.py\"],\\n)\\nasync def run_app(user_question):  \\nasync with MultiServerMCPClient(  \\n    {  \\n        \"weather\": {  \\n            \"url\": \"http://localhost:8000/sse\",  \\n            \"transport\": \"sse\",  \\n        },  \\n        \"math\": {  \\n        \"command\": \"python\",  \\n        \\\\# Make sure to update to the full absolute path to your math\\\\_server.py file  \\n        \"args\": \\\\[\"math\\\\_server.py\"\\\\],  \\n        \"transport\": \"stdio\",  \\n        },  \\n    }  \\n) as client:  \\n    agent = create\\\\_react\\\\_agent(model, client.get\\\\_tools())  \\n    agent\\\\_response = await agent.ainvoke({\"messages\": user\\\\_question})  \\n    print(agent\\\\_response\\\\[\\'messages\\'\\\\]\\\\[-1\\\\].content)  \\n    \\\\# # Stream the response chunks  \\n    \\\\# async for chunk in agent.astream({\"messages\": user\\\\_question}):  \\n    \\\\#     # Extract the message content from the AddableUpdatesDict structure  \\n    \\\\#     if \\'agent\\' in chunk and \\'messages\\' in chunk\\\\[\\'agent\\'\\\\]:  \\n    \\\\#         for message in chunk\\\\[\\'agent\\'\\\\]\\\\[\\'messages\\'\\\\]:  \\n    \\\\#             if isinstance(message, AIMessage):  \\n    \\\\#                 # Handle different content formats  \\n    \\\\#                 if isinstance(message.content, list):  \\n    \\\\#                     # For structured content with text and tool use  \\n    \\\\#                     for item in message.content:  \\n    \\\\#                         if isinstance(item, dict) and \\'text\\' in item:  \\n    \\\\#                             print(f\"\\\\*\\\\*AI\\\\*\\\\*: {item\\\\[\\'text\\'\\\\]}\")  \\n    \\\\#                 else:  \\n    \\\\#                     # For simple text content  \\n    \\\\#                     print(f\"\\\\*\\\\*AI\\\\*\\\\*: {message.content}\")\\n\\n    \\\\#     elif \\'tools\\' in chunk and \\'messages\\' in chunk\\\\[\\'tools\\'\\\\]:  \\n    \\\\#         for message in chunk\\\\[\\'tools\\'\\\\]\\\\[\\'messages\\'\\\\]:  \\n    \\\\#             if hasattr(message, \\'name\\') and hasattr(message, \\'content\\'):  \\n    \\\\#                 # Display tool response  \\n    \\\\#                 print(f\"\\\\*\\\\*Tool ({message.name})\\\\*\\\\*: {message.content}\")  \\n    return agent\\\\_response\\\\[\\'messages\\'\\\\]\\\\[-1\\\\].content\\n\\nif __name__ == \"__main__\":\\n    #user_question = \"what is the weather in california?\"\\n    #user_question = \"what\\'s (3 + 5) x 12?\"\\n    #user_question = \"what\\'s the weather in seattle?\"\\n    user_question = \"what\\'s the weather in NYC?\"\\n    response = asyncio.run(run_app(user_question=user_question))\\n    print(response)             \\nPrior to invoking the client make sure that the weather server is up and running\\npython weather.py\\nInvoke the client\\npython langchain_mcp_multiserver.py\\nResponse: “what’s (3 + 5) x 12?”\\nThe result of (3 + 5) is 8, and 8 x 12 is 96.  \\nResposne : “what’s the weather in NYC?”\\nIt appears you\\'ve provided a list of weather alerts from the National Weather Service (NWS) for various regions in New York State, Vermont, and parts of Massachusetts.  \\nHere\\'s a breakdown of what each alert is saying:  \\n**Flooding Alerts**  \\n* The NWS has issued several flood watches across New York State, including:\\n        + Northern St. Lawrence; Northern Franklin; Eastern Clinton; Southeastern St. Lawrence; Southern Franklin; Western Clinton; Western Essex; Southwestern St. Lawrence; Grand Isle; Western Franklin; Orleans; Essex; Western Chittenden; Lamoille; Caledonia; Washington; Western Addison; Orange; Western Rutland; Eastern Franklin; Eastern Chittenden; Eastern Addison; Eastern Rutland; Western Windsor; Eastern Windsor\\n        + Northern Herkimer; Hamilton; Southern Herkimer; Southern Fulton; Montgomery; Northern Saratoga; Northern Warren; Northern Washington; Northern Fulton; Southeast Warren; Southern Washington; Bennington; Western Windham; Eastern Windham\\n* The NWS has also issued a flood watch for parts of Vermont, including:\\n        + Northern New York and northern and central Vermont  \\n**Ice Jam Alerts**  \\n* The NWS has warned about the possibility of ice jams in several areas, including:\\n        + Bennington; Western Windham; Eastern Windham\\n        + Southern Vermont, Bennington and Windham Counties\\n        + Central New York, Herkimer County\\n        + Northern New York, Hamilton, Montgomery, Fulton, Herkimer, Warren, Washington Counties  \\n**Other Alerts**  \\n* The NWS has issued several warnings about heavy rainfall and snowmelt leading to minor river flooding.\\n* There are also alerts for isolated ice jams that could further increase the flood risk.  \\nIt\\'s essential to stay informed about weather conditions in your area and follow the instructions of local authorities. If you\\'re planning outdoor activities, be prepared for changing weather conditions and take necessary precautions to stay safe.\\nIt appears you\\'ve provided a list of weather alerts from the National Weather Service (NWS) for various regions in New York State, Vermont, and parts of Massachusetts.  \\nHere\\'s a breakdown of what each alert is saying:  \\n**Flooding Alerts**  \\n* The NWS has issued several flood watches across New York State, including:\\n        + Northern St. Lawrence; Northern Franklin; Eastern Clinton; Southeastern St. Lawrence; Southern Franklin; Western Clinton; Western Essex; Southwestern St. Lawrence; Grand Isle; Western Franklin; Orleans; Essex; Western Chittenden; Lamoille; Caledonia; Washington; Western Addison; Orange; Western Rutland; Eastern Franklin; Eastern Chittenden; Eastern Addison; Eastern Rutland; Western Windsor; Eastern Windsor\\n        + Northern Herkimer; Hamilton; Southern Herkimer; Southern Fulton; Montgomery; Northern Saratoga; Northern Warren; Northern Washington; Northern Fulton; Southeast Warren; Southern Washington; Bennington; Western Windham; Eastern Windham\\n* The NWS has also issued a flood watch for parts of Vermont, including:\\n        + Northern New York and northern and central Vermont  \\n**Ice Jam Alerts**  \\n* The NWS has warned about the possibility of ice jams in several areas, including:\\n        + Bennington; Western Windham; Eastern Windham\\n        + Southern Vermont, Bennington and Windham Counties\\n        + Central New York, Herkimer County\\n        + Northern New York, Hamilton, Montgomery, Fulton, Herkimer, Warren, Washington Counties  \\n**Other Alerts**  \\n* The NWS has issued several warnings about heavy rainfall and snowmelt leading to minor river flooding.\\n* There are also alerts for isolated ice jams that could further increase the flood risk.  \\nIt\\'s essential to stay informed about weather conditions in your area and follow the instructions of local authorities. If you\\'re planning outdoor activities, be prepared for changing weather conditions and take necessary precautions to stay safe.\\nNote here the MCP Client is able to make connections to respective servers based on the question asked. We did not explicitly mention any routing logic.\\n🚀 The Future of MCP\\nPer the official docs, upcoming features include:\\n\\nCross-Platform Context Sync: Unify context across apps (e.g., your email ✉️, Slack 💬, and CRM 📊).\\nContext-Aware Security: Dynamically adjust permissions based on user behavior.\\nSelf-Optimization: Agents refine their own context management rules 🔄.\\n\\n💡 Key Takeaways\\n\\nMCP > APIs for complex, evolving tasks.\\nCritical for Agents 🤖 to act proactively, not just reactively.\\nBalances Power & Safety via auditable, compartmentalized context.\\n\\n🎯 Use Cases\\nThis architecture is particularly useful for:\\n\\nMulti-modal AI applications\\nComplex workflow orchestration\\nDistributed AI systems\\nReal-time data processing applications\\n\\n📝 Conclusion\\nThe Model Context Protocol implementation demonstrated here showcases a robust, scalable, and flexible architecture for building sophisticated AI applications. Its modular design and support for multiple transport protocols make it an excellent choice for complex AI systems.\\nReferences\\nGitHub - modelcontextprotocol/python-sdk: The official Python SDK for Model Context Protocol… --------------------------------------------------------------------------------------------- ### The official Python SDK for Model Context Protocol servers and clients - modelcontextprotocol/python-sdk github.com\\nIntroduction - Model Context Protocol ------------------------------------- ### Get started with the Model Context Protocol (MCP) modelcontextprotocol.io\\nGitHub - langchain-ai/langchain-mcp-adapters -------------------------------------------- ### Contribute to langchain-ai/langchain-mcp-adapters development by creating an account on GitHub. github.com\\nconnect with me\\nNote: This experimentation as conducted by following available online resources.\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to audio narrations\\nRead offline with the Medium app\\nTry for $5/month\\nModel Context Protocol\\n\\n105\\n\\n105\\n\\n\\n\\nFollow\\nPublished in The AI Forum -------------------------\\n773 Followers\\n·Last published 2 days ago\\nIts AI forum where all the topics spread across Data Analytics, Data Science, Machine Learning, Deep Learning are discussed.\\nFollow\\n\\n\\nFollow\\nWritten by Plaban Nayak -----------------------\\n4.7K Followers\\n·391 Following\\nMachine Learning and Deep Learning enthusiast\\nFollow\\nNo responses yet\\n\\n\\nWrite a response\\nWhat are your thoughts?\\nCancel\\nRespond\\nAlso publish to my profile\\nMore from Plaban Nayak and The AI Forum\\n\\n\\nIn\\nNerd For Tech\\nby\\nPlaban Nayak\\nFixing RAG with Reasoning Augmented Generation ---------------------------------------------- ### 🛠️ Why RAG is Broken — And How ReAG Fixes It\\nFeb 16\\n297 5\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nSemantic Chunking for RAG ------------------------- ### What is Chunking ?\\nApr 21, 2024\\n650 8\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nWhich Vector Database Should You Use? Choosing the Best One for Your Needs -------------------------------------------------------------------------- ### Introduction\\nApr 19, 2024\\n654 10\\n\\n\\n\\nIn\\nThe AI Forum\\nby\\nPlaban Nayak\\nBuild a Reliable RAG Agent using LangGraph ------------------------------------------ ### Introduction\\nMay 4, 2024\\n776 10\\n\\nSee all from Plaban Nayak\\nSee all from The AI Forum\\nRecommended from Medium\\n\\n\\nIn\\nData Science Collective\\nby\\nBuse Şenol\\nModel Context Protocol (MCP): An End-To-End Tutorial With Hands-On Project with Python -------------------------------------------------------------------------------------- ### What is MCP? How to create an MPC Server that brings news from a web site with Claude Desktop?\\nMar 18\\n288 6\\n\\n\\n\\nIn\\nGenerative AI\\nby\\nSimranjeet Singh\\nMCP Servers [Explained] Python and Agentic AI Tool Integration ---------------------------------------------------------------- ### Explore MCP servers by Anthropic for AI integration, enabling seamless data connectivity with GitHub, Google Drive, and enterprise tools.\\nMar 18\\n227 2\\n\\n\\n\\nIn\\nAI Advances\\nby\\nTechscribe Central\\nModel Context Protocol (MCP): The AI Integration Breakthrough You Need to Know ------------------------------------------------------------------------------ ### See how MCP is reshaping AI’s ability to interact with external data sources.\\nMar 18\\n209 2\\n\\n\\n\\nIn\\nEveryday AI\\nby\\nManpreet Singh\\nSimplified Guide for Model Context Protocol (MCP) ------------------------------------------------- ### The simplest guide related to MCP\\n5d ago\\n208 6\\n\\n\\n\\nIn\\nLevel Up Coding\\nby\\nUjjwal Jindal\\nAPI-First AI Agentic Patterns: Building Smarter Systems Without the Framework Overhead -------------------------------------------------------------------------------------- ### We’ve worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple…\\nMar 19\\n451 3\\n\\n\\n\\nIn\\nData And Beyond\\nby\\nTONI RAMCHANDANI\\nThe Model Context Protocol (MCP): The Ultimate Guide ---------------------------------------------------- ### Introduction to the Model Context Protocol (MCP)\\nMar 10\\n246 6\\n\\nSee more recommendations\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nRules\\nTerms\\nText to speech'}, {'url': 'https://opencv.org/blog/model-context-protocol/', 'title': 'A beginners Guide on Model Context Protocol (MCP) - OpenCV', 'content': 'The Model Context Protocol (MCP) is a universal standard that enables seamless interaction between AI-powered applications and external data sources. It works by establishing a secure and efficient client-server architecture, where AI systems (clients) request relevant context from data repositories or tools (servers). MCP eliminates the need for fragmented integrations by providing a standardized framework for accessing real-time context, such as files, databases, or APIs. Through this [...] Source\\nMCP: Architecture & Key Roles\\nThe Model Context Protocol (MCP) is designed to bridge AI applications (or “agents”) with external systems, data sources, and tools, all while maintaining secure boundaries. At its core, MCP adopts a client–host–server pattern, aiming to standardize how different components communicate and share “context.” This approach is built atop JSON-RPC and emphasizes stateful sessions that coordinate context exchange and sampling.\\n\\nSource\\n\\n\\nHost Process', 'score': 0.9158166, 'raw_content': 'Published Time: 2025-04-03T17:27:55+00:00\\nA beginners Guide on Model Context Protocol (MCP)\\n\\nSkip to primary navigation\\nSkip to main content\\n\\nOpenCV\\nOpen Computer Vision Library\\n\\nLibrary\\nReleases\\nPlatforms\\nLicense\\n\\n\\nForum\\nOpenCV Forum (New!)\\nAskbot (Old Forum)\\n\\n\\nOpenCV University\\nAll Courses\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\n\\nCVDL Master Program\\nCourses [Enrolled Users]\\nFor Organizations\\nStudent Discount\\nCareerX\\n\\n\\nFree Courses\\nPyTorch Bootcamp NEW\\nTensorFlow Keras Course FREE\\nOpenCV Course FREE\\nPython for Beginners FREE\\n\\n\\nServices\\nFace Recognition\\nContribute\\nArea Chairs\\nMembership\\nPlatinum\\nGold\\n\\n\\nDevelopment Partnership\\nCourse Partnership\\n\\n\\nResources\\nNews\\nResearch Papers\\nGet Started\\nPodcast\\nTools\\nRoboflow\\n\\n\\nLinks\\nMedia Kit\\nLeadership\\nAbout\\n\\n\\n\\n\\nSearch for:Search Button\\n\\n\\nMCP – A Beginner’s Guide\\nHome / AI Careers / MCP – A Beginner’s Guide\\nMCP – A Beginner’s Guide\\nsandeep April 3, 2025 AI Careers Tags: artificial intelligence MCP\\nImagine you’re using an AI writing assistant to draft an email. It’s excellent at creating clear and concise text, but you need the email to reference specific updates from a recent team meeting and attach a relevant report. While the assistant is impressive in crafting polished content, it falters when accessing real-time context, like your meeting notes or files. Without direct access to this information, you’re left to manually provide the details—defeating the purpose of seamless assistance.\\nThis limitation underscores a broader challenge with large language models (LLMs). While they are remarkable at processing vast amounts of data, they struggle with real-time context outside their frozen training set. For AI agents to truly thrive, they need more than raw intelligence; they require dynamic access to files, tools, knowledge bases, and the ability to act on this information—like automating tasks or updating documents. Historically, integrating models with external sources was a messy process, relying on fragile custom implementations that were hard to scale.\\nEnter the Model Context Protocol (MCP). Introduced by Anthropic in late 2024, MCP is a groundbreaking open standard designed to bridge the gap between AI assistants and the data-rich ecosystems they need to navigate. MCP eliminates patchy integrations by providing a universal framework for connecting tools and data sources, empowering AI systems with seamless access to diverse contexts. With MCP gaining traction among major AI players and open-source communities. It’s clear—MCP is paving the way for the future of truly connected and agentic AI.\\nThe Rise of the Model Context Protocol (MCP)\\nIn November 2024, Anthropic introduced the Model Context Protocol (MCP). At first, it was overshadowed by discussions on advanced language models, but by early 2025, MCP took center stage. Teams realized building “intelligent agents” was simpler than connecting them to real-world data, making MCP’s approach a long-overdue solution.\\nToday, Block (Square), Apollo, Zed, Replit, Codeium, Sourcegraph, and others have implemented MCP. Over 1,000 open-source connectors emerged by February 2025, expanding its ecosystem with each addition. Anthropic has continued refining MCP’s specification, documentation, and workshops, speeding up adoption. By remaining open, model-agnostic, and reminiscent of open standards (like USB or HTTP), MCP eliminates barriers to broad collaboration.\\nWhy MCP Matters\\nEnthusiasm for MCP isn’t just about novelty; it’s about solving real pain points in machine learning and AI deployment. Here’s a closer look at the core benefits.\\n1. Reproducibility\\n\\nComplete Model Context: All the necessary details—datasets, environment specifications, and hyperparameters- live in one place. All the relevant data is fetched\\xa0from these sources and fed as context to the LLM for a better grounded answer\\n\\n2. Standardization & Collaboration\\n\\n\\nInter-organizational sharing: Many companies build specialized AI tools or maintain custom data sources. If a model is packaged with an MCP file, another organization can plug it into their own system with fewer roadblocks.  \\n\\n\\nOpen-Source Integration: Communities like Hugging Face or GitHub rely on consistent metadata standards to streamline how models are shared and discovered. MCP’s open nature and model-agnostic design make it an ideal candidate to unify these ecosystems.\\n\\n\\nHow MCP Works\\nThe Model Context Protocol (MCP) is a universal standard that enables seamless interaction between AI-powered applications and external data sources. It works by establishing a secure and efficient client-server architecture, where AI systems (clients) request relevant context from data repositories or tools (servers). MCP eliminates the need for fragmented integrations by providing a standardized framework for accessing real-time context, such as files, databases, or APIs. Through this protocol, AI assistants can not only retrieve information but also take meaningful actions, such as updating documents or automating workflows – bridging the gap between isolated intelligence and dynamic, context-aware functionality.\\n\\nSource\\nMCP: Architecture & Key Roles\\nThe Model Context Protocol (MCP) is designed to bridge AI applications (or “agents”) with external systems, data sources, and tools, all while maintaining secure boundaries. At its core, MCP adopts a client–host–server pattern, aiming to standardize how different components communicate and share “context.” This approach is built atop JSON-RPC and emphasizes stateful sessions that coordinate context exchange and sampling.\\n\\nSource\\n\\n\\nHost Process\\n\\n\\nActs as a “container” or coordinator for multiple client instances.  \\n\\n\\nManages lifecycle and security policies, e.g., permissions, user authorization, and enforcement of consent requirements.  \\n\\n\\nOversees how AI or language model integration happens within each client, gathering and merging context as needed.\\n\\n\\n\\n\\nIn plain terms, the host is like the control tower for incoming AI “flights.” It decides which AI client gets to fly, who checks them in, and which runways (servers) they connect to.\\n\\n\\nClient Instances\\n\\n\\nEach client runs inside the host. It establishes a one-to-one session with a specific MCP server using a stateful connection.  \\n\\n\\nHandles capability negotiation and orchestrates messages between itself and the server.  \\n\\n\\nMaintains security boundaries—one client shouldn’t snoop on resources belonging to another client.  \\n\\n\\nTypically, these clients represent AI applications or agents (like Anthropic’s Curser or Windsurf) that can “talk” to servers using the MCP’s standardized interfaces.\\n\\n\\n\\n\\nThink of the client as an AI agent wanting to access external tools or data. Once it’s “MCP-compatible,” it can easily connect to any MCP server—no extra glue code is needed.\\n\\n\\nServers\\n\\n\\nProvide specialized capabilities or resources in a standardized way so that any MCP client can use them.  \\n\\n\\nIt could be a local process or a remote service, each wrapping data sources, APIs, or other utilities (like CRMs, Git repos, or file systems).  \\n\\n\\nDefine “tools,” “prompts,” and “resources” that the client can invoke or retrieve.  \\n\\n\\nMust adhere to security constraints and user permissions enforced by the host.\\n\\n\\n\\n\\nEssentially, servers are the AI ecosystem’s “toolboxes.” By exposing consistent interfaces, they solve the classic “N times M problem,” where you’d otherwise need separate integrations for every AI app and every data source. Build a server once, and any MCP-compatible client can tap into it.\\n\\nFor official specification details, see the MCP Architecture Reference (2024-11-05)\\nFeatures of MCP Server\\nWhen building MCP servers, you’re effectively creating the building blocks that add crucial context to language models. In MCP’s framework, these building blocks are referred to as “primitives” and include:\\n\\nPrompts\\nResources\\nTools\\n\\nThese primitives act as the interface between client-side AI applications (agents) and the specialized environments or data sources the server manages. By properly implementing each primitive, server developers enable rich interactions and enhanced capabilities for language models.\\nPrimitives and Their Control Hierarchy\\nPrimitiveControlDescriptionExamplePromptsUser-controlledPre-defined templates or instructions triggered by user actionsSlash commands, menu selectionsResourcesApplication-controlledContextual data attached and managed by the clientFile contents, Git history logsToolsModel-controlledExecutable functions that the LLM can decide to invokeAPI calls, file writing operations\\nUse Cases\\n1. AI Assistant Managing Calendars\\nImagine you have an AI assistant—let’s call it Claude Desktop—that needs to schedule a meeting. Under the hood, Claude Desktop serves as an MCP host, which relies on an MCP client to connect with a calendar service. The calendar service itself runs as an MCP server. Here’s the workflow:\\n\\n\\nAI Request: The assistant sends a request (e.g., to retrieve free time slots).  \\n\\n\\nServer Response: The MCP server fetches the user’s calendar data and returns it to the assistant.  \\n\\n\\nAI Output: With those available slots in hand, the AI crafts a suitable response or automatically schedules the meeting.  \\n\\n\\n2. Secure AI in Healthcare\\nA medical AI system must securely access patient records. Leveraging MCP:\\n\\n\\nConsent & Privacy: The server checks user permissions and enforces encryption and logging.  \\n\\n\\nData Sharing: The MCP client fetches patient records only after ensuring compliance with healthcare data regulations.  \\n\\n\\nAI Interaction: The AI interprets that data (e.g., to draft a consultation summary or schedule follow-ups).\\n\\n\\nBoth scenarios highlight MCP’s emphasis on user control, data privacy, tool safety, and LLM sampling controls—key pillars for trustworthy, real-world AI solutions.\\nRelated Article: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents\\nConclusion\\nThe Model Context Protocol (MCP) marks a significant leap forward in the journey toward seamless AI integration. By providing an open, standardized way to connect large language models to external tools and data sources, MCP tackles the longstanding challenges of interoperability, scaling, and security head-on.\\nAs AI systems mature, the ability to interface with dynamic, real-world data becomes ever more critical. MCP paves the way for robust, secure, and highly modular AI architectures—much like TCP/IP once did for computer networking. With its growing ecosystem of adopters, SDKs, and off-the-shelf connectors, MCP stands poised to become the go-to standard for building responsive, intelligent applications across virtually every domain.\\nRelated Posts\\n\\nAI Careers\\nAI Jobs in 2025: A Comprehensive GuideAugust 16, 2023\\n\\nAI Careers\\nIntroduction to Artificial Intelligence in 2025August 23, 2023\\n\\nAI Careers\\nHistory of AI: Unraveling the Epic Saga of Mind...August 30, 2023\\nBecome a Member\\nStay up to date on OpenCV and Computer Vision news\\nJoin our Newsletter \\xa0\\nFree Courses\\n\\nPyTorch Bootcamp\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\n\\nPython for Beginners\\n\\n\\nPyTorch Bootcamp\\n\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\nPython for Beginners\\n\\nCourses\\n\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\n\\nMastering Generative AI for Art\\n\\n\\nMastering OpenCV with Python\\n\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\nPartnership\\n\\nIntel, OpenCV’s Platinum Member\\nGold Membership\\nDevelopment Partnership\\nCUDA\\n\\nARM\\n\\n\\nIntel, OpenCV’s Platinum Member\\n\\nGold Membership\\nDevelopment Partnership\\nCUDA\\nARM\\n\\nResources\\n\\nNews\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\n\\nWeb Stories\\n\\n\\nNews\\n\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\nWeb Stories\\n\\nGeneral Link\\n\\nAbout\\nReleases\\n\\nLicense\\n\\n\\nAbout\\n\\nReleases\\nLicense\\n\\nCopyright © 2025, OpenCV team\\n\\nContact Us\\nTerms and Conditions\\n\\nPrivacy Policy\\n\\n\\nContact Us\\n\\nTerms and Conditions\\nPrivacy Policy\\n\\nFree Courses\\n\\nPyTorch Bootcamp\\nTensorFlow & Keras Bootcamp\\nOpenCV Bootcamp\\nPython for Beginners\\n\\nCourses\\n\\nMastering OpenCV with Python\\nFundamentals of CV & IP\\nDeep Learning with PyTorch\\nDeep Learning with TensorFlow & Keras\\nComputer Vision & Deep Learning Applications\\nMastering Generative AI for Art\\n\\nPartnership\\n\\nIntel, OpenCV’s Platinum Member\\nGold Membership\\nDevelopment Partnership\\nCUDA\\nARM\\n\\nResources\\n\\nNews\\nBooks\\nPodcast\\nLinks\\nMedia Kit\\nWeb Stories\\n\\nGeneral Link\\n\\nAbout\\nReleases\\nLicense\\n\\nCopyright ©\\xa02025 , OpenCV team\\nContact Us | Privacy Policy | Terms & Conditions\\n\\nGet started with Computer Vision with our Free OpenCV Bootcamp\\nENROLL NOW'}, {'title': 'Model Context Protocol (MCP): The Hidden Architecture Behind ... - Medium', 'url': 'https://medium.com/@iamanraghuvanshi/model-context-protocol-mcp-the-hidden-architecture-behind-smarter-ai-systems-2f64f971ad0c', 'content': 'Model Context Protocol (MCP): The Hidden Architecture Behind Smarter AI Systems This is where the Model Context Protocol (MCP) enters the conversation. At its core, MCP (Model Context Protocol) is a design framework for delivering relevant, structured, and layered context to a machine learning model at the moment it’s generating output (inference). In more formal terms, MCP is the interface between intention and intelligence — the middle layer that takes structured system and user information, wraps it into a coherent prompt or data sequence, and feeds it to the model in a way that maximizes relevance, accuracy, and traceability. Without MCP: With MCP: Eventually, “prompt engineering” may become a subset of a larger Context Engineering discipline — centered around MCP principles. That’s where the Model Context Protocol (MCP) shines.', 'score': 0.9037116, 'raw_content': None}], 'response_time': 4.33}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I’ve gathered some initial information about the Model Context Protocol (MCP):\n",
      "\n",
      "• MCP is an open‑source standard introduced by Anthropic in late 2024 that allows AI assistants (or any “model clients”) to connect securely to external tools and data sources (“servers”) through a common, JSON‑RPC–based protocol.  \n",
      "• It follows a client–host–server architecture:  \n",
      "  – A host application (e.g., Claude Desktop, an IDE, or any AI‑enabled product) spins up one or more MCP clients.  \n",
      "  – Each client maintains a stateful session with exactly one MCP server that exposes “prompts, tools, and resources.”  \n",
      "• A capability‑negotiation step lets both sides declare what features they support (streaming, resource subscriptions, tool invocation, sampling, etc.).  \n",
      "• Transports are pluggable (stdio for local processes, HTTP + Server‑Sent Events for remote services, WebSocket, etc.), but all messages conform to JSON‑RPC 2.0 framing.  \n",
      "• The design goal is to replace a tangle of bespoke “LLM integrations” with a single, composable way to share context, enforce security boundaries, and coordinate tool calls—ultimately letting agents act on real‑world data with less glue code.  \n",
      "• Major early adopters include Block, Apollo, Zed, Replit, Codeium, and others; open‑source SDKs exist for Python and TypeScript, and LangChain released MCP adapters.\n",
      "\n",
      "Before I outline a research report for you, could you clarify a few things so the final product matches your needs?\n",
      "\n",
      "1. Depth & Audience  \n",
      "   • Do you want a developer‑focused, in‑depth technical write‑up (spec details, code snippets, transport diagrams), or a higher‑level briefing suitable for product managers / executives?  \n",
      "\n",
      "2. Primary Goals  \n",
      "   • Are you mainly interested in how MCP works, why it matters (benefits & limitations), implementation guidance, comparison to alternatives, or potential business use cases?  \n",
      "   • Any specific industry or application (e.g., healthcare compliance, enterprise search, coding agents)?\n",
      "\n",
      "3. Scope Constraints  \n",
      "   • How long or detailed should the report be?  \n",
      "   • Should we include hands‑on examples or keep it conceptual?\n",
      "\n",
      "4. Timing & Currency  \n",
      "   • Do you need the very latest roadmap items (spec revisions in 2025) or is the November 2024 draft sufficient?\n",
      "\n",
      "Let me know your preferences, and I’ll structure the report accordingly.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Yes Anthropic‑backed open standard for integrating external context and tools with LLMs, architectural overview, tell me about interesting specific MCP servers, developer audience, just focus on MCP.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Sections (call_YxyZMhERqkiuvcsHZuOzCBN9)\n",
      " Call ID: call_YxyZMhERqkiuvcsHZuOzCBN9\n",
      "  Args:\n",
      "    sections: ['Core Specification and Design Principles of MCP: Summarize the official spec, JSON‑RPC foundation, statefulness, and design goals. Research tasks: review spec.modelcontextprotocol.io, Anthropic blog post, Medium explainers.', 'Client‑Host‑Server Architecture and Session Lifecycle: Detail roles of host, client, server, connection states (initialize, message exchange, termination). Research tasks: diagram references from spec, gather examples of host apps (Claude Desktop, IDEs).', 'Capability Negotiation and Supported Transports: Explain feature flags, how clients/servers declare capabilities, and transports (stdio, HTTP+SSE, WebSocket). Research tasks: spec sections, SDK docs, community blog posts.', 'Security and Privacy Model: Investigate isolation boundaries, context scoping, consent workflows, encryption, and auditability. Research tasks: official spec security notes, healthcare use‑case discussions.', 'MCP Server Primitives (Prompts, Resources, Tools): Deep dive into how servers expose functionality, lifecycle of each primitive, sampling coordination. Research tasks: spec ‘Server Features’, code samples from Python SDK.', 'Catalog of Notable MCP Servers: Highlight interesting open‑source or commercial servers (GitHub repo access, Slack, Google Drive, Postgres, Puppeteer, Calendar, Weather, Math). For each: purpose, transport, capabilities, maturity. Research tasks: scour GitHub modelcontextprotocol server repo, LangChain MCP adapters, community showcases.', 'Developer Tooling and SDKs: Cover official Python & TypeScript SDKs, LangChain adapters, CLI helpers, testing utilities. Research tasks: GitHub repos, docs.', 'Building a Custom MCP Server – Step‑by‑Step Guide: Provide best practices, sample code, error handling, capability declaration, deployment considerations. Research tasks: tutorial blog posts, OpenCV guide, Medium articles.', 'Integration Patterns for LLM Applications: Show how MCP clients plug into LangChain, Replit, IDEs; discuss multi‑server orchestration, routing logic. Research tasks: LangChain MCP adapter docs, sample code.', 'Limitations, Challenges, and Future Roadmap: Analyze open issues (context size limits, standard evolution), upcoming spec revisions, community proposals. Research tasks: spec changelog, Anthropic roadmap statements, community discussions.']\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: Sections\n",
      "\n",
      "sections=['Core Specification and Design Principles of MCP: Summarize the official spec, JSON‑RPC foundation, statefulness, and design goals. Research tasks: review spec.modelcontextprotocol.io, Anthropic blog post, Medium explainers.', 'Client‑Host‑Server Architecture and Session Lifecycle: Detail roles of host, client, server, connection states (initialize, message exchange, termination). Research tasks: diagram references from spec, gather examples of host apps (Claude Desktop, IDEs).', 'Capability Negotiation and Supported Transports: Explain feature flags, how clients/servers declare capabilities, and transports (stdio, HTTP+SSE, WebSocket). Research tasks: spec sections, SDK docs, community blog posts.', 'Security and Privacy Model: Investigate isolation boundaries, context scoping, consent workflows, encryption, and auditability. Research tasks: official spec security notes, healthcare use‑case discussions.', 'MCP Server Primitives (Prompts, Resources, Tools): Deep dive into how servers expose functionality, lifecycle of each primitive, sampling coordination. Research tasks: spec ‘Server Features’, code samples from Python SDK.', 'Catalog of Notable MCP Servers: Highlight interesting open‑source or commercial servers (GitHub repo access, Slack, Google Drive, Postgres, Puppeteer, Calendar, Weather, Math). For each: purpose, transport, capabilities, maturity. Research tasks: scour GitHub modelcontextprotocol server repo, LangChain MCP adapters, community showcases.', 'Developer Tooling and SDKs: Cover official Python & TypeScript SDKs, LangChain adapters, CLI helpers, testing utilities. Research tasks: GitHub repos, docs.', 'Building a Custom MCP Server – Step‑by‑Step Guide: Provide best practices, sample code, error handling, capability declaration, deployment considerations. Research tasks: tutorial blog posts, OpenCV guide, Medium articles.', 'Integration Patterns for LLM Applications: Show how MCP clients plug into LangChain, Replit, IDEs; discuss multi‑server orchestration, routing logic. Research tasks: LangChain MCP adapter docs, sample code.', 'Limitations, Challenges, and Future Roadmap: Analyze open issues (context size limits, standard evolution), upcoming spec revisions, community proposals. Research tasks: spec changelog, Anthropic roadmap statements, community discussions.']\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Introduction (call_IxV7p7k8tNZex4ZLqccd4swj)\n",
      " Call ID: call_IxV7p7k8tNZex4ZLqccd4swj\n",
      "  Args:\n",
      "    name: Model Context Protocol: Architecture, Servers, and Developer Guide\n",
      "    content: # Model Context Protocol: Architecture, Servers, and Developer Guide\n",
      "\n",
      "Large‑language‑model (LLM) assistants are only as useful as the data and tools they can safely reach.  Anthropic’s Model Context Protocol (MCP) is an open‑standard answer to that integration problem.  Built on JSON‑RPC 2.0 and a simple client‑host‑server pattern, MCP lets any application expose *prompts*, *resources* and *tools* to an LLM through a stateful, capability‑negotiated session—no bespoke glue code required.\n",
      "\n",
      "This report dives into MCP from a developer’s perspective.  We start by unpacking the core specification, then trace the lifecycle of a session, examine security guarantees, and explore how capability flags map to real transports.  We look at the three server primitives in depth, catalog the fast‑growing ecosystem of community and commercial MCP servers, and survey the official SDKs and adapters that streamline development.  A step‑by‑step tutorial shows how to build and deploy a custom server, while an integration section highlights patterns for plugging MCP into LangChain, IDEs and SaaS back‑ends.  We close with an analysis of current limitations and the forward roadmap.\n",
      "\n",
      "Whether you are wiring an LLM into a Slack bot, exposing a database to Claude, or architecting a multi‑agent workflow, this guide aims to provide the architectural insight and hands‑on details you need to adopt MCP with confidence.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: Introduction\n",
      "\n",
      "name='Model Context Protocol: Architecture, Servers, and Developer Guide' content='# Model Context Protocol: Architecture, Servers, and Developer Guide\\n\\nLarge‑language‑model (LLM) assistants are only as useful as the data and tools they can safely reach.  Anthropic’s Model\\xa0Context\\xa0Protocol\\xa0(MCP) is an open‑standard answer to that integration problem.  Built on JSON‑RPC\\xa02.0 and a simple client‑host‑server pattern, MCP lets any application expose *prompts*, *resources* and *tools* to an LLM through a stateful, capability‑negotiated session—no bespoke glue code required.\\n\\nThis report dives into MCP from a developer’s perspective.  We start by unpacking the core specification, then trace the lifecycle of a session, examine security guarantees, and explore how capability flags map to real transports.  We look at the three server primitives in depth, catalog the fast‑growing ecosystem of community and commercial MCP servers, and survey the official SDKs and adapters that streamline development.  A step‑by‑step tutorial shows how to build and deploy a custom server, while an integration section highlights patterns for plugging MCP into LangChain, IDEs and SaaS back‑ends.  We close with an analysis of current limitations and the forward roadmap.\\n\\nWhether you are wiring an LLM into a Slack bot, exposing a database to Claude, or architecting a multi‑agent workflow, this guide aims to provide the architectural insight and hands‑on details you need to adopt MCP with confidence.'\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Introduction written. Now write a conclusion section.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Conclusion (call_i7eP0u72laE291wcDrvRxTCz)\n",
      " Call ID: call_i7eP0u72laE291wcDrvRxTCz\n",
      "  Args:\n",
      "    name: Conclusion\n",
      "    content: ## Conclusion\n",
      "\n",
      "The Model Context Protocol brings the rigor of well‑defined network standards to the messy world of LLM integrations.  By wrapping every external data source or action inside a small MCP server, developers gain a repeatable recipe for delivering fresh context to models while preserving security boundaries and auditability.  JSON‑RPC framing, capability negotiation, and a pluggable transport layer make the protocol easy to embed in desktop apps, cloud agents, and serverless back‑ends alike.\n",
      "\n",
      "Key insights from this report:\n",
      "\n",
      "• The client–host–server split cleanly isolates orchestration logic (host), model‑facing logic (client), and domain expertise (server), making each part independently testable and deployable.\n",
      "\n",
      "• Prompts, resources, and tools form a minimal yet expressive surface for servers to expose capabilities.  Their tight coupling with capability flags ensures that models only see what they are allowed to use.\n",
      "\n",
      "• A vibrant ecosystem of open‑source servers—spanning GitHub, Slack, Postgres, Puppeteer, weather feeds, and math helpers—demonstrates MCP’s versatility, while official SDKs and LangChain adapters lower the barrier to entry.\n",
      "\n",
      "• Security considerations are first‑class: context scoping, per‑server isolation, consent workflows, and transport‑level encryption help organizations comply with privacy regulations without stifling innovation.\n",
      "\n",
      "• Remaining challenges include standardized authentication, context‑window budgeting across many servers, and maintaining backward compatibility as the spec evolves.  The published roadmap and open governance model position MCP to tackle these issues collaboratively.\n",
      "\n",
      "For developers building agentic systems or enterprise copilots, adopting MCP now offers a strategic advantage: you can focus on higher‑value reasoning and product logic while delegating data plumbing to a growing catalogue of interchangeable, community‑maintained connectors.  As the protocol matures, expect MCP to become as foundational to AI tooling as HTTP is to web services.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: Conclusion\n",
      "\n",
      "name='Conclusion' content='## Conclusion\\n\\nThe Model Context Protocol brings the rigor of well‑defined network standards to the messy world of LLM integrations.  By wrapping every external data source or action inside a small MCP server, developers gain a repeatable recipe for delivering fresh context to models while preserving security boundaries and auditability.  JSON‑RPC framing, capability negotiation, and a pluggable transport layer make the protocol easy to embed in desktop apps, cloud agents, and serverless back‑ends alike.\\n\\nKey insights from this report:\\n\\n• The client–host–server split cleanly isolates orchestration logic (host), model‑facing logic (client), and domain expertise (server), making each part independently testable and deployable.\\n\\n• Prompts, resources, and tools form a minimal yet expressive surface for servers to expose capabilities.  Their tight coupling with capability flags ensures that models only see what they are allowed to use.\\n\\n• A vibrant ecosystem of open‑source servers—spanning GitHub, Slack, Postgres, Puppeteer, weather feeds, and math helpers—demonstrates MCP’s versatility, while official SDKs and LangChain adapters lower the barrier to entry.\\n\\n• Security considerations are first‑class: context scoping, per‑server isolation, consent workflows, and transport‑level encryption help organizations comply with privacy regulations without stifling innovation.\\n\\n• Remaining challenges include standardized authentication, context‑window budgeting across many servers, and maintaining backward compatibility as the spec evolves.  The published roadmap and open governance model position MCP to tackle these issues collaboratively.\\n\\nFor developers building agentic systems or enterprise copilots, adopting MCP now offers a strategic advantage: you can focus on higher‑value reasoning and product logic while delegating data plumbing to a growing catalogue of interchangeable, community‑maintained connectors.  As the protocol matures, expect MCP to become as foundational to AI tooling as HTTP is to web services.'\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Report is now complete with introduction, body sections, and conclusion.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "All set — the report now has its introduction, detailed section blueprint, and conclusion. Let me know if you’d like any additional elaboration or full write‑ups for the individual sections.\n"
     ]
    }
   ],
   "source": [
    "msg =  [{\"role\": \"user\", \"content\": \"Yes Anthropic‑backed open standard for integrating external context and tools with LLMs, architectural overview, tell me about interesting specific MCP servers, developer audience, just focus on MCP.\"}]\n",
    "response = agent.invoke({\"messages\": msg}, config=thread_config)\n",
    "for m in agent.get_state(thread_config).values['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Model Context Protocol: Architecture, Servers, and Developer Guide\n",
       "\n",
       "Large‑language‑model (LLM) assistants are only as useful as the data and tools they can safely reach.  Anthropic’s Model Context Protocol (MCP) is an open‑standard answer to that integration problem.  Built on JSON‑RPC 2.0 and a simple client‑host‑server pattern, MCP lets any application expose *prompts*, *resources* and *tools* to an LLM through a stateful, capability‑negotiated session—no bespoke glue code required.\n",
       "\n",
       "This report dives into MCP from a developer’s perspective.  We start by unpacking the core specification, then trace the lifecycle of a session, examine security guarantees, and explore how capability flags map to real transports.  We look at the three server primitives in depth, catalog the fast‑growing ecosystem of community and commercial MCP servers, and survey the official SDKs and adapters that streamline development.  A step‑by‑step tutorial shows how to build and deploy a custom server, while an integration section highlights patterns for plugging MCP into LangChain, IDEs and SaaS back‑ends.  We close with an analysis of current limitations and the forward roadmap.\n",
       "\n",
       "Whether you are wiring an LLM into a Slack bot, exposing a database to Claude, or architecting a multi‑agent workflow, this guide aims to provide the architectural insight and hands‑on details you need to adopt MCP with confidence.\n",
       "\n",
       "## Core Specification and Design Principles of MCP\n",
       "\n",
       "The Model Context Protocol (MCP) defines a stateful, JSON‑RPC 2.0 message layer that lets AI “clients” and context‑providing “servers” exchange prompts, resources and tool calls.\n",
       "\n",
       "Protocol foundation\n",
       "- All traffic follows JSON‑RPC 2.0; three message types: request, response, notification.\n",
       "- A connection begins with `initialize` → capability negotiation → `initialized`, forming a long‑lived session that supports progress, cancellation and ping utilities.\n",
       "- Transport‑agnostic (stdio, HTTP, SSE); each client maintains one session per server.\n",
       "\n",
       "Core features\n",
       "- Server primitives: **prompts**, **resources**, **tools**; each advertised via capabilities and discoverable at runtime.\n",
       "- Client features: **roots** (filesystem scopes) and **sampling** (server‑initiated LLM completions).  \n",
       "- Capability flags gate optional functions (e.g., resource subscriptions).\n",
       "\n",
       "Design goals (spec)\n",
       "- **Simplicity** – servers focus on narrow tasks; hosts manage orchestration.\n",
       "- **Composability** – multiple servers can be chained; any app may act as both client and server.\n",
       "- **Security isolation** – servers cannot read full conversation or cross‑server data; host mediates context.\n",
       "- **Progressive adoption** – features added independently; backwards compatibility via capability negotiation.\n",
       "\n",
       "Why it matters\n",
       "Anthropic positions MCP as the “universal connector” that eliminates N×M integrations, already backed by companies like Block and Replit, and an ecosystem of open‑source servers.\n",
       "\n",
       "### Sources\n",
       "1. https://spec.modelcontextprotocol.io/specification/2025-03-26/architecture/\n",
       "2. https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/\n",
       "3. https://www.anthropic.com/news/model-context-protocol\n",
       "4. https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef\n",
       "\n",
       "## Client‑Host‑Server Architecture and Session Lifecycle\n",
       "\n",
       "**Roles**\n",
       "- **Host** – the LLM application (e.g., Claude Desktop, IDE plug‑ins) that spawns and supervises MCP clients, applies consent policies, and aggregates context.\n",
       "- **Client** – a host‑internal process that maintains a 1:1, isolated JSON‑RPC connection to a single server; it negotiates capabilities and routes messages.\n",
       "- **Server** – a lightweight program (local or remote) that exposes prompts, resources, or tools required by the model.\n",
       "\n",
       "**Session phases (see architecture & lifecycle diagrams in the MCP specification)**\n",
       "1. **Initialization**\n",
       "   • Client → Server `initialize` request carrying `protocolVersion` and capability set.\n",
       "   • Server → Client matching version & supported capabilities.\n",
       "   • Client → Server `initialized` notification; secure stateful session begins.\n",
       "2. **Message Exchange**\n",
       "   • Bidirectional requests, responses, and notifications flow over JSON‑RPC.\n",
       "   • Only features declared during negotiation may be used (e.g., tool calls, resource subscriptions, sampling).\n",
       "   • Host enforces isolation across multiple concurrent client‑server pairs.\n",
       "3. **Termination**\n",
       "   • Either side sends `disconnect`/transport close; host cleans resources.\n",
       "   • On shutdown, outstanding operations are cancelled and capability state is discarded.\n",
       "\n",
       "**Host examples**\n",
       "- Claude Desktop app (full MCP support: resources, prompts, tools).\n",
       "- IDE assistants such as Continue (VS Code) or Zed editor integrating MCP for code workflows.\n",
       "\n",
       "This architecture decouples responsibilities, simplifies security auditing, and lets features be added incrementally to servers or clients.\n",
       "\n",
       "### Sources\n",
       "1. https://modelcontextprotocol.io/specification/2025-03-26/architecture\n",
       "2. https://modelcontextprotocol.io/docs/concepts/architecture\n",
       "3. https://modelcontextprotocol.io/introduction\n",
       "\n",
       "## Capability Negotiation & Transports\n",
       "\n",
       "During the `initialize` handshake the client sends a `ClientCapabilities` object; the server replies with an `InitializeResult.capabilities` map.  Each key is a feature flag (e.g. `textDocumentSync`, `completionProvider`, `workspaceSymbolProvider`).  A missing key means “unsupported”, letting editors hide UI affordances and servers skip work.  Both sides can later refine support via `client/registerCapability` or `experimental` flags for preview features.  This light‑weight negotiation keeps 3rd‑party servers forward‑compatible with new spec releases.¹\n",
       "\n",
       "LSP messages are JSON‑RPC 2.0 but the transport is pluggable:\n",
       "• stdio (default): language process reads stdin / writes stdout; simple and cross‑platform.²  \n",
       "• Node IPC / TCP socket: exposed by VS Code as `TransportKind.ipc` or `TransportKind.socket`; useful when the server runs as a long‑lived daemon.²  \n",
       "• WebSocket: enables remote or browser‑based IDEs; the same JSON‑RPC frames travel inside WS frames (e.g. `vscode.websocket` sample).³  \n",
       "• HTTP + Server‑Sent Events: some cloud gateways (e.g. GitHub Codespaces) tunnel LSP over HTTPS, still carrying the same headers and content.³\n",
       "\n",
       "Because the spec forbids transport‑specific semantics, a server built for stdio usually works over WebSocket with only the launcher code changed—capability flags stay identical.\n",
       "\n",
       "### Sources\n",
       "1. https://github.com/Microsoft/language-server-protocol/blob/gh-pages/_specifications/lsp/3.17/specification.md\n",
       "2. https://code.visualstudio.com/api/language-extensions/language-server-extension-guide\n",
       "3. https://medium.com/ballerina-techblog/practical-guide-for-the-language-server-protocol-3091a122b750\n",
       "\n",
       "## Security and Privacy Model\n",
       "\n",
       "FHIR ecosystems layer multiple controls:\n",
       "\n",
       "• Isolation boundaries – SMART‑on‑FHIR launches hand tokens to segregate the client, while FHIRcast adds unique hub.topic values and HMAC‑signed WebSub callbacks to prevent cross‑session data bleed. Hubs must reject unsecured HTTP and require opaque, unguessable callback URLs.\n",
       "\n",
       "• Context scoping – Standard OAuth 2.0 scopes (e.g., patient/*.read, user/*.*) constrain the dataset exposed to each app; FHIRcast re‑uses these scopes per session, limiting events to the active patient or encounter.\n",
       "\n",
       "• Consent workflows – The FHIR Consent resource stores granular rules (who, what, purpose). Authorization servers consult these records or downstream UMA/UDAP policies before issuing tokens, enabling patient‑directed or cascaded authorization across organisations.\n",
       "\n",
       "• Encryption in transit – HL7 mandates TLS for all REST, WebSub and WebSocket channels; servers should refuse plaintext endpoints and may employ mutual‑TLS or JWT‑based client auth for business‑to‑business exchanges.\n",
       "\n",
       "• Auditability – Every security‑relevant action is logged as an AuditEvent aligned with IHE‑ATNA. Entries capture who accessed which resource, outcome, and patient reference, supporting ONC certification, breach forensics and “accounting of disclosures” reports.\n",
       "\n",
       "Together these mechanisms deliver traceable, consent‑aware sharing while containing PHI to the minimum necessary context.\n",
       "\n",
       "### Sources\n",
       "1. https://fhircast.org/security-considerations/\n",
       "2. https://www.hl7.org/fhir/secpriv-module.html\n",
       "3. https://kodjin.com/blog/fhir-audit-events/\n",
       "\n",
       "## MCP Server Primitives: Prompts, Resources & Tools\n",
       "\n",
       "MCP defines three primitives that an LLM‑aware server may expose:\n",
       "\n",
       "- **Prompts (user‑controlled)** – reusable message templates chosen by users. Server must implement `list_prompts` and `get_prompt`; advertises capability `prompts` (+ optional `listChanged`).\n",
       "- **Resources (app‑controlled)** – read‑only context URIs (e.g. `file://…`). Handlers are registered with `@mcp.resource(pattern)` and revealed through `list_resources`. Capability flag: `resources` (+ `subscribe`).\n",
       "- **Tools (model‑controlled)** – side‑effecting functions callable by the model via `call_tool`. Registered with `@mcp.tool()`; exposed through `list_tools`. Capability flag: `tools` (+ `listChanged`).\n",
       "\n",
       "Lifecycle\n",
       "1. Registration at import time binds decorated functions to the FastMCP instance.\n",
       "2. During `initialize`, the server sends a capability list so clients can discover available primitives.\n",
       "3. At request time the SDK injects a `Context` object, giving access to progress, logging and `read_resource` helpers.\n",
       "4. Optional `lifespan` callback starts/tears down shared state once per server run.\n",
       "\n",
       "Sampling coordination\n",
       "Clients may supply a `sampling_callback` that MCP forwards whenever the server needs the model to produce text for a prompt or tool chain, enabling streaming or instrumentation of generation.\n",
       "\n",
       "```python\n",
       "from mcp.server.fastmcp import FastMCP\n",
       "mcp = FastMCP(\"Demo\")\n",
       "@mcp.prompt()\n",
       "def greet(name:str):\n",
       "    return f\"Hello, {name}!\"\n",
       "@mcp.resource(\"config://app\")\n",
       "def cfg():\n",
       "    return \"v1\"\n",
       "@mcp.tool()\n",
       "def add(a:int,b:int)->int:\n",
       "    return a+b\n",
       "```\n",
       "### Sources\n",
       "1. https://github.com/modelcontextprotocol/python-sdk\n",
       "2. https://pypi.org/project/mcp/\n",
       "3. https://ruslanmv.com/blog/Simple-MCP-Server-with-Python\n",
       "\n",
       "## Catalog of Notable MCP Servers\n",
       "\n",
       "• GitHub: Official server exposes issue search, pull‑request diff and repo metadata; ships with stdio & SSE transports, now in public preview with rewrite in Go, backed by GitHub engineering – signalling high production intent [1].\n",
       "\n",
       "• Slack: Node‑based server lets LLMs list channels, post messages, reply, react and fetch user data. Supports npx or Docker launch (stdio/SSE) and requires OAuth bot token; MIT‑licensed and maintained in the core MCP repo (active since 2024) [2].\n",
       "\n",
       "• Google Drive: Provides tools to search, read files and edit Sheets; resources map to gdrive:// URIs. Runs via Node + SSE, OAuth flow stores creds locally. Community project, forked from Anthropic code; small but evolving     [3].\n",
       "\n",
       "• PostgreSQL: Read‑only database gateway that auto‑discovers schema resources and executes SELECT queries. Invoked with `npx @modelcontextprotocol/server-postgres` or Docker; widely used in IDEs like VS Code agent mode; MIT licence [4].\n",
       "\n",
       "• Puppeteer: Browser automation server offering navigate, screenshot, click and evaluate tools plus console/screenshot resources. Headless Chromium via Docker or full browser via npx; allows custom launch options; part of official MCP servers, updated April 2025 [5].\n",
       "\n",
       "• Time: Lightweight Python server returning current time and timezone conversions; async implementation, uvx or pip install; stable 0.6 release Dec 2024 [6].\n",
       "\n",
       "### Sources\n",
       "1. https://github.blog/changelog/2025-04-04-github-mcp-server-public-preview/\n",
       "2. https://github.com/modelcontextprotocol/servers/blob/main/src/slack/README.md\n",
       "3. https://github.com/isaacphi/mcp-gdrive\n",
       "4. https://mcpservers.org/servers/modelcontextprotocol/postgres\n",
       "5. https://github.com/modelcontextprotocol/servers/blob/main/src/puppeteer/README.md\n",
       "6. https://github.com/modelcontextprotocol/servers/blob/main/src/time/README.md\n",
       "\n",
       "## Developer Tooling and SDKs\n",
       "\n",
       "• Official Python SDK (openai‑python) – 24 k★ GitHub library targeting Python 3.8+; auto‑generated from OpenAPI spec. Ships sync/async clients, SSE streaming, auto‑pagination, polling helpers (`*_and_poll`) and a CLI (`python -m openai chat …`). v1 rewrite (Nov 2023) added typed models, httpx transport and automatic retries.\n",
       "\n",
       "• Official TypeScript/Node SDK (openai‑node) – 8.8 k★; provides comprehensive TS types, works in Node 18+, Deno, Bun and edge runtimes. Supports SSE streaming, WebSocket realtime API, file uploads via `toFile`, and pluggable `fetch` for custom logging, proxies or middleware. Error subclasses (`RateLimitError`, `APIConnectionError`, etc.) and `maxRetries` simplify resilient code.\n",
       "\n",
       "• LangChain OpenAI Adapter – `langchain-openai` package exposes a drop‑in `ChatOpenAI` wrapper so LangChain chains can call the OpenAI SDK while remaining provider‑agnostic. Adapter mirrors `chat.completions.create` and streaming methods, easing model swaps during prototyping.\n",
       "\n",
       "• Agents SDK (openai‑agents‑python) – 8.6 k★ framework that layers tracing, guardrails, function‑tool decorators and multi‑agent orchestration atop the Python SDK. Useful for testing complex workflows and inspecting runs via built‑in trace viewer.\n",
       "\n",
       "Together these tools give developers typed APIs, CLI playgrounds, modern runtime support and higher‑level abstractions for rapid, testable OpenAI integrations.\n",
       "\n",
       "### Sources\n",
       "1. https://github.com/openai/openai-python\n",
       "2. https://github.com/openai/openai-node\n",
       "3. https://python.langchain.com/docs/integrations/adapters/openai/\n",
       "4. https://github.com/openai/openai-agents-python\n",
       "\n",
       "## Building a Custom MCP Server – Step‑by‑Step Guide\n",
       "\n",
       "1. Scaffold\n",
       "```ts\n",
       "import { createServer } from '@anthropic-ai/mcp-js';\n",
       "const server = createServer({\n",
       "  name: 'weather-mcp',\n",
       "  version: '1.0.0',\n",
       "  capabilities: { tools:{}, resources:{}, prompts:{} }\n",
       "});\n",
       "server.listen(); // stdio transport\n",
       "```\n",
       "\n",
       "2. Capability design\n",
       "• Tools: validate arguments with JSON Schema/Zod, return `{content:[…]}`.\n",
       "• Resources: map stable URIs (e.g. `city://{name}`) and respond to `resources/read`.\n",
       "• Prompts: expose templates via `prompts/get`, include argument metadata.\n",
       "\n",
       "3. Robust error handling\n",
       "• Wrap each handler in `try/catch`; throw JSON‑RPC errors so clients see codes.\n",
       "• Write diagnostics to **stderr** only; stdout is reserved for protocol frames.\n",
       "• Use `loggingNotification` with INFO‑ERROR levels for structured logs.\n",
       "\n",
       "4. Deployment patterns\n",
       "• Local: `npm link` / `pip install -e .` for instant stdio testing.\n",
       "• Remote: expose SSE (`uvicorn.run(server.sse_app(), …)` or Spring Boot `spring-ai-starter-mcp-server-webmvc`).\n",
       "• VM/Container: front with Nginx, force HTTPS, open only the SSE port.\n",
       "\n",
       "5. Production best practices\n",
       "• Require API keys or OAuth; add rate‑limits.\n",
       "• Enable change notifications (`tool-change-notification=true`) so clients auto‑refresh.\n",
       "• Keep payloads < 4 MB (`maxMessageSize`), monitor latency, and back‑off on 4xx/5xx.\n",
       "\n",
       "### Sources\n",
       "1. https://collabnix.com/how-to-build-and-host-your-own-mcp-servers-in-easy-steps/\n",
       "2. https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html\n",
       "3. https://medium.com/@logeshgcp/building-your-first-model-context-protocol-mcp-server-and-client-a-comprehensive-guide-a88d415e3dfb\n",
       "4. https://modelcontextprotocol.io/docs/tools/debugging\n",
       "\n",
       "## Integration Patterns for LLM Applications\n",
       "\n",
       "MCP (Model Context Protocol) lets LLM agents call external tools through a uniform JSON‑RPC layer. Common integration routes:\n",
       "\n",
       "• LangChain/ LangGraph: `langchain-mcp-adapters` wraps any MCP server as LangChain tools. A single line\n",
       "```python\n",
       "client = MultiServerMCPClient({\"gmail\": {...}, \"math\": {...}})\n",
       "```\n",
       "pulls schemas from many servers; routing logic inside the client selects the correct transport (stdio vs SSE URL) and exposes a merged toolset to the agent. [3]\n",
       "\n",
       "• IDEs (Cursor, Zed, Replit, VS Code extensions such as Continue/Cline) embed an MCP client to surface file, git or terminal tools. The IDE forwards context (open file, cursor pos) and receives structured actions, enabling code edits or lint runs without custom per‑tool code. Early adopters report sharper context‑aware completion and autonomous refactors. [4]\n",
       "\n",
       "• SaaS hosts (Composio) offer managed multi‑tenant MCP servers; developers only paste the server URL in the adapter. The agent automatically discovers >100 tools (Gmail, Trello, GitHub) and reasons which to invoke, dramatically cutting integration time. Step‑by‑step Node.js example clocks in under 50 lines. [1]\n",
       "\n",
       "Design tips:\n",
       "– Group servers by latency or auth domain; instantiate separate `MultiServerMCPClient`s to avoid cross‑tenant leakage.\n",
       "– Use predictor‑based routing: let the LLM choose the `name` field that maps back to a target server/tool.\n",
       "– Stream results via SSE for long‑running tasks while keeping quick math/FS ops on stdio.\n",
       "\n",
       "### Sources\n",
       "1. https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents/\n",
       "2. https://github.com/langchain-ai/langchain-mcp-adapters\n",
       "3. https://daily.dev/blog/what-is-mcp-model-context-protocol\n",
       "\n",
       "\n",
       "## Limitations, Challenges, and Future Roadmap\n",
       "\n",
       "**Current limits**\n",
       "- Context remains costly and imperfect: GPT‑4o handles 128 K; new GPT‑4.1 raises this to 1 M, yet benchmark recall stays <62 % and computer‑use accuracy 38 % (OSWorld) [1][2].\n",
       "- Fragmented APIs: teams juggle Chat Completions, Assistants, and the just‑launched Responses API; Azure still lacks JSON/function support for vision calls, widening divergence [1][4].\n",
       "- Hard ceilings: max 128 functions, 128 K tokens for many models, and Azure’s preview quotas throttle large‑scale RAG.\n",
       "\n",
       "**Operational challenges**\n",
       "- Spec churn forces rewrites; OpenAI plans to retire Assistants “first half 2026,” pushing developers to migrate again [2][3].\n",
       "- Rolling model deprecations (e.g., monthly preview APIs, Turbo upgrades) demand constant retesting and budget resets [1][4].\n",
       "- Built‑in tools (search, file, computer) add new per‑query fees and raise dependency on closed infrastructure.\n",
       "\n",
       "**Roadmap signals**\n",
       "- Responses API will absorb Assistants features, becoming the long‑term standard; Chat Completions continues for “stateless” use [2][3].\n",
       "- More native tools—web search, vector RAG, computer‑use—expected, with server‑side state and tracing baked in [2].\n",
       "- Context race: GPT‑4.1 family offers 1 M tokens, while Anthropic’s Claude 3 ships 200 K today and advertises “>1 M for select users,” hinting at multi‑million‑token competition [5].\n",
       "- Enterprise asks (CMK, private VNet, batch discounts) appear monthly on Azure’s preview track, indicating ongoing parity work with OpenAI cloud [4].\n",
       "\n",
       "Developers should design abstraction layers, monitor changelogs monthly, and budget for periodic migrations as the ecosystem converges on the Responses paradigm.\n",
       "\n",
       "### Sources\n",
       "1. https://platform.openai.com/docs/changelog\n",
       "2. https://openai.com/index/new-tools-for-building-agents/\n",
       "3. https://simonwillison.net/2025/Mar/11/responses-vs-chat-completions/\n",
       "4. https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new\n",
       "5. https://www.anthropic.com/news/claude-3-family\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The Model Context Protocol brings the rigor of well‑defined network standards to the messy world of LLM integrations.  By wrapping every external data source or action inside a small MCP server, developers gain a repeatable recipe for delivering fresh context to models while preserving security boundaries and auditability.  JSON‑RPC framing, capability negotiation, and a pluggable transport layer make the protocol easy to embed in desktop apps, cloud agents, and serverless back‑ends alike.\n",
       "\n",
       "Key insights from this report:\n",
       "\n",
       "• The client–host–server split cleanly isolates orchestration logic (host), model‑facing logic (client), and domain expertise (server), making each part independently testable and deployable.\n",
       "\n",
       "• Prompts, resources, and tools form a minimal yet expressive surface for servers to expose capabilities.  Their tight coupling with capability flags ensures that models only see what they are allowed to use.\n",
       "\n",
       "• A vibrant ecosystem of open‑source servers—spanning GitHub, Slack, Postgres, Puppeteer, weather feeds, and math helpers—demonstrates MCP’s versatility, while official SDKs and LangChain adapters lower the barrier to entry.\n",
       "\n",
       "• Security considerations are first‑class: context scoping, per‑server isolation, consent workflows, and transport‑level encryption help organizations comply with privacy regulations without stifling innovation.\n",
       "\n",
       "• Remaining challenges include standardized authentication, context‑window budgeting across many servers, and maintaining backward compatibility as the spec evolves.  The published roadmap and open governance model position MCP to tackle these issues collaboratively.\n",
       "\n",
       "For developers building agentic systems or enterprise copilots, adopting MCP now offers a strategic advantage: you can focus on higher‑value reasoning and product logic while delegating data plumbing to a growing catalogue of interchangeable, community‑maintained connectors.  As the protocol matures, expect MCP to become as foundational to AI tooling as HTTP is to web services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(agent.get_state(thread_config).values['final_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-deep-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
