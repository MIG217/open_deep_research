{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge ending at unknown node `supervisor`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 280\u001b[0m\n\u001b[1;32m    278\u001b[0m supervisor_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor\u001b[39m\u001b[38;5;124m\"\u001b[39m, supervisor)\n\u001b[1;32m    279\u001b[0m supervisor_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor_tools\u001b[39m\u001b[38;5;124m\"\u001b[39m, supervisor_tools)\n\u001b[0;32m--> 280\u001b[0m supervisor_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresearch_team\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresearch_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Flow of the supervisor agent\u001b[39;00m\n\u001b[1;32m    283\u001b[0m supervisor_builder\u001b[38;5;241m.\u001b[39madd_edge(START, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/graph/state.py:599\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[0;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug, name)\u001b[0m\n\u001b[1;32m    596\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[1;32m    608\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m     ]\n\u001b[1;32m    617\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/graph/graph.py:292\u001b[0m, in \u001b[0;36mGraph.validate\u001b[0;34m(self, interrupt)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m END:\n\u001b[0;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge ending at unknown node `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# validate interrupts\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupt:\n",
      "\u001b[0;31mValueError\u001b[0m: Found edge ending at unknown node `supervisor`"
     ]
    }
   ],
   "source": [
    "from typing import List,  Annotated, TypedDict, operator, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "## LLM \n",
    "llm = init_chat_model(\n",
    "    model=\"openai:o3-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "## Tools \n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    ")\n",
    "\n",
    "@tool\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Research scope for this section of the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the section.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Introduction(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the introduction, giving an overview of the report.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Conclusion(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for the conclusion of the report.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the conclusion, summarizing the report.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class Done(BaseModel):\n",
    "      \"\"\"Tool to signal that the work is complete.\"\"\"\n",
    "      done: bool\n",
    "\n",
    "\n",
    "## State\n",
    "class ReportStateOutput(TypedDict):\n",
    "    final_report: str # Final report\n",
    "\n",
    "class ReportState(MessagesState):\n",
    "    sections: list[Section] # List of report sections \n",
    "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
    "    final_report: str # Final report\n",
    "\n",
    "class SectionState(MessagesState):\n",
    "    section: Section # Report section  \n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SectionOutputState(TypedDict):\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "## Supervisor\n",
    "\n",
    "SUPERVISOR_INSTRUCTIONS = \"\"\"\n",
    "You are scoping research for a report based on a user-provided topic.\n",
    "\n",
    "### Your responsibilities:\n",
    "\n",
    "1. **Clarify the Topic**  \n",
    "   Engage with the user to clarify their intent. Ask follow-up questions to fully understand the topic, goals, constraints, and any preferences for the report.\n",
    "\n",
    "2. **Gather Background Information**  \n",
    "   Use the `tavily_search_tool` to collect relevant information about the topic. Use multiple focused queries to build context from different angles.\n",
    "\n",
    "3. **Define Report Structure**  \n",
    "   Once you understand the topic:\n",
    "   - Use the `Sections` tool to define a structured outline of the report.\n",
    "   - Each section should include a clear name and scope description.\n",
    "   - Leave the content preview empty because the research agent will fill it in.\n",
    "   - Ensure sections are scoped to be independently researchable.\n",
    "\n",
    "4. **Assemble the Final Report**  \n",
    "   When all sections are returned:\n",
    "   - Use the `Introduction` tool to generate a clear and informative introduction.\n",
    "   - Use the `Conclusion` tool to summarize key insights and close the report.\n",
    "\n",
    "5. **Finish the Workflow**  \n",
    "   When the final report is complete, call the `Done` tool to signal that the work is finished.\n",
    "\n",
    "### Additional Notes:\n",
    "- You are a reasoning model. Think through problems step-by-step before acting.\n",
    "- Use your tools wisely—especially the search tool—to improve research quality.\n",
    "- Maintain a clear, informative, and professional tone throughout.\"\"\"\n",
    "\n",
    "RESEARCH_INSTRUCTIONS = \"\"\"\n",
    "You are a researcher responsible for completing a specific section of a report.\n",
    "\n",
    "### Your goals:\n",
    "\n",
    "1. **Understand the Section Scope**  \n",
    "   Begin by reviewing the section name and description. This defines your research focus. Use it as your objective.\n",
    "\n",
    "<Section Name>\n",
    "{section_name}\n",
    "</Section Name>\n",
    "\n",
    "<Section Description>\n",
    "{section_description}\n",
    "</Section Description>\n",
    "\n",
    "2. **Research the Topic**  \n",
    "   Use the `tavily_search_tool` to gather relevant information and evidence. Search iteratively if needed to fully understand the section’s scope.\n",
    "\n",
    "3. **Use the Section Tool**  \n",
    "   Once you’ve gathered sufficient context, write a high-quality called the Section tool to write the section. Your content should:\n",
    "   - `name`: The title of the section\n",
    "   - `description`: The scope of research you completed \n",
    "   - `content`: The completed body of text for the section\n",
    "\n",
    "---\n",
    "\n",
    "### Reasoning Guidance\n",
    "\n",
    "You are a reasoning model. Think through the task step-by-step before writing. Break down complex questions. If you're unsure about something, search again.\n",
    "\n",
    "- You may reason internally before producing content.\n",
    "- Your job is not to summarize randomly—it's to **research and synthesize a strong, scoped contribution** to a report.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- Do not write introductions or conclusions unless explicitly part of your section.\n",
    "- Keep a professional, factual tone.\n",
    "- If you do not have enough information to complete the section, search again or clarify your approach before continuing.\n",
    "\"\"\"\n",
    "\n",
    "# Tools\n",
    "supervisor_tools = [tavily_search_tool, Sections, Introduction, Conclusion]\n",
    "supervisor_tools_by_name = {tool.name: tool for tool in supervisor_tools}\n",
    "\n",
    "research_tools = [tavily_search_tool, Section]\n",
    "research_tools_by_name = {tool.name: tool for tool in research_tools}\n",
    "\n",
    "def supervisor(state: ReportState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    # Messages\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Check if we have research completed\n",
    "    if state.get(\"sections\"):\n",
    "        research_complete_message = {\"role\": \"user\", \"content\": \"Research is complete. Write the introduction and conclusion.\\n\\n\" + \"\\n\\n\".join([s.content for s in state[\"sections\"]])}\n",
    "        messages = messages + [research_complete_message]\n",
    "\n",
    "    # Invoke LLM\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm.bind_tools(supervisor_tools_by_name).invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": SUPERVISOR_INSTRUCTIONS,\n",
    "                    }\n",
    "                ]\n",
    "                + messages\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def supervisor_tools(state: ReportState)  -> Command[Literal[\"supervisor\", \"research_team\", \"__end__\"]]:\n",
    "    \"\"\"Performs the tool call and sends to the research agent\"\"\"\n",
    "\n",
    "    result = []\n",
    "    # Get the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Get the tool\n",
    "        tool = supervisor_tools_by_name[tool_call[\"name\"]]\n",
    "        # Perform the tool call\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        # Append to messages \n",
    "        result.append([{\"role\": \"tool\", \n",
    "                        \"content\": observation, \n",
    "                        \"name\": tool_call[\"name\"], \n",
    "                        \"tool_call_id\": tool_call[\"id\"]}])\n",
    "        # Update state depending on the tool call \n",
    "        if tool_call[\"name\"] == \"Sections\":\n",
    "            return Command(goto=[Send(\"research_team\", {\"section\": s}) for s in state['sections']])\n",
    "        if tool_call[\"name\"] == \"Introduction\":\n",
    "            return Command(goto=\"supervisor\", update={\"sections\": [tool_call[\"args\"]] + state[\"sections\"], \"messages\": result})\n",
    "        if tool_call[\"name\"] == \"Conclusion\":\n",
    "            return Command(goto=\"supervisor\", update={\"sections\": state[\"sections\"] + [tool_call[\"args\"]], \"messages\": result})\n",
    "        else:\n",
    "            return Command(goto=\"supervisor\", update={\"messages\": result})\n",
    "\n",
    "def research_agent(state: SectionState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            # Enforce tool calling to either perform more search or call the Section tool to write the section\n",
    "            init_chat_model(\"openai:o3\", tool_choice=\"required\", temperature=0.0).bind_tools(research_tools).invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": RESEARCH_INSTRUCTIONS.format(section_name=state[\"section\"].name, section_description=state[\"section\"].description)\n",
    "                    }\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def research_agent_tools(state: SectionState) -> Command[Literal[\"supervisor\", \"research_agent\"]]:\n",
    "    \"\"\"Performs the tool call and route to supervisor or continue the research loop\"\"\"\n",
    "\n",
    "    result = []\n",
    "    # Get the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Get the tool\n",
    "        tool = research_tools_by_name[tool_call[\"name\"]]\n",
    "        # Perform the tool call\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        # Append to messages \n",
    "        result.append([{\"role\": \"tool\", \n",
    "                        \"content\": observation, \n",
    "                        \"name\": tool_call[\"name\"], \n",
    "                        \"tool_call_id\": tool_call[\"id\"]}])\n",
    "        # It it wrote the section, send it to the supervisor\n",
    "        if tool_call[\"name\"] == \"Section\":\n",
    "            return Command(goto=\"supervisor\", update={\"messages\": result, \"completed_sections\": [tool_call[\"args\"]]})\n",
    "        # Otherwise continue the research loop\n",
    "        elif tool_call[\"name\"] == \"tavily_search_tool\":\n",
    "            return Command(goto=\"research_agent\", update={\"messages\": result})\n",
    "        else:\n",
    "            return ValueError(f\"Tool {tool_call['name']} invalid\")\n",
    "\n",
    "def should_continue(state: ReportState) -> Literal[\"supervisor_tools\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"supervisor_tools\"\n",
    "\n",
    "    # Otherwise research is complete and we can assemble the final report\n",
    "    all_sections = \"\\n\\n\".join([s.content for s in state[\"sections\"]])\n",
    "    return Command(goto=END, update={\"final_report\": all_sections})\n",
    "\n",
    "# Research agent workflow\n",
    "research_builder = StateGraph(SectionState, output=SectionOutputState)\n",
    "research_builder.add_node(\"research_agent\", research_agent)\n",
    "research_builder.add_node(\"research_agent_tools\", research_agent_tools)\n",
    "research_builder.add_edge(START, \"research_agent\") \n",
    "\n",
    "# Build workflow\n",
    "supervisor_builder = StateGraph(ReportState, input=MessagesState, output=ReportStateOutput)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "supervisor_builder.add_node(\"research_team\", research_builder.compile())\n",
    "\n",
    "# Flow of the supervisor agent\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"supervisor_tools\": \"supervisor_tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the supervisor agent\n",
    "agent = supervisor_builder.compile(name=\"research_team\")\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11b494f10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "agent_builder = StateGraph(ReportState, input=MessagesState,output=ReportStateOutput)\n",
    "agent_builder.add_node(\"supervisor\", supervisor)\n",
    "agent_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "agent_builder.add_node(\"research_agent\", research_agent)\n",
    "agent_builder.add_node(\"research_agent_tools\", research_agent_tools)\n",
    "\n",
    "agent_builder.add_edge(START, \"supervisor\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"supervisor_tools\": \"supervisor_tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "agent = agent_builder.compile(name=\"research_team\")\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-deep-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
