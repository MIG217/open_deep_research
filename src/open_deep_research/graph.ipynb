{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rlm/Desktop/Code/open_deep_research/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlm/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U -q open-deep-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.11\n"
     ]
    }
   ],
   "source": [
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.graph import builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Set the API keys used for any model or search tool selections below, such as:\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GROQ_API_KEY\")\n",
    "_set_env(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please provide feedback on the following report plan. \n",
       "                        \n",
       "\n",
       "Section: Introduction\n",
       "Description: A brief overview of the AI inference market, its growth, and the importance of efficient inference solutions. Introduce Fireworks, Together.ai, and Groq as key players in the market.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "Section: Market Overview and Trends\n",
       "Description: An analysis of the overall AI inference market including key trends, challenges, and drivers such as performance demands, cost efficiency, scalability, and adoption by businesses.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Detailed Analysis of Providers\n",
       "Description: A deep dive into the specifics of Fireworks, Together.ai, and Groq. This section will cover performance metrics, scalability, pricing models, and developer experience for each provider.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Comparative Evaluation\n",
       "Description: A side-by-side comparison of the three providers highlighting key differences and similarities in terms of performance, cost-effectiveness, scalability, and overall value.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Conclusion and Recommendations\n",
       "Description: A concise summary of the main findings, including a distilled table or list of key insights from the analysis and recommendations based on the comparative evaluation.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "                        \n",
       "Does the report plan meet your needs?\n",
       "Pass 'true' to approve the report plan.\n",
       "Or, provide feedback to regenerate the report plan:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid \n",
    "from IPython.display import Markdown\n",
    "\n",
    "REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "1. Introduction (no research needed)\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the report\"\"\"\n",
    "\n",
    "# Claude 3.7 Sonnet for planning with perplexity search\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"perplexity\",\n",
    "                           \"planner_provider\": \"anthropic\",\n",
    "                           \"planner_model\": \"claude-3-7-sonnet-latest\",\n",
    "                           # \"planner_model_kwargs\": {\"temperature\":0.8}, # if set custom parameters\n",
    "                           \"writer_provider\": \"anthropic\",\n",
    "                           \"writer_model\": \"claude-3-5-sonnet-latest\",\n",
    "                           # \"writer_model_kwargs\": {\"temperature\":0.8}, # if set custom parameters\n",
    "                           \"max_search_depth\": 2,\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           }}\n",
    "\n",
    "# DeepSeek-R1-Distill-Llama-70B for planning and llama-3.3-70b-versatile for writing\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"planner_provider\": \"groq\",\n",
    "                           \"planner_model\": \"deepseek-r1-distill-llama-70b\",\n",
    "                           \"writer_provider\": \"groq\",\n",
    "                           \"writer_model\": \"llama-3.3-70b-versatile\",\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           \"max_search_depth\": 1,}\n",
    "                           }\n",
    "\n",
    "# Fast config (less search depth) with o3-mini for planning and Claude 3.5 Sonnet for writing\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"planner_provider\": \"openai\",\n",
    "                           \"planner_model\": \"o3-mini\",\n",
    "                           \"writer_provider\": \"anthropic\",\n",
    "                           \"writer_model\": \"claude-3-5-sonnet-latest\",\n",
    "                           \"max_search_depth\": 1,\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           }}\n",
    "\n",
    "# Create a topic\n",
    "topic = \"Overview of the AI inference market with focus on Fireworks, Together.ai, Groq\"\n",
    "\n",
    "# Run the graph until the interruption\n",
    "async for event in graph.astream({\"topic\":topic,}, thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please provide feedback on the following report plan. \n",
       "                        \n",
       "\n",
       "Section: Introduction\n",
       "Description: Provides a brief overview of the AI inference market and introduces the key players: Fireworks AI, Together.ai, and Groq.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "Section: Market Overview\n",
       "Description: An analysis of the overall AI inference market, including trends, demand, cost-efficiency, and scalability factors within the sector.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Fireworks AI Analysis with Revenue Estimates\n",
       "Description: A focused look at Fireworks AI covering its performance, cost models, scalability, developer experience, and estimated annual recurring revenue (ARR).\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Together.ai Analysis with Revenue Estimates\n",
       "Description: A detailed examination of Together.ai concentrating on its training capabilities, cost efficiency in GPU usage, enterprise features, and revenue (ARR) estimates.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Groq Analysis with Revenue Estimates\n",
       "Description: A section dedicated to Groq, reviewing its unique hardware optimizations for inference, pricing models, performance metrics, and its estimated ARR.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Comparative Analysis\n",
       "Description: A side-by-side comparison of Fireworks AI, Together.ai, and Groq, highlighting their strengths, weaknesses, cost-effectiveness, and revenue implications to help stakeholders decide.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Conclusion\n",
       "Description: Summarizes the report findings, distills the main points into a concise table or list, and provides a final assessment of the market and the key players.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "                        \n",
       "Does the report plan meet your needs?\n",
       "Pass 'true' to approve the report plan.\n",
       "Or, provide feedback to regenerate the report plan:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass feedback to update the report plan  \n",
    "async for event in graph.astream(Command(resume=\"Include individual sections for Together.ai, Groq, and Fireworks with revenue estimates (ARR)\"), thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human_feedback': None}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Together.ai Analysis with Revenue Estimates', description='A detailed examination of Together.ai concentrating on its training capabilities, cost efficiency in GPU usage, enterprise features, and revenue (ARR) estimates.', research=True, content=\"Based on the provided source material, I'll write a comprehensive analysis of Together.ai with revenue estimates:\\n\\n## Together.ai Analysis with Revenue Estimates\\n\\nTogether.ai has demonstrated remarkable growth since its founding in 2022, reaching significant revenue milestones in rapid succession. The company achieved $44 million in annual recurring revenue (ARR) by April 2024, representing a staggering 2,095% year-over-year increase, and further accelerated to $100 million ARR by July 2024 [1]. Current estimates suggest Together.ai's ARR has reached approximately $130 million as of early 2025 [2].\\n\\nThe company's growth is driven by its position as a developer-focused platform for AI infrastructure, offering both GPU compute resources and comprehensive tooling for training and deploying open-source AI models. Together.ai differentiates itself through token-based pricing rather than hourly rates, which appeals particularly to startups with variable workloads [3].\\n\\nTogether.ai's recent $228.5 million in funding and $1.25 billion valuation reflect investor confidence in its business model, which maintains approximately 45% gross margins despite operating in the competitive cloud GPU market [4]. The company has secured strategic partnerships with major industry players like Dell Technologies and Hypertec Cloud to expand its infrastructure capabilities [5].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://www.together.ai/enterprise\\n[4] https://sacra.com/c/together-ai/\\n[5] https://www.dell.com/en-us/dt/corporate/newsroom/announcements/detailpage.press-releases~usa~2025~03~together-ai-taps-dell-technologies-to-scale-ai-acceleration-cloud-for-the-enterprise.htm\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Groq Analysis with Revenue Estimates', description='A section dedicated to Groq, reviewing its unique hardware optimizations for inference, pricing models, performance metrics, and its estimated ARR.', research=True, content=\"Here's my synthesized section on Groq's Analysis with Revenue Estimates:\\n\\n## Groq Analysis with Revenue Estimates\\n\\nGroq has demonstrated impressive technological capabilities with its Language Processing Unit (LPU) for AI inference, though its revenue figures remain modest as it scales up operations. The company's estimated annual revenue is currently $46.6M, with revenue per employee of approximately $116,842 [1][2]. Groq has raised significant funding totaling $1B and achieved a valuation of $2.8B as of August 2024 [1][2].\\n\\nThe company's competitive positioning centers on its unique TSP (Tensor Streaming Processor) architecture that delivers superior low-latency performance for AI inference workloads. Groq's LPU has significant cost advantages in manufacturing, with wafer costs estimated at under $6,000 on 14nm compared to Nvidia's $16,000 for 5nm H100 wafers. This translates to a 70% lower raw silicon cost per token, though system-level costs narrow the total cost of ownership advantage to around 40% [3].\\n\\nGroq monetizes its technology through multiple revenue streams: cloud-based inference API services, hardware leasing and colocation services, and direct hardware sales for data centers [4]. The company has secured major enterprise deployments in Norway and Saudi Arabia, indicating growing commercial traction [4]. While still an emerging player, Groq's aggressive pricing and performance advantages position it to potentially capture a meaningful share of the rapidly growing AI inference market.\\n\\n### Sources\\n[1] https://growjo.com/company/Groq\\n[2] https://compworth.com/company/groq \\n[3] https://sacra.com/c/groq/\\n[4] https://www.chipstrat.com/p/groqs-business-model-part-3-competing\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Fireworks AI Analysis with Revenue Estimates', description='A focused look at Fireworks AI covering its performance, cost models, scalability, developer experience, and estimated annual recurring revenue (ARR).', research=True, content='## Fireworks AI Analysis with Revenue Estimates\\n\\nFireworks AI has established itself as a leading platform for serving open-source AI models with competitive performance and pricing. The company offers multiple deployment options including serverless inference, on-demand deployments, and enterprise reserved GPUs [1].\\n\\nKey strengths include superior performance, with their proprietary FireAttention serving stack delivering up to 2.5x faster generation speeds and 3.5x higher throughput compared to competitors like HuggingFace TGI on the same hardware [2]. Their on-demand deployments achieve 6x lower costs while maintaining high performance.\\n\\nThe platform supports a comprehensive model ecosystem including popular open-source models like Llama, DeepSeek, and Mixtral across text, vision, audio and embedding modalities [3]. Their serverless models feature industry-leading speeds of up to 300 tokens/second for models like Mixtral, with rate limits of 600 requests/minute [2].\\n\\nPricing is competitive and usage-based, with options like:\\n- Serverless inference with per-token pricing starting at $0.20 per million tokens\\n- On-demand GPU deployments at $3.89 per hour for A100s\\n- Enterprise plans with custom pricing based on reserved capacity [2]\\n\\nWhile private financial data is limited, given their technology advantages and major customers like Tome, Quora and Sourcegraph [2], estimated annual recurring revenue (ARR) is likely in the $10-20M range based on market comparables.\\n\\n### Sources\\n[1] https://docs.fireworks.ai/getting-started/introduction \\n[2] https://fireworks.ai/blog/spring-update-faster-models-dedicated-deployments-postpaid-pricing\\n[3] https://artificialanalysis.ai/providers/fireworks')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Comparative Analysis', description='A side-by-side comparison of Fireworks AI, Together.ai, and Groq, highlighting their strengths, weaknesses, cost-effectiveness, and revenue implications to help stakeholders decide.', research=True, content='## Comparative Analysis\\n\\nBased on the provided source material, here is a side-by-side comparison of Fireworks AI, Together.ai, and Groq highlighting their key strengths and differentiators:\\n\\nPerformance:\\n- Groq leads in raw inference speed, hitting 250-450 tokens/second with their custom LPU hardware\\n- Fireworks AI achieves around 70-100 tokens/second with optimized software on NVIDIA GPUs\\n- Together.ai delivers 80-90 tokens/second using GPU clusters, with strong reliability\\n\\nCost-Effectiveness:\\n- Together.ai offers competitive pricing at $0.10-0.20 per million tokens for base models\\n- Fireworks AI focuses on cost-efficient inference with rates around $0.20-0.50 per million tokens\\n- Groq balances premium performance with midrange pricing of $0.60-0.70 per million tokens\\n\\nKey Advantages:\\n- Fireworks AI: Strong developer experience, fast API deployment, extensive model support, and consistent performance at scale\\n- Together.ai: Best for AI training, robust fine-tuning capabilities, private model ownership, and enterprise features\\n- Groq: Fastest raw inference speeds, custom hardware optimization, and ultra-low latency for real-time applications\\n\\nBest Suited For:\\n- Fireworks AI: Startups and teams needing reliable, cost-effective inference with good developer tools\\n- Together.ai: Enterprises requiring training capabilities, model customization, and scalable infrastructure\\n- Groq: Applications demanding maximum speed and lowest possible latency\\n\\n### Sources:\\n[1] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[2] https://artificialanalysis.ai/models/llama-3-instruct-70b/providers\\n[3] https://www.helicone.ai/blog/llm-api-providers\\n[4] https://sacra.com/c/groq/')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Market Overview', description='An analysis of the overall AI inference market, including trends, demand, cost-efficiency, and scalability factors within the sector.', research=True, content=\"Here's my synthesis of the market overview section focused on the AI inference market with a focus on Fireworks, Together.ai, and Groq:\\n\\n## Market Overview\\n\\nThe AI inference market is experiencing rapid growth, projected to reach between $780 billion and $990 billion by 2027, with a compound annual growth rate of 19.2% [1]. This expansion is driven by increasing demand for real-time processing capabilities and the need for efficient AI model deployment across industries.\\n\\nIn the competitive landscape, each provider has distinct advantages. Groq leads in raw inference speed, achieving up to 250 tokens per second with their custom hardware [2]. Fireworks AI differentiates itself through cost-effective inference and consistent performance across varying workload sizes, particularly excelling at processing large context windows up to 10K tokens [3]. Together.ai focuses on AI training capabilities with access to over 10,000 GPUs, though their inference speeds can be variable under heavy loads [4].\\n\\nThe market is seeing significant innovation in infrastructure optimization, with providers taking different approaches. While some like Groq utilize custom chips for maximum speed, others like Fireworks AI focus on optimizing conventional GPU infrastructure for better cost-efficiency and flexibility [3]. This diversification in approaches helps serve different customer needs, from enterprises requiring maximum speed to startups seeking cost-effective solutions.\\n\\n### Sources\\n[1] https://www.globenewswire.com/news-release/2025/04/21/3064502/0/en/AI-Inference-Market-Forecast-Report-to-2030-with-Case-Studies-of-Intel-Siemens-Healthineers-Nvidia-Eleuther-AI.html\\n[2] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[3] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\\n[4] https://www.peerspot.com/products/comparisons/fireworks-ai_vs_together-inference\")]}}\n",
      "\n",
      "\n",
      "{'gather_completed_sections': {'report_sections_from_research': \"\\n============================================================\\nSection 1: Market Overview\\n============================================================\\nDescription:\\nAn analysis of the overall AI inference market, including trends, demand, cost-efficiency, and scalability factors within the sector.\\nRequires Research: \\nTrue\\n\\nContent:\\nHere's my synthesis of the market overview section focused on the AI inference market with a focus on Fireworks, Together.ai, and Groq:\\n\\n## Market Overview\\n\\nThe AI inference market is experiencing rapid growth, projected to reach between $780 billion and $990 billion by 2027, with a compound annual growth rate of 19.2% [1]. This expansion is driven by increasing demand for real-time processing capabilities and the need for efficient AI model deployment across industries.\\n\\nIn the competitive landscape, each provider has distinct advantages. Groq leads in raw inference speed, achieving up to 250 tokens per second with their custom hardware [2]. Fireworks AI differentiates itself through cost-effective inference and consistent performance across varying workload sizes, particularly excelling at processing large context windows up to 10K tokens [3]. Together.ai focuses on AI training capabilities with access to over 10,000 GPUs, though their inference speeds can be variable under heavy loads [4].\\n\\nThe market is seeing significant innovation in infrastructure optimization, with providers taking different approaches. While some like Groq utilize custom chips for maximum speed, others like Fireworks AI focus on optimizing conventional GPU infrastructure for better cost-efficiency and flexibility [3]. This diversification in approaches helps serve different customer needs, from enterprises requiring maximum speed to startups seeking cost-effective solutions.\\n\\n### Sources\\n[1] https://www.globenewswire.com/news-release/2025/04/21/3064502/0/en/AI-Inference-Market-Forecast-Report-to-2030-with-Case-Studies-of-Intel-Siemens-Healthineers-Nvidia-Eleuther-AI.html\\n[2] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[3] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\\n[4] https://www.peerspot.com/products/comparisons/fireworks-ai_vs_together-inference\\n\\n\\n============================================================\\nSection 2: Fireworks AI Analysis with Revenue Estimates\\n============================================================\\nDescription:\\nA focused look at Fireworks AI covering its performance, cost models, scalability, developer experience, and estimated annual recurring revenue (ARR).\\nRequires Research: \\nTrue\\n\\nContent:\\n## Fireworks AI Analysis with Revenue Estimates\\n\\nFireworks AI has established itself as a leading platform for serving open-source AI models with competitive performance and pricing. The company offers multiple deployment options including serverless inference, on-demand deployments, and enterprise reserved GPUs [1].\\n\\nKey strengths include superior performance, with their proprietary FireAttention serving stack delivering up to 2.5x faster generation speeds and 3.5x higher throughput compared to competitors like HuggingFace TGI on the same hardware [2]. Their on-demand deployments achieve 6x lower costs while maintaining high performance.\\n\\nThe platform supports a comprehensive model ecosystem including popular open-source models like Llama, DeepSeek, and Mixtral across text, vision, audio and embedding modalities [3]. Their serverless models feature industry-leading speeds of up to 300 tokens/second for models like Mixtral, with rate limits of 600 requests/minute [2].\\n\\nPricing is competitive and usage-based, with options like:\\n- Serverless inference with per-token pricing starting at $0.20 per million tokens\\n- On-demand GPU deployments at $3.89 per hour for A100s\\n- Enterprise plans with custom pricing based on reserved capacity [2]\\n\\nWhile private financial data is limited, given their technology advantages and major customers like Tome, Quora and Sourcegraph [2], estimated annual recurring revenue (ARR) is likely in the $10-20M range based on market comparables.\\n\\n### Sources\\n[1] https://docs.fireworks.ai/getting-started/introduction \\n[2] https://fireworks.ai/blog/spring-update-faster-models-dedicated-deployments-postpaid-pricing\\n[3] https://artificialanalysis.ai/providers/fireworks\\n\\n\\n============================================================\\nSection 3: Together.ai Analysis with Revenue Estimates\\n============================================================\\nDescription:\\nA detailed examination of Together.ai concentrating on its training capabilities, cost efficiency in GPU usage, enterprise features, and revenue (ARR) estimates.\\nRequires Research: \\nTrue\\n\\nContent:\\nBased on the provided source material, I'll write a comprehensive analysis of Together.ai with revenue estimates:\\n\\n## Together.ai Analysis with Revenue Estimates\\n\\nTogether.ai has demonstrated remarkable growth since its founding in 2022, reaching significant revenue milestones in rapid succession. The company achieved $44 million in annual recurring revenue (ARR) by April 2024, representing a staggering 2,095% year-over-year increase, and further accelerated to $100 million ARR by July 2024 [1]. Current estimates suggest Together.ai's ARR has reached approximately $130 million as of early 2025 [2].\\n\\nThe company's growth is driven by its position as a developer-focused platform for AI infrastructure, offering both GPU compute resources and comprehensive tooling for training and deploying open-source AI models. Together.ai differentiates itself through token-based pricing rather than hourly rates, which appeals particularly to startups with variable workloads [3].\\n\\nTogether.ai's recent $228.5 million in funding and $1.25 billion valuation reflect investor confidence in its business model, which maintains approximately 45% gross margins despite operating in the competitive cloud GPU market [4]. The company has secured strategic partnerships with major industry players like Dell Technologies and Hypertec Cloud to expand its infrastructure capabilities [5].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://www.together.ai/enterprise\\n[4] https://sacra.com/c/together-ai/\\n[5] https://www.dell.com/en-us/dt/corporate/newsroom/announcements/detailpage.press-releases~usa~2025~03~together-ai-taps-dell-technologies-to-scale-ai-acceleration-cloud-for-the-enterprise.htm\\n\\n\\n============================================================\\nSection 4: Groq Analysis with Revenue Estimates\\n============================================================\\nDescription:\\nA section dedicated to Groq, reviewing its unique hardware optimizations for inference, pricing models, performance metrics, and its estimated ARR.\\nRequires Research: \\nTrue\\n\\nContent:\\nHere's my synthesized section on Groq's Analysis with Revenue Estimates:\\n\\n## Groq Analysis with Revenue Estimates\\n\\nGroq has demonstrated impressive technological capabilities with its Language Processing Unit (LPU) for AI inference, though its revenue figures remain modest as it scales up operations. The company's estimated annual revenue is currently $46.6M, with revenue per employee of approximately $116,842 [1][2]. Groq has raised significant funding totaling $1B and achieved a valuation of $2.8B as of August 2024 [1][2].\\n\\nThe company's competitive positioning centers on its unique TSP (Tensor Streaming Processor) architecture that delivers superior low-latency performance for AI inference workloads. Groq's LPU has significant cost advantages in manufacturing, with wafer costs estimated at under $6,000 on 14nm compared to Nvidia's $16,000 for 5nm H100 wafers. This translates to a 70% lower raw silicon cost per token, though system-level costs narrow the total cost of ownership advantage to around 40% [3].\\n\\nGroq monetizes its technology through multiple revenue streams: cloud-based inference API services, hardware leasing and colocation services, and direct hardware sales for data centers [4]. The company has secured major enterprise deployments in Norway and Saudi Arabia, indicating growing commercial traction [4]. While still an emerging player, Groq's aggressive pricing and performance advantages position it to potentially capture a meaningful share of the rapidly growing AI inference market.\\n\\n### Sources\\n[1] https://growjo.com/company/Groq\\n[2] https://compworth.com/company/groq \\n[3] https://sacra.com/c/groq/\\n[4] https://www.chipstrat.com/p/groqs-business-model-part-3-competing\\n\\n\\n============================================================\\nSection 5: Comparative Analysis\\n============================================================\\nDescription:\\nA side-by-side comparison of Fireworks AI, Together.ai, and Groq, highlighting their strengths, weaknesses, cost-effectiveness, and revenue implications to help stakeholders decide.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Comparative Analysis\\n\\nBased on the provided source material, here is a side-by-side comparison of Fireworks AI, Together.ai, and Groq highlighting their key strengths and differentiators:\\n\\nPerformance:\\n- Groq leads in raw inference speed, hitting 250-450 tokens/second with their custom LPU hardware\\n- Fireworks AI achieves around 70-100 tokens/second with optimized software on NVIDIA GPUs\\n- Together.ai delivers 80-90 tokens/second using GPU clusters, with strong reliability\\n\\nCost-Effectiveness:\\n- Together.ai offers competitive pricing at $0.10-0.20 per million tokens for base models\\n- Fireworks AI focuses on cost-efficient inference with rates around $0.20-0.50 per million tokens\\n- Groq balances premium performance with midrange pricing of $0.60-0.70 per million tokens\\n\\nKey Advantages:\\n- Fireworks AI: Strong developer experience, fast API deployment, extensive model support, and consistent performance at scale\\n- Together.ai: Best for AI training, robust fine-tuning capabilities, private model ownership, and enterprise features\\n- Groq: Fastest raw inference speeds, custom hardware optimization, and ultra-low latency for real-time applications\\n\\nBest Suited For:\\n- Fireworks AI: Startups and teams needing reliable, cost-effective inference with good developer tools\\n- Together.ai: Enterprises requiring training capabilities, model customization, and scalable infrastructure\\n- Groq: Applications demanding maximum speed and lowest possible latency\\n\\n### Sources:\\n[1] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[2] https://artificialanalysis.ai/models/llama-3-instruct-70b/providers\\n[3] https://www.helicone.ai/blog/llm-api-providers\\n[4] https://sacra.com/c/groq/\\n\\n\"}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Introduction', description='Provides a brief overview of the AI inference market and introduces the key players: Fireworks AI, Together.ai, and Groq.', research=False, content=\"# AI Inference Market: Analyzing Fireworks, Together.ai, and Groq\\n\\nThe AI inference market is undergoing rapid transformation as new providers emerge with innovative approaches to deploying and scaling AI models. This report examines three key players - Fireworks AI, Together.ai, and Groq - who represent distinct strategies in addressing the growing demand for AI inference solutions. While Fireworks AI focuses on optimizing conventional GPU infrastructure for cost-efficiency, Together.ai emphasizes comprehensive training capabilities, and Groq differentiates through custom hardware designed for maximum speed. Understanding these providers' unique approaches and market positions is crucial for organizations navigating the evolving landscape of AI deployment options.\")]}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Conclusion', description='Summarizes the report findings, distills the main points into a concise table or list, and provides a final assessment of the market and the key players.', research=False, content='## Conclusion\\n\\nThe AI inference market is experiencing rapid evolution with three distinct approaches emerging. Groq leads in raw performance through custom hardware, achieving speeds up to 450 tokens/second, while maintaining healthy margins despite significant infrastructure investments. Together.ai has demonstrated remarkable revenue growth to $130M ARR through its developer-friendly platform and comprehensive training capabilities. Fireworks AI captures the middle ground with optimized performance and cost-efficiency.\\n\\n| Metric | Groq | Together.ai | Fireworks AI |\\n|--------|------|-------------|--------------|\\n| Revenue | $46.6M | $130M | $10-20M |\\n| Speed | 250-450 tok/s | 80-90 tok/s | 70-100 tok/s |\\n| Cost | $0.60-0.70/M tok | $0.10-0.20/M tok | $0.20-0.50/M tok |\\n| Key Strength | Custom Hardware | Training Focus | Cost-Efficiency |\\n| Target Market | Enterprise | Developers | Startups |\\n\\nAs the market matures, expect further specialization among providers, with Groq likely expanding its hardware advantage, Together.ai strengthening its training capabilities, and Fireworks AI optimizing its middle-market position. Success will increasingly depend on matching specific customer needs rather than competing solely on speed or cost.')]}}\n",
      "\n",
      "\n",
      "{'compile_final_report': {'final_report': \"# AI Inference Market: Analyzing Fireworks, Together.ai, and Groq\\n\\nThe AI inference market is undergoing rapid transformation as new providers emerge with innovative approaches to deploying and scaling AI models. This report examines three key players - Fireworks AI, Together.ai, and Groq - who represent distinct strategies in addressing the growing demand for AI inference solutions. While Fireworks AI focuses on optimizing conventional GPU infrastructure for cost-efficiency, Together.ai emphasizes comprehensive training capabilities, and Groq differentiates through custom hardware designed for maximum speed. Understanding these providers' unique approaches and market positions is crucial for organizations navigating the evolving landscape of AI deployment options.\\n\\nHere's my synthesis of the market overview section focused on the AI inference market with a focus on Fireworks, Together.ai, and Groq:\\n\\n## Market Overview\\n\\nThe AI inference market is experiencing rapid growth, projected to reach between $780 billion and $990 billion by 2027, with a compound annual growth rate of 19.2% [1]. This expansion is driven by increasing demand for real-time processing capabilities and the need for efficient AI model deployment across industries.\\n\\nIn the competitive landscape, each provider has distinct advantages. Groq leads in raw inference speed, achieving up to 250 tokens per second with their custom hardware [2]. Fireworks AI differentiates itself through cost-effective inference and consistent performance across varying workload sizes, particularly excelling at processing large context windows up to 10K tokens [3]. Together.ai focuses on AI training capabilities with access to over 10,000 GPUs, though their inference speeds can be variable under heavy loads [4].\\n\\nThe market is seeing significant innovation in infrastructure optimization, with providers taking different approaches. While some like Groq utilize custom chips for maximum speed, others like Fireworks AI focus on optimizing conventional GPU infrastructure for better cost-efficiency and flexibility [3]. This diversification in approaches helps serve different customer needs, from enterprises requiring maximum speed to startups seeking cost-effective solutions.\\n\\n### Sources\\n[1] https://www.globenewswire.com/news-release/2025/04/21/3064502/0/en/AI-Inference-Market-Forecast-Report-to-2030-with-Case-Studies-of-Intel-Siemens-Healthineers-Nvidia-Eleuther-AI.html\\n[2] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[3] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\\n[4] https://www.peerspot.com/products/comparisons/fireworks-ai_vs_together-inference\\n\\n## Fireworks AI Analysis with Revenue Estimates\\n\\nFireworks AI has established itself as a leading platform for serving open-source AI models with competitive performance and pricing. The company offers multiple deployment options including serverless inference, on-demand deployments, and enterprise reserved GPUs [1].\\n\\nKey strengths include superior performance, with their proprietary FireAttention serving stack delivering up to 2.5x faster generation speeds and 3.5x higher throughput compared to competitors like HuggingFace TGI on the same hardware [2]. Their on-demand deployments achieve 6x lower costs while maintaining high performance.\\n\\nThe platform supports a comprehensive model ecosystem including popular open-source models like Llama, DeepSeek, and Mixtral across text, vision, audio and embedding modalities [3]. Their serverless models feature industry-leading speeds of up to 300 tokens/second for models like Mixtral, with rate limits of 600 requests/minute [2].\\n\\nPricing is competitive and usage-based, with options like:\\n- Serverless inference with per-token pricing starting at $0.20 per million tokens\\n- On-demand GPU deployments at $3.89 per hour for A100s\\n- Enterprise plans with custom pricing based on reserved capacity [2]\\n\\nWhile private financial data is limited, given their technology advantages and major customers like Tome, Quora and Sourcegraph [2], estimated annual recurring revenue (ARR) is likely in the $10-20M range based on market comparables.\\n\\n### Sources\\n[1] https://docs.fireworks.ai/getting-started/introduction \\n[2] https://fireworks.ai/blog/spring-update-faster-models-dedicated-deployments-postpaid-pricing\\n[3] https://artificialanalysis.ai/providers/fireworks\\n\\nBased on the provided source material, I'll write a comprehensive analysis of Together.ai with revenue estimates:\\n\\n## Together.ai Analysis with Revenue Estimates\\n\\nTogether.ai has demonstrated remarkable growth since its founding in 2022, reaching significant revenue milestones in rapid succession. The company achieved $44 million in annual recurring revenue (ARR) by April 2024, representing a staggering 2,095% year-over-year increase, and further accelerated to $100 million ARR by July 2024 [1]. Current estimates suggest Together.ai's ARR has reached approximately $130 million as of early 2025 [2].\\n\\nThe company's growth is driven by its position as a developer-focused platform for AI infrastructure, offering both GPU compute resources and comprehensive tooling for training and deploying open-source AI models. Together.ai differentiates itself through token-based pricing rather than hourly rates, which appeals particularly to startups with variable workloads [3].\\n\\nTogether.ai's recent $228.5 million in funding and $1.25 billion valuation reflect investor confidence in its business model, which maintains approximately 45% gross margins despite operating in the competitive cloud GPU market [4]. The company has secured strategic partnerships with major industry players like Dell Technologies and Hypertec Cloud to expand its infrastructure capabilities [5].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://www.together.ai/enterprise\\n[4] https://sacra.com/c/together-ai/\\n[5] https://www.dell.com/en-us/dt/corporate/newsroom/announcements/detailpage.press-releases~usa~2025~03~together-ai-taps-dell-technologies-to-scale-ai-acceleration-cloud-for-the-enterprise.htm\\n\\nHere's my synthesized section on Groq's Analysis with Revenue Estimates:\\n\\n## Groq Analysis with Revenue Estimates\\n\\nGroq has demonstrated impressive technological capabilities with its Language Processing Unit (LPU) for AI inference, though its revenue figures remain modest as it scales up operations. The company's estimated annual revenue is currently $46.6M, with revenue per employee of approximately $116,842 [1][2]. Groq has raised significant funding totaling $1B and achieved a valuation of $2.8B as of August 2024 [1][2].\\n\\nThe company's competitive positioning centers on its unique TSP (Tensor Streaming Processor) architecture that delivers superior low-latency performance for AI inference workloads. Groq's LPU has significant cost advantages in manufacturing, with wafer costs estimated at under $6,000 on 14nm compared to Nvidia's $16,000 for 5nm H100 wafers. This translates to a 70% lower raw silicon cost per token, though system-level costs narrow the total cost of ownership advantage to around 40% [3].\\n\\nGroq monetizes its technology through multiple revenue streams: cloud-based inference API services, hardware leasing and colocation services, and direct hardware sales for data centers [4]. The company has secured major enterprise deployments in Norway and Saudi Arabia, indicating growing commercial traction [4]. While still an emerging player, Groq's aggressive pricing and performance advantages position it to potentially capture a meaningful share of the rapidly growing AI inference market.\\n\\n### Sources\\n[1] https://growjo.com/company/Groq\\n[2] https://compworth.com/company/groq \\n[3] https://sacra.com/c/groq/\\n[4] https://www.chipstrat.com/p/groqs-business-model-part-3-competing\\n\\n## Comparative Analysis\\n\\nBased on the provided source material, here is a side-by-side comparison of Fireworks AI, Together.ai, and Groq highlighting their key strengths and differentiators:\\n\\nPerformance:\\n- Groq leads in raw inference speed, hitting 250-450 tokens/second with their custom LPU hardware\\n- Fireworks AI achieves around 70-100 tokens/second with optimized software on NVIDIA GPUs\\n- Together.ai delivers 80-90 tokens/second using GPU clusters, with strong reliability\\n\\nCost-Effectiveness:\\n- Together.ai offers competitive pricing at $0.10-0.20 per million tokens for base models\\n- Fireworks AI focuses on cost-efficient inference with rates around $0.20-0.50 per million tokens\\n- Groq balances premium performance with midrange pricing of $0.60-0.70 per million tokens\\n\\nKey Advantages:\\n- Fireworks AI: Strong developer experience, fast API deployment, extensive model support, and consistent performance at scale\\n- Together.ai: Best for AI training, robust fine-tuning capabilities, private model ownership, and enterprise features\\n- Groq: Fastest raw inference speeds, custom hardware optimization, and ultra-low latency for real-time applications\\n\\nBest Suited For:\\n- Fireworks AI: Startups and teams needing reliable, cost-effective inference with good developer tools\\n- Together.ai: Enterprises requiring training capabilities, model customization, and scalable infrastructure\\n- Groq: Applications demanding maximum speed and lowest possible latency\\n\\n### Sources:\\n[1] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[2] https://artificialanalysis.ai/models/llama-3-instruct-70b/providers\\n[3] https://www.helicone.ai/blog/llm-api-providers\\n[4] https://sacra.com/c/groq/\\n\\n## Conclusion\\n\\nThe AI inference market is experiencing rapid evolution with three distinct approaches emerging. Groq leads in raw performance through custom hardware, achieving speeds up to 450 tokens/second, while maintaining healthy margins despite significant infrastructure investments. Together.ai has demonstrated remarkable revenue growth to $130M ARR through its developer-friendly platform and comprehensive training capabilities. Fireworks AI captures the middle ground with optimized performance and cost-efficiency.\\n\\n| Metric | Groq | Together.ai | Fireworks AI |\\n|--------|------|-------------|--------------|\\n| Revenue | $46.6M | $130M | $10-20M |\\n| Speed | 250-450 tok/s | 80-90 tok/s | 70-100 tok/s |\\n| Cost | $0.60-0.70/M tok | $0.10-0.20/M tok | $0.20-0.50/M tok |\\n| Key Strength | Custom Hardware | Training Focus | Cost-Efficiency |\\n| Target Market | Enterprise | Developers | Startups |\\n\\nAs the market matures, expect further specialization among providers, with Groq likely expanding its hardware advantage, Together.ai strengthening its training capabilities, and Fireworks AI optimizing its middle-market position. Success will increasingly depend on matching specific customer needs rather than competing solely on speed or cost.\"}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass True to approve the report plan \n",
    "async for event in graph.astream(Command(resume=True), thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI Inference Market: Analyzing Fireworks, Together.ai, and Groq\n",
       "\n",
       "The AI inference market is undergoing rapid transformation as new providers emerge with innovative approaches to deploying and scaling AI models. This report examines three key players - Fireworks AI, Together.ai, and Groq - who represent distinct strategies in addressing the growing demand for AI inference solutions. While Fireworks AI focuses on optimizing conventional GPU infrastructure for cost-efficiency, Together.ai emphasizes comprehensive training capabilities, and Groq differentiates through custom hardware designed for maximum speed. Understanding these providers' unique approaches and market positions is crucial for organizations navigating the evolving landscape of AI deployment options.\n",
       "\n",
       "Here's my synthesis of the market overview section focused on the AI inference market with a focus on Fireworks, Together.ai, and Groq:\n",
       "\n",
       "## Market Overview\n",
       "\n",
       "The AI inference market is experiencing rapid growth, projected to reach between $780 billion and $990 billion by 2027, with a compound annual growth rate of 19.2% [1]. This expansion is driven by increasing demand for real-time processing capabilities and the need for efficient AI model deployment across industries.\n",
       "\n",
       "In the competitive landscape, each provider has distinct advantages. Groq leads in raw inference speed, achieving up to 250 tokens per second with their custom hardware [2]. Fireworks AI differentiates itself through cost-effective inference and consistent performance across varying workload sizes, particularly excelling at processing large context windows up to 10K tokens [3]. Together.ai focuses on AI training capabilities with access to over 10,000 GPUs, though their inference speeds can be variable under heavy loads [4].\n",
       "\n",
       "The market is seeing significant innovation in infrastructure optimization, with providers taking different approaches. While some like Groq utilize custom chips for maximum speed, others like Fireworks AI focus on optimizing conventional GPU infrastructure for better cost-efficiency and flexibility [3]. This diversification in approaches helps serve different customer needs, from enterprises requiring maximum speed to startups seeking cost-effective solutions.\n",
       "\n",
       "### Sources\n",
       "[1] https://www.globenewswire.com/news-release/2025/04/21/3064502/0/en/AI-Inference-Market-Forecast-Report-to-2030-with-Case-Studies-of-Intel-Siemens-Healthineers-Nvidia-Eleuther-AI.html\n",
       "[2] https://koonka.ai/fireworks-ai-vs-together-ai/\n",
       "[3] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\n",
       "[4] https://www.peerspot.com/products/comparisons/fireworks-ai_vs_together-inference\n",
       "\n",
       "## Fireworks AI Analysis with Revenue Estimates\n",
       "\n",
       "Fireworks AI has established itself as a leading platform for serving open-source AI models with competitive performance and pricing. The company offers multiple deployment options including serverless inference, on-demand deployments, and enterprise reserved GPUs [1].\n",
       "\n",
       "Key strengths include superior performance, with their proprietary FireAttention serving stack delivering up to 2.5x faster generation speeds and 3.5x higher throughput compared to competitors like HuggingFace TGI on the same hardware [2]. Their on-demand deployments achieve 6x lower costs while maintaining high performance.\n",
       "\n",
       "The platform supports a comprehensive model ecosystem including popular open-source models like Llama, DeepSeek, and Mixtral across text, vision, audio and embedding modalities [3]. Their serverless models feature industry-leading speeds of up to 300 tokens/second for models like Mixtral, with rate limits of 600 requests/minute [2].\n",
       "\n",
       "Pricing is competitive and usage-based, with options like:\n",
       "- Serverless inference with per-token pricing starting at $0.20 per million tokens\n",
       "- On-demand GPU deployments at $3.89 per hour for A100s\n",
       "- Enterprise plans with custom pricing based on reserved capacity [2]\n",
       "\n",
       "While private financial data is limited, given their technology advantages and major customers like Tome, Quora and Sourcegraph [2], estimated annual recurring revenue (ARR) is likely in the $10-20M range based on market comparables.\n",
       "\n",
       "### Sources\n",
       "[1] https://docs.fireworks.ai/getting-started/introduction \n",
       "[2] https://fireworks.ai/blog/spring-update-faster-models-dedicated-deployments-postpaid-pricing\n",
       "[3] https://artificialanalysis.ai/providers/fireworks\n",
       "\n",
       "Based on the provided source material, I'll write a comprehensive analysis of Together.ai with revenue estimates:\n",
       "\n",
       "## Together.ai Analysis with Revenue Estimates\n",
       "\n",
       "Together.ai has demonstrated remarkable growth since its founding in 2022, reaching significant revenue milestones in rapid succession. The company achieved $44 million in annual recurring revenue (ARR) by April 2024, representing a staggering 2,095% year-over-year increase, and further accelerated to $100 million ARR by July 2024 [1]. Current estimates suggest Together.ai's ARR has reached approximately $130 million as of early 2025 [2].\n",
       "\n",
       "The company's growth is driven by its position as a developer-focused platform for AI infrastructure, offering both GPU compute resources and comprehensive tooling for training and deploying open-source AI models. Together.ai differentiates itself through token-based pricing rather than hourly rates, which appeals particularly to startups with variable workloads [3].\n",
       "\n",
       "Together.ai's recent $228.5 million in funding and $1.25 billion valuation reflect investor confidence in its business model, which maintains approximately 45% gross margins despite operating in the competitive cloud GPU market [4]. The company has secured strategic partnerships with major industry players like Dell Technologies and Hypertec Cloud to expand its infrastructure capabilities [5].\n",
       "\n",
       "### Sources\n",
       "[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\n",
       "[2] https://sacra.com/c/together-ai/\n",
       "[3] https://www.together.ai/enterprise\n",
       "[4] https://sacra.com/c/together-ai/\n",
       "[5] https://www.dell.com/en-us/dt/corporate/newsroom/announcements/detailpage.press-releases~usa~2025~03~together-ai-taps-dell-technologies-to-scale-ai-acceleration-cloud-for-the-enterprise.htm\n",
       "\n",
       "Here's my synthesized section on Groq's Analysis with Revenue Estimates:\n",
       "\n",
       "## Groq Analysis with Revenue Estimates\n",
       "\n",
       "Groq has demonstrated impressive technological capabilities with its Language Processing Unit (LPU) for AI inference, though its revenue figures remain modest as it scales up operations. The company's estimated annual revenue is currently $46.6M, with revenue per employee of approximately $116,842 [1][2]. Groq has raised significant funding totaling $1B and achieved a valuation of $2.8B as of August 2024 [1][2].\n",
       "\n",
       "The company's competitive positioning centers on its unique TSP (Tensor Streaming Processor) architecture that delivers superior low-latency performance for AI inference workloads. Groq's LPU has significant cost advantages in manufacturing, with wafer costs estimated at under $6,000 on 14nm compared to Nvidia's $16,000 for 5nm H100 wafers. This translates to a 70% lower raw silicon cost per token, though system-level costs narrow the total cost of ownership advantage to around 40% [3].\n",
       "\n",
       "Groq monetizes its technology through multiple revenue streams: cloud-based inference API services, hardware leasing and colocation services, and direct hardware sales for data centers [4]. The company has secured major enterprise deployments in Norway and Saudi Arabia, indicating growing commercial traction [4]. While still an emerging player, Groq's aggressive pricing and performance advantages position it to potentially capture a meaningful share of the rapidly growing AI inference market.\n",
       "\n",
       "### Sources\n",
       "[1] https://growjo.com/company/Groq\n",
       "[2] https://compworth.com/company/groq \n",
       "[3] https://sacra.com/c/groq/\n",
       "[4] https://www.chipstrat.com/p/groqs-business-model-part-3-competing\n",
       "\n",
       "## Comparative Analysis\n",
       "\n",
       "Based on the provided source material, here is a side-by-side comparison of Fireworks AI, Together.ai, and Groq highlighting their key strengths and differentiators:\n",
       "\n",
       "Performance:\n",
       "- Groq leads in raw inference speed, hitting 250-450 tokens/second with their custom LPU hardware\n",
       "- Fireworks AI achieves around 70-100 tokens/second with optimized software on NVIDIA GPUs\n",
       "- Together.ai delivers 80-90 tokens/second using GPU clusters, with strong reliability\n",
       "\n",
       "Cost-Effectiveness:\n",
       "- Together.ai offers competitive pricing at $0.10-0.20 per million tokens for base models\n",
       "- Fireworks AI focuses on cost-efficient inference with rates around $0.20-0.50 per million tokens\n",
       "- Groq balances premium performance with midrange pricing of $0.60-0.70 per million tokens\n",
       "\n",
       "Key Advantages:\n",
       "- Fireworks AI: Strong developer experience, fast API deployment, extensive model support, and consistent performance at scale\n",
       "- Together.ai: Best for AI training, robust fine-tuning capabilities, private model ownership, and enterprise features\n",
       "- Groq: Fastest raw inference speeds, custom hardware optimization, and ultra-low latency for real-time applications\n",
       "\n",
       "Best Suited For:\n",
       "- Fireworks AI: Startups and teams needing reliable, cost-effective inference with good developer tools\n",
       "- Together.ai: Enterprises requiring training capabilities, model customization, and scalable infrastructure\n",
       "- Groq: Applications demanding maximum speed and lowest possible latency\n",
       "\n",
       "### Sources:\n",
       "[1] https://koonka.ai/fireworks-ai-vs-together-ai/\n",
       "[2] https://artificialanalysis.ai/models/llama-3-instruct-70b/providers\n",
       "[3] https://www.helicone.ai/blog/llm-api-providers\n",
       "[4] https://sacra.com/c/groq/\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The AI inference market is experiencing rapid evolution with three distinct approaches emerging. Groq leads in raw performance through custom hardware, achieving speeds up to 450 tokens/second, while maintaining healthy margins despite significant infrastructure investments. Together.ai has demonstrated remarkable revenue growth to $130M ARR through its developer-friendly platform and comprehensive training capabilities. Fireworks AI captures the middle ground with optimized performance and cost-efficiency.\n",
       "\n",
       "| Metric | Groq | Together.ai | Fireworks AI |\n",
       "|--------|------|-------------|--------------|\n",
       "| Revenue | $46.6M | $130M | $10-20M |\n",
       "| Speed | 250-450 tok/s | 80-90 tok/s | 70-100 tok/s |\n",
       "| Cost | $0.60-0.70/M tok | $0.10-0.20/M tok | $0.20-0.50/M tok |\n",
       "| Key Strength | Custom Hardware | Training Focus | Cost-Efficiency |\n",
       "| Target Market | Enterprise | Developers | Startups |\n",
       "\n",
       "As the market matures, expect further specialization among providers, with Groq likely expanding its hardware advantage, Together.ai strengthening its training capabilities, and Fireworks AI optimizing its middle-market position. Success will increasingly depend on matching specific customer needs rather than competing solely on speed or cost."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/d32aad4d-e5b8-49f9-a082-4987de5a8970/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-deep-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
